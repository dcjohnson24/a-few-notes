[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A few notes"
  },
  {
    "objectID": "posts/post-with-code/Assessment.html",
    "href": "posts/post-with-code/Assessment.html",
    "title": "Pepkor Assessment Part 2",
    "section": "",
    "text": "Part 2 of the assesment I completed for Pepkor. Part 1 is available here. Data for the assessment is available here.\n\nQuestion 5¶\n\n\nThe registrations dataset contains customer details which was harvested when they registered for or activated a product or service within the Pepkor group. You need to do the following:\n\n\n\nAnalyse and show the following statistics:\n\n\n\nNumber of unique customers\n\n\nGender distribution\n\n\nAverage age of customers\n\n\nNumber of laybyes per subsidiary as percentage of overall number of laybyes (hint: sk_subsidiary_no &lt;&gt; prod_subsidiary_no)\n\n\nMedian age of ‘Competition 6’ customers\n\n\n\n\nAdd additional analyses based on the registration dataset that you picked up, including two graphs of your own choice and document two findings/insights from this dataset (Do not overthink it).\n\n\n\n\nStart by importing the relevant libraries\n\n\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n/tmp/ipykernel_13517/427504256.py:3: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\n\nThen load in the excel file and separate the sheets into separate DataFrames in Pandas\n\n\n\np = Path.home() / 'pepkor-test' \nxls = pd.ExcelFile(p / 'data' / 'Assessment.xlsx')\nsheet_names = xls.sheet_names\ndf_list = [pd.read_excel(xls, sheet_name=sheet) for sheet in sheet_names]\nregistration, transactions, dim_cust_prod, dim_subsidiary = df_list\n\n\n\nThe number of unique customers is 9969\n\n\n\nprint(registration.SK_CUST_NO.nunique())\n\n\n9969\n\n\n\nThe gender distribution is 80 percent women, 20 percent men.\n\n\n\nprint(registration.GENDER_IND.value_counts(normalize=True))\n\n\nGENDER_IND\nF    0.801736\nM    0.198264\nName: proportion, dtype: float64\n\n\n\nTo calculate the average age of customers, we first get the ages\n\n\n\ntoday = pd.to_datetime('today')\nregistration['AGE'] = registration.BIRTH_DATE.apply(lambda x: today - x)\nregistration['AGE'] = registration['AGE'] / np.timedelta64(365, 'D')\nprint(registration[['AGE', 'BIRTH_DATE']].head())\n\n\n         AGE BIRTH_DATE\n0  53.010456 1971-02-13\n1  52.385798 1971-09-29\n2  44.100867 1980-01-09\n3  47.615935 1976-07-05\n4  43.857031 1980-04-07\n\n\n\nThen take the average to get\n\n\n\nprint(registration['AGE'].mean())\n\n\n42.08705549921488\n\n\n\nNext we find the number of laybyes per subsidiary. Start by merging the registration table with the dim_subsidary table\n\n\n\nmerged = registration.merge(\n            dim_subsidiary, \n            left_on='SK_SOURCE_SUBSIDIARY_NO', \n            right_on='SK_SUBSIDIARY_NO').drop(['SUBSIDIARY_NO', 'SUBSIDIARY_ABBR'], axis=1)    \n\n\n\nThen we subset the data to take only the laybyes\n\n\n\nmerged_laybyes = merged[merged.CUST_PROD_DESC == 'LAYBYE']\n\n\n\nFinally we get the count by subsidiary and divide by the total to get the proportions\n\n\n\ncounts = merged_laybyes.groupby('SUBSIDIARY_NAME').count()['SK_CUST_PROD_NO'] \nprint(counts / len(merged_laybyes))\n    \n\n\nSUBSIDIARY_NAME\nAckermans           0.531265\nPep                 0.350232\nSpeciality Group    0.118503\nName: SK_CUST_PROD_NO, dtype: float64\n\n\n\nThe dim_cust_prod data also has information on laybyes, so we repeat the procedure there as well\n\n\n\nmerged_dim_cust = dim_cust_prod.merge(\n            dim_subsidiary,\n            left_on='PROD_SUBSIDIARY_NO',\n            right_on='SK_SUBSIDIARY_NO').drop(['SUBSIDIARY_NO', 'SUBSIDIARY_ABBR'], axis=1)\n\nmerged_dim_cust_laybye = merged_dim_cust[\n            merged_dim_cust.PROD_GRP_NAME == 'Laybye']\n    \n\n\n\n\nprint(\n    merged_dim_cust_laybye.groupby('SUBSIDIARY_NAME').count()['SK_CUST_PROD_NO'] / len(merged_dim_cust_laybye)\n)\n\n\nSUBSIDIARY_NAME\nAckermans     0.1\nDunns         0.1\nFutureCELL    0.1\nJohn Craig    0.1\nNONE          0.1\nPep           0.5\nName: SK_CUST_PROD_NO, dtype: float64\n\n\n\nTo find the median age of ‘competition 6’ customers, we first merge the registration and dim_cust_prod tables\n\n\n\nmerged_reg_cust = registration.merge(\n            dim_cust_prod,\n            on='SK_CUST_PROD_NO')\n\n\n\nThen we subset for the ‘competition 6’ customers. There are none.\n\n\n\ncomp6 = merged_reg_cust[merged_reg_cust.CUST_PROD_DESC_y == 'COMPETITION 6']\nprint(comp6)\n\n\nEmpty DataFrame\nColumns: [SK_CUST_PROD_NO, SK_SOURCE_SUBSIDIARY_NO, SK_CUST_NO, CUST_REF_CODE, BIRTH_DATE, TAKE_UP_DATE, GENDER_IND, VALID_EMAIL_ADD_IND, CELL_PHONE, EMAIL_ADD, FIRST_NAME, HOME_PHONE, LAST_NAME, WORK_PHONE, HOME_ADD_LINE1, HOME_ADD_LINE2, HOME_ADD_LINE3, HOME_POST_CODE, WORK_ADD_LINE1, WORK_ADD_LINE2, WORK_ADD_LINE3, WORK_POST_CODE, POSTAL_ADD_LINE1, POSTAL_ADD_LINE2, POSTAL_ADD_LINE3, POSTAL_POST_CODE, CUST_PROD_DESC_x, Question, Answer, All work was done in Python. There is an accompanying Jupyter notebook to explain the code, Unnamed: 30, Unnamed: 31, AGE, CUST_PROD_NO, CUST_PROD_DESC_y, PROD_SUBSIDIARY_NO, PROD_GRP_NAME, MSISDN_PREFIX, COUNTRY_CODE]\nIndex: []\n\n[0 rows x 39 columns]\n\n\n\nOne interesting finding is that the average age is highest for bill payments. We also note that the average of age of men for shoe city is 51; for women it is 36.\n\n\n\ngender_group = registration.groupby(['CUST_PROD_DESC', 'GENDER_IND'])['AGE'].mean()\ngender_group.unstack().plot(kind='bar')\nplt.xlabel('Product Description')\nplt.ylabel('Age')\nplt.legend(title='Gender')\nplt.title('Average Age by Product Description')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nA second finding is that, from a logistic regression, a one year increase in age is associated with at most a 0.02 percent decrease in having a valid email.\n\n\n\n\n\nFrom the same regression output, a man is also at most 16 percent more likely to have a valid email than a woman. Here a woman is coded as 0 and a man as 1\n\n\n\n\n\nQuestion 6¶\n\n\nThe transactions dataset contains point of sale basket information for one of the Pepkor brands. You need to do the following:\n\n\n\nAnalyse and show the following statistics:\n\n\n\nNumber of unique customers that shopped\n\n\nAverage transaction (basket) value (hint: trn_no_code)\n\n\nAverage sk_store_no\n\n\nHow many stores per subsidiary in this dataset\n\n\nIf someone mentioned to you that the sales for this specific date were significantly lower than two days prior, what do you think could be possible reasons for it?\n\n\n\n\nAdd additional analyses based on the transactions dataset that you picked up, including two graphs of your own choice and document two findings/insights from this dataset (Do not overthink it).\n\n\n\n\nThere are 22 unique customers\n\n\n\nprint(transactions.SK_CUST_NO.nunique())\n\n\n22\n\n\n\nTo find the average transaction value, I start by changing the negative transaction values to positive because this will affect the average.\n\n\n\ntransactions.BINC_TRN_AMT = transactions.BINC_TRN_AMT.abs()\n\n\n\nIf we use the simple average of BINC_TRN_AMT, we get 100.\n\n\n\nprint(transactions.groupby('TRN_NO_CODE')['BINC_TRN_AMT'].mean())\nprint(transactions.BINC_TRN_AMT.mean())\n\n\nTRN_NO_CODE\n73135     117.950000\n100249    138.300000\n100252     24.310714\n100254    126.616667\n226124    112.302941\n226125     29.500000\n306568     35.170000\n306569     38.407143\n306571     79.950000\n306573     20.483333\n306577    100.733333\n306587    156.616667\n306592     38.725000\n349965    499.500000\n349971     49.666667\n349973     99.950000\n349974    150.950000\n349976     22.950000\n361653    199.950000\n361659     99.950000\n361668    514.000000\n361677    129.475000\nName: BINC_TRN_AMT, dtype: float64\n99.9964912280702\n\n\n\nThe average sk_store_no is 2400.9\n\n\n\nprint(transactions.SK_STORE_NO.mean())\n\n\n2400.90350877193\n\n\n\nThere are 2 stores per subsidiary\n\n\n\nprint(transactions.groupby('SK_STORE_SUBSIDIARY_NO').SK_STORE_NO.nunique())\n\n\nSK_STORE_SUBSIDIARY_NO\n3    2\nName: SK_STORE_NO, dtype: int64\n\n\n\nIf sales were lower on a day than the previous two days, it could be that sales are seasonal and that historically on this date, sales were lower.\n\n\nThe store hours could have been affected by public holiday and so sales could be lower.\n\n\nIt could also be because of changes in management at a particular store or subsidiary.\n\n\nAnother possibility is that the trend of sales is unstable to begin with.\n\n\nOne interesting finding is that the average transaction value of the store 2396 is 84.72 and for store 2409 it is 125.21. Store 2396 has a positive skew (75th percentile is R99.95); store 2409 has a slight negative skew (75th percentile is R149.95)\n\n\n\ntransactions.boxplot(column='BINC_TRN_AMT', by='SK_STORE_NO') \nplt.title('Boxplot grouped by Store Number')\nplt.suptitle('')\nplt.xlabel('Store Number')\nplt.ylabel('Transaction Value (ZAR)')\nplt.tight_layout()\nplt.show()    \n\n\n\n\n\n\n\n\n\n\nAnother finding is that Terminal No 2 has the lowest number of transactions at 5. Terminal No 4 has the highest number of transactions at 42. Terminal 3 has the highest median transaction value at R99.95. Terminal 1 has a positive skew with the 75 percentile at R89\n\n\n\ntransactions.boxplot(column='BINC_TRN_AMT', by='TERMINAL_NO_CODE')\nplt.title('Boxplot grouped by Store Number')\nplt.suptitle('')\nplt.xlabel('Terminal Number Code')\nplt.ylabel('Transaction Value (ZAR)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI would have liked to explore the relationship between age and transaction value, but a merge with the registrations table on SK_CUST_NO turned up empty."
  },
  {
    "objectID": "posts/post-with-code/Refinitive_Test.html",
    "href": "posts/post-with-code/Refinitive_Test.html",
    "title": "Refinitiv Assessment",
    "section": "",
    "text": "This is an assessment I did for Refinitiv in May 2019. The data is available here. I received an offer.\n\nRefinitive Test¶\n\n\nQuestion 1¶\n\n\nExamine the dataset in sheet ‘Question1_Data’ and answer the following questions: 1.1 How many employees are currently employed (Inactive = 0)? Enter the formula in column P 1.2 How many employees are actively employed in the IT Department in Cape Town? Enter the formula in column P 1.3 What is the total number of leave days for all active employees in Cape Town? Enter the formula in column P\n\n\nBegin by reading in the excel book and creating data frames in pandas\n\n\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n\n\np = Path.home() / 'refinitiv-test'\nxls = pd.ExcelFile(p / 'ExcelTest1.2.xlsx')\ndf_list = [pd.read_excel(xls, sheet) for sheet in xls.sheet_names]\n_, data1, _, data2, _, data3, _ = df_list\n\n\n\nThe first dataset data1 has empty columns so we drop them\n\n\n\ndata1.dropna(axis=1, how='all', inplace=True)\nprint(data1.head())\n\n\n   EmployeeID DepartmentID       Office  InActive  Employee ID  LeaveDays\n0           1           HR  East London         0            1          7\n1           2           HR     Pretoria         0            2         13\n2           3  Procurement    Cape Town         1            3         12\n3           4  Procurement       Durban         1            4          7\n4           5          R&D    Cape Town         0            5          6\n\n\n\n1.1¶\n\n\nHow many employees are currently employed (Inactive = 0)? Enter the formula in column P\n\n\n\n# Question 1.1\ndata1.InActive.value_counts()\n    \n\n\nInActive\n1    1112\n0    1097\nName: count, dtype: int64\n\n\n\nWe see that there are 1097 active employees\n\n\n1.2¶\n\n\nHow many employees are actively employed in the IT Department in Cape Town? Enter the formula in column P\n\n\nWe take the rows for the IT department that have an InActive code of 0\n\n\n\n# Question 1.2\nlen(data1[(data1.DepartmentID == 'IT') & (data1.InActive == 0)])\n\n\n118\n\n\n\nThere are 118 people employed in the IT department in Cape Town\n\n\n1.3¶\n\n\nWhat is the total number of leave days for all active employees in Cape Town? Enter the formula in column P\n\n\nStart by getting all the active employees in Cape Town\n\n\n\ncape_town = data1[(data1.Office == 'Cape Town') & (data1.InActive==0)]\nprint(cape_town.head())\n\n\n    EmployeeID DepartmentID     Office  InActive  Employee ID  LeaveDays\n4            5          R&D  Cape Town         0            5          6\n6            7      Payroll  Cape Town         0            7         10\n14          15          R&D  Cape Town         0           15         12\n31          32      Payroll  Cape Town         0           32          7\n32          33           HR  Cape Town         0           33         14\n\n\n\nThen sum up the leave days\n\n\n\ncape_town.LeaveDays.sum()\n\n\n2379\n\n\n\nThere are a total of 2379 leave days\n\n\nQuestion 2¶\n\n\nExamine the dataset in sheet ‘Question2_Data’ and answer the following questions: 2.1 How many distinct UIDs contain the Source URL ‘www.abc.com’? 2.2 How many distinct UIDs contain duplicate Source URLs? 2.3 How many distinct Source URLs does UID 2989625 have?\n\n\n2.1¶\n\n\nHow many distinct UIDs contain the Source URL ‘www.abc.com’?\n\n\nFirst we look for the distinct UIDs\n\n\n\ndata2_dedup = data2.drop_duplicates(subset='UID')\nprint('There are ', len(data2.UID), 'UIDs in the data')\nprint('There are ', data2.UID.nunique(), 'distinct UIDs in the data')\n\n\nThere are  1329 UIDs in the data\nThere are  1219 distinct UIDs in the data\n\n\n\nNext, search for the string www.abc.com in the Source URL string\n\n\n\ndata2_dedup['Source URLs'].str.contains('www.abc.com').sum()\n\n\n568\n\n\n\nThere are 568 distinct UIDs that contain the source url ‘www.abc.com’\n\n\n2.2¶\n\n\nHow many distinct UIDs contain duplicate Source URLs?\n\n\nUsing our de-duplicated data on UID, check for duplicates in the Source URL\n\n\n\ndata2_dedup['Source URLs'].duplicated().sum()\n\n\n1162\n\n\n\nThere are 1,162 duplicates in the Source URLs\n\n\n2.3¶\n\n\nHow many distinct Source URLs does UID 2989625 have?\n\n\nWe check for rows where UID is 2989625 and then count the unique Source URLs for those rows\n\n\n\ndata2[data2.UID == 2989625]['Source URLs'].nunique()\n\n\n1\n\n\n\nThere is only one unique source URL for UID 2989625\n\n\n2.4¶\n\n\nProvide the count of distinct citizens for each of the Countries in column O:\n\n\nWe start by breaking the Citizenship column into multiple columns to account for multiple nationalities\n\n\n\ncitizen = data2_dedup.CITIZENSHIP.str.split(';', expand=True)\ncitizen.columns = ['Citizenship1', 'Citizenship2', 'Citizenship3']\nprint(citizen.head())    \n\n\n  Citizenship1 Citizenship2 Citizenship3\n0          USA         None         None\n1          USA   MOZAMBIQUE         None\n2          USA         None         None\n3          USA       CYPRUS         None\n4          USA         None         None\n\n\n\nThen we get the counts for each column\n\n\n\ncitizen.Citizenship1.value_counts()\n\n\nCitizenship1\nUSA    1219\nName: count, dtype: int64\n\n\n\n\ncitizen.Citizenship2.value_counts()\n\n\nCitizenship2\nRUSSIAN FEDERATION    47\nCYPRUS                33\nZAMBIA                31\nMOZAMBIQUE            26\nCHINA                 24\nFRANCE                18\nName: count, dtype: int64\n\n\n\n\ncitizen.Citizenship3.value_counts()\n\n\nCitizenship3\nPARAGUAY    18\nName: count, dtype: int64\n\n\n\n2.5¶\n\n\nProvide the count of individuals born between 1960 and 1970\n\n\nSince there are multiple dates of birth for a person, we start by separating them\n\n\n\ndob = data2_dedup.DOB.str.split(';', expand=True).fillna(np.nan)\ndob.columns = [f'dob{i}' for i in range(dob.shape[1])]\nprint(dob.head())\n\n\n         dob0        dob1 dob2 dob3 dob4 dob5 dob6 dob7\n0  1968/00/00         NaN  NaN  NaN  NaN  NaN  NaN  NaN\n1         NaN         NaN  NaN  NaN  NaN  NaN  NaN  NaN\n2  1955/03/01  1953/00/00  NaN  NaN  NaN  NaN  NaN  NaN\n3         NaN         NaN  NaN  NaN  NaN  NaN  NaN  NaN\n4         NaN         NaN  NaN  NaN  NaN  NaN  NaN  NaN\n\n\n\nSince we are only interested in the year, we take the first part before the ‘/’ of every string of the form ‘yyyy/mm/dd’. We define a function that will extract the year portion of the string\n\n\n\ndef date_strip(x):\n    return  pd.DataFrame(x.astype(str).str.split('/').tolist()).iloc[:,0].astype(float)\n\n\n\nApplying the date_strip function to every column gives us\n\n\n\nyears = dob.apply(date_strip)\nprint(years.head())\n\n\n     dob0    dob1  dob2  dob3  dob4  dob5  dob6  dob7\n0  1968.0     NaN   NaN   NaN   NaN   NaN   NaN   NaN\n1     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN\n2  1955.0  1953.0   NaN   NaN   NaN   NaN   NaN   NaN\n3     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN\n4     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN\n\n\n\nThen we count the number of years of birth that are between 1960 and 1970 inclusive, being careful to check for people with multiple years that fall in the range to avoid double counting.\n\n\nThe function defined by lambda will count the number of birth years that are between 1960 and 1970\n\n\n\nyears.apply(lambda x: ((x &gt;= 1960) & (x &lt;= 1970)).sum(), axis=1).value_counts()   \n\n\n0    1176\n1      37\n2       6\nName: count, dtype: int64\n\n\n\nWe can see that there are 37 people born between 1960 and 1970. The additional 6 are for people with different birthdates recorded but the same birth year\n\n\nQuestion 3¶\n\n\nExamine the dataset in sheet ‘Question3_Data’ and answer the following questions: 3.1 For each of the TEJ_Code values, extract the number between underscore characters and list them in column C. What is the sum of all the extracted numbers? 3.2 Replace the extracted number in the original TEJ_Code with the corresponding KW in Column K. Place the new TEJ_Code in a new column called ‘TEJ_Code_KW’\n\n\n3.1¶\n\n\nFor each of the TEJ_Code values, extract the number between underscore characters and list them in column C. What is the sum of all the extracted numbers?\n\n\nThere are columns in data3 that are empty so we drop them\n\n\n\ndata3.dropna(axis=1, how='all', inplace=True)\n\n\n\nNext, we extract the number part of the TEJ codes\n\n\n\ntej_codes = data3.TEJ_Code.str.split('_', expand=True)\nprint(tej_codes.head())\n\n\n     0   1        2\n0  TEJ  64     DGHM\n1  TEJ  75    WESGV\n2  TEJ  61    SATRU\n3  TEJ  35     AERH\n4  TEJ  51  AQEWRGT\n\n\n\nThen sum up the second column\n\n\n\ntej_codes.iloc[:,1].astype(int).sum()\n\n\n49465\n\n\n\nThe sum of the numbers is 49465\n\n\n3.2¶\n\n\nReplace the extracted number in the original TEJ_Code with the corresponding KW in Column K. Place the new TEJ_Code in a new column called ‘TEJ_Code_KW’\n\n\nUsing the tej_codes subset from part 3.1, we rename the columns from 0, 1, 2 to ‘TEJ’, ‘ID’, ‘LAST’. We also cast the ID column as a float, so that it can be used to merge with data3\n\n\n\ntej_codes.columns = ['TEJ', 'ID', 'LAST']\ntej_codes.ID = tej_codes.ID.astype(float)  \nprint(tej_codes.head())\n\n\n   TEJ    ID     LAST\n0  TEJ  64.0     DGHM\n1  TEJ  75.0    WESGV\n2  TEJ  61.0    SATRU\n3  TEJ  35.0     AERH\n4  TEJ  51.0  AQEWRGT\n\n\n\nNext, we match the integer parts with the ID column. After matching, we use the corresponding KW to create the new TEJ_Code_KW column.\n\n\n\nmerged = tej_codes.merge(data3, on='ID')\nprint(merged.head())\n\n\n   TEJ    ID     LAST     UID       TEJ_Code   KW\n0  TEJ  64.0     DGHM  289079    TEJ_34_DGHM  ghi\n1  TEJ  75.0    WESGV  237266   TEJ_48_AERGH  stu\n2  TEJ  61.0    SATRU   26111   TEJ_72_ADFRH  ijk\n3  TEJ  35.0     AERH  260015  TEJ_60_ZDGTJH  lmn\n4  TEJ  51.0  AQEWRGT   59990    TEJ_75_AE5Y  vwx\n\n\n\nFinally, TEJ_Code_KW is created by concatenating the string TEJ, the KW and the last 4 characters.\n\n\n\nmerged['TEJ_Code_KW'] = merged['TEJ'] + '_' + merged['KW'] + '_' + merged['LAST']\nprint(merged.head())\nprint(merged.tail())\n\n\n   TEJ    ID     LAST     UID       TEJ_Code   KW      TEJ_Code_KW\n0  TEJ  64.0     DGHM  289079    TEJ_34_DGHM  ghi     TEJ_ghi_DGHM\n1  TEJ  75.0    WESGV  237266   TEJ_48_AERGH  stu    TEJ_stu_WESGV\n2  TEJ  61.0    SATRU   26111   TEJ_72_ADFRH  ijk    TEJ_ijk_SATRU\n3  TEJ  35.0     AERH  260015  TEJ_60_ZDGTJH  lmn     TEJ_lmn_AERH\n4  TEJ  51.0  AQEWRGT   59990    TEJ_75_AE5Y  vwx  TEJ_vwx_AQEWRGT\n     TEJ    ID     LAST     UID          TEJ_Code   KW      TEJ_Code_KW\n994  TEJ  73.0  ASFDREH   49161      TEJ_29_ASFDG  pqr  TEJ_pqr_ASFDREH\n995  TEJ  75.0    DCGYJ  237266      TEJ_48_AERGH  stu    TEJ_stu_DCGYJ\n996  TEJ  39.0     SDFH   50976  TEJ_80_DFGSAEFDG  vwx     TEJ_vwx_SDFH\n997  TEJ  32.0    ADFRH   84768       TEJ_39_AE5Y  cde    TEJ_cde_ADFRH\n998  TEJ  77.0     SADF  156696      TEJ_66_ASRTU  ghi     TEJ_ghi_SADF"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome! I will be sharing some blog posts on various topics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A few notes",
    "section": "",
    "text": "Pepkor Assessment\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRefinitiv Assessment\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPepkor Assessment Part 2\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nREACH / IMPACT Initiatives Assessment\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGiveDirectly Assessment\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/Assessment_Pepkor_Data_&_Analytics.html",
    "href": "posts/post-with-code/Assessment_Pepkor_Data_&_Analytics.html",
    "title": "Pepkor Assessment",
    "section": "",
    "text": "Assessment Pepkor Data & Analytics¶\n\nThis is an assessment I did for Pepkor in May 2019. Part 2 is available here. Data for the assessment can be found here.\n\nQuestion 1¶\n\n\nPlease see attached list of birthdays for a certain company (birthdays.txt). Do the values supplied make sense? Why/why not? Are there any interesting things in the data? If so, what are they and how do you think they happened?\n\n\nWe start by importing the necessary Python libraries\n\n\n\nimport calendar\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport sqlite3\nimport csv\n%matplotlib inline\n\n\n/tmp/ipykernel_18256/1689879818.py:3: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\n\nThen we load in the data and begin our investigation\n\n\n\np = Path.home() / 'pepkor-test'\nbdays = pd.read_csv(p / 'data' / 'birthdays.txt')\n# convert date columns to date format\nbdays['date of birth'] = pd.to_datetime(bdays['date of birth'].str.replace('_', '-'))\nbdays['Date'] = pd.to_datetime(bdays['Date'])\n\n\n\n\nbdays['Date'].equals(bdays['date of birth'])\n\n\nTrue\n\n\n\nThe Date column and date of birth column are equal, so we drop one\n\n\n\nbdays.drop(columns='Date', inplace=True)\n\n\n\nOne unusual thing is that there are roughly 62 percent of people older than 65. This suggests that this is historic company data rather than more recent data.\n\n\n\nretirement = datetime.datetime.now().year - 65\n(bdays['date of birth'].dt.year &lt; retirement).sum() / len(bdays)\n\n\n0.6806198430267659\n\n\n\nNext we check the plot of birth months. February has the fewest number of people\n\n\n\nbdays.groupby([bdays['date of birth'].dt.month]).count()['number of people'].plot(kind='bar')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNext we plot the birth years. Between 1910 and 1982, the number of birthdays for the year was 365, suggesting the company had roughly the same number of employees in that period.\n\n\n\nbdays.groupby([bdays['date of birth'].dt.year]).count()['number of people'].plot(kind='bar')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNext we check the distribution of people with the same birthday, both with a histogram and boxplot. There is a positive skew in the distribution.\n\n\n\n_, bins = np.histogram(bdays['number of people'], bins='fd')\nplt.hist(bdays['number of people'], bins=bins)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nplt.cla(); plt.clf()\nbdays.boxplot('number of people')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAn interesting thing with the outliers is that the day and month are the same in 193 of 295 (65 %) cases.\n\n\nThere may have been something wrong with the capturing of data.\n\n\n\noutliers = bdays[bdays['number of people'] &gt; int(bdays.quantile(q=0.99).values[1])]\n(outliers['date of birth'].dt.day == outliers['date of birth'].dt.month).sum()\n\n\n193\n\n\n\nQuestion 2¶\n\n\nSay you had a very precise balance scale as in the picture below. Say you also had 8 balls that are exactly the same size and colour, except 1 of them is slightly heavier. By weighing the balls on the balance scale, how many weighs would it take to determine the heavier one? What would be the minimum required weighs to find the heavier one? How would you guarantee that each time you did this exercise, you would only use the minimum number of weighs? Explain your reasoning.\n\n\nThe solution is to take two groups of 3 balls and one group of 2. On the first weigh, put 3 balls on each side. If the scale balances, the heavier ball is in the remaining group of 2. In the second weigh, put one ball on each side. You will then see the heavier ball.\n\n\nIf the scale doesn’t balance after the first weigh, it means that you can discard the lighter group. You are now left with 3 balls. On the second weigh, put the one ball on each side. If the scale balances, you know it is the third ball that wasn’t weighed. Otherwise you will identify the ball.\n\n\nQuestion 3¶\n\n\nThis question is meant to test basic SQL skills and data manipulation. Please see attached tables of data (survey_data.txt and survey_keys.txt). The data represents results of a survey asking people their age, gender and education. See example below.\n\n\n\n\n\nid\n\n\nageband\n\n\ngender\n\n\neducation\n\n\n\n\n\n\n\n\n1\n\n\n3\n\n\n6\n\n\n7\n\n\n\n\n\n\n2\n\n\n1\n\n\n6\n\n\n7\n\n\n\n\n\n\n3\n\n\n2\n\n\n5\n\n\n7\n\n\n\n\n\n\n4\n\n\n3\n\n\n6\n\n\n10\n\n\n\n\n\n\n5\n\n\n2\n\n\n5\n\n\n10\n\n\n\n\n\n\n6\n\n\n3\n\n\n6\n\n\n9\n\n\n\n\n\n\n\n\n\n\nlookup\n\n\nDescr\n\n\n\n\n\n\n1\n\n\n20-29\n\n\n\n\n2\n\n\n30-39\n\n\n\n\n3\n\n\n40-49\n\n\n\n\n4\n\n\n50-59\n\n\n\n\n5\n\n\nmale\n\n\n\n\n6\n\n\nfemale\n\n\n\n\n\nWrite a SQL statement that produces the following summary for ageband:\n\n\n\n\n\nageband\n\n\ncount\n\n\nid’s\n\n\n\n\n\n\n20-29\n\n\n18\n\n\n2,11,36,42, …\n\n\n\n\n30-39\n\n\n24\n\n\n3,5,10,12, …\n\n\n\n\n40-49\n\n\n26\n\n\n1,4,6,9,13, …\n\n\n\n\n50-59\n\n\n32\n\n\n7,8,15,16, …\n\n\n\n\n\nSimilarly give the SQL statement for gender and education. How would you alter your solution so that if there were many more questions, you wouldn’t have to change much?\n\n\nStart by setting up a sqlite db in Python. The schema is defined as\n\n\nCREATE TABLE IF NOT EXISTS survey_data (\nid int,\nageband int,\ngender int,\neducation int\n);\n\nCREATE TABLE IF NOT EXISTS survey_key (\nlookup int,\ndescr text\n);\n\n\n\n\nconn = sqlite3.connect('data/my.db')\nc = conn.cursor()\nwith open(p / 'sql'/ 'sql_schema.sql') as f:\n    schema = f.read()\nc.executescript(schema)    \nconn.commit()\n\n\n\nThen populate the tables. Define a function that will make read in the text data\n\n\n\ndef populate_table(data: str):\n    csv_reader = csv.reader(open(data), delimiter='|',skipinitialspace=True)\n    next(csv_reader)\n    return tuple(line for line in csv_reader)\n\n\n\n\nto_survey_data = populate_table(p / 'data' / 'survey_data.txt')    \nc.executemany('INSERT INTO survey_data VALUES (?, ?, ?, ?);', to_survey_data)\n\n\n&lt;sqlite3.Cursor at 0x7f3fc76b0640&gt;\n\n\n\n\nto_survey_key = populate_table(p / 'data' / 'survey_key.txt')\nc.executemany('INSERT INTO survey_key VALUES (?, ?);', to_survey_key)\nconn.commit()\n\n\n\nWe check that all went as expected\n\n\n\nsurvey_data = pd.read_sql('SELECT * from survey_data limit 5', conn)\nsurvey_key = pd.read_sql('SELECT * from survey_key limit 5', conn)\nprint(survey_data)\nprint(survey_key)\n\n\n   id  ageband  gender  education\n0   1        3       6          7\n1   2        1       6          7\n2   3        2       5          7\n3   4        3       6         10\n4   5        2       5         10\n   lookup  descr\n0       1  20-29\n1       2  30-39\n2       3  40-49\n3       4  50-59\n4       5   male\n\n\n\nTo calculate the ageband summary, we use the following query\n\n\n\nquery = \"\"\"\nSELECT descr as ageband, count(*), group_concat(id, ',') from\n(SELECT * FROM survey_data \nLEFT JOIN survey_key ON survey_data.ageband = survey_key.lookup) \ngroup by ageband\"\"\"\n\ndf = pd.read_sql(query, conn)\nprint(df)\n\n\n  ageband  count(*)                              group_concat(id, ',')\n0   20-29       288  2,2,2,2,11,11,11,11,36,36,36,36,42,42,42,42,44...\n1   30-39       384  3,3,3,3,5,5,5,5,10,10,10,10,12,12,12,12,19,19,...\n2   40-49       416  1,1,1,1,4,4,4,4,6,6,6,6,9,9,9,9,13,13,13,13,14...\n3   50-59       512  7,7,7,7,8,8,8,8,15,15,15,15,16,16,16,16,17,17,...\n\n\n\nFor gender we use\n\n\n\nquery_gender = \"\"\"\nSELECT descr as gender, count(*), group_concat(id, ',') from\n(SELECT * FROM survey_data \nLEFT JOIN survey_key ON survey_data.gender = survey_key.lookup) \ngroup by gender\"\"\"\ndf_gender = pd.read_sql(query_gender, conn)\nprint(df_gender)\n\n\n   gender  count(*)                              group_concat(id, ',')\n0    male       736  3,3,3,3,5,5,5,5,7,7,7,7,11,11,11,11,14,14,14,1...\n1  female       864  1,1,1,1,2,2,2,2,4,4,4,4,6,6,6,6,8,8,8,8,9,9,9,...\n\n\n\nAnd education we use\n\n\n\nquery_education = \"\"\"\nSELECT descr as education, count(*), group_concat(id, ',') from\n(SELECT * FROM survey_data \nLEFT JOIN survey_key ON survey_data.education = survey_key.lookup) \ngroup by education \"\"\"\ndf_education = pd.read_sql(query_education, conn)\nprint(df_education)\n\n\n   education  count(*)                              group_concat(id, ',')\n0       none       368  1,1,1,1,2,2,2,2,3,3,3,3,13,13,13,13,14,14,14,1...\n1    primary       384  11,11,11,11,12,12,12,12,16,16,16,16,20,20,20,2...\n2  secondary       448  6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,15,15,15,15,24...\n3   tertiary       400  4,4,4,4,5,5,5,5,10,10,10,10,18,18,18,18,19,19,...\n\n\n\nTo generalize this query, it would be helpful to define a function that takes the column of interest as a parameter. Something such as\n\n\nCREATE FUNCTION survey_counts (@column int)\nRETURNS TABLE\nAS \nRETURN\n(\nSELECT descr as @column, count(*), group_concat(id, ',') from\n(SELECT * FROM survey_data \nLEFT JOIN survey_key ON survey_data.@column = survey_key.lookup) \ngroup by @column)\n\n\n\nQuestion 4¶\n\n\nWrite a python function that take in an input, a year, and outputs whether the year is a leap year. e.g.\n\n\n&gt;&gt; def is_leap_year(year):\n    #some processing\n\n&gt;&gt; is_leap_year(1900)\n&gt;&gt; False\n\n\n\nA leap year is defined by 3 criteria:\n\n\n\nThe year can be evenly divided by 4, is a leap year, unless:\n\n\nThe year can be evenly divided by 100, it is NOT a leap year, unless:\n\n\nThe year is also evenly divisible by 400. Then it is a leap year.\n\n\n\nWhat tests would you perform when evaluating your function?\n\n\n\ndef is_leap_year(year: int):\n    assert isinstance(year, int) or isinstance(year, float), 'Must be numeric'\n    if year % 4 != 0:\n        return False\n    elif year % 100 !=0:\n        return True\n    elif year % 400 !=0:\n        return False\n    else:\n        return True\n\n\n\nThe major test would be comparing my function with the built in function in the calendar library\n\n\n\nfor year in range(1500, 2000):\n    if not (calendar.isleap(year) == is_leap_year(year)):\n        raise Exception('Disagreement for year {year}')\n\n\n\nQuestion 5¶\n\n\nValidation of South African ID numbers is a very important task. The check can be found here https://www.westerncape.gov.za/general-publication/decoding-your-south-african-id-number-0 . Write a SQL statement that does the various checks OR write a Python function that does the required checks. You may use the number in the above website to check. What do you think would be the possible advantages/disadvantages of using your chosen language over the other? How would you check for a passport number?\n\n\nwe define a function that checks\n\n\n\nthe length of the input is 13 digits\n\n\nThat the first 6 digits are a valid date\n\n\nThat the 11th digit is either 0 or 1\n\n\nthat the 12th digit is an 8\n\n\nthat the 13th digit is calculated by Luhn’s Algorithm\n\n\n\n\ndef sa_id(id_number: int):\n    str_id = str(id_number)\n    assert len(str_id) == 13, 'ID is incorrect number of digits'\n    # Check that date is valid. datetime constructor will throw an error if invalid\n    datetime.datetime(int(str_id[:2]), int(str_id[2:4]), int(str_id[4:6]))\n    # Check citizenship\n    assert str_id[10] == '0' or str_id[10] == '1', 'Invalid Citizenship Code'\n    # Check for the number 8\n    assert str_id[11] == '8', '12th digit must be 8'\n    # Checksum using Luhn's algorithm\n    # Extract digis \n    digits = [int(d) for d in str_id]\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for d in even_digits:\n        # Double each digit and add the digits of the result\n        checksum += sum([int(i) for i in str(d*2)])\n    return checksum % 10 == 0\n\n\n\nWe check this with a few sample ID numbers\n\n\n\nid_list = [8306045800086, 8306044800087, 7302015800082]\nfor id_no in id_list:\n    print(sa_id(id_no))\n\nsa_id(7302015800682)\n\n\nTrue\nTrue\nTrue\n\n\nAssertionError: Invalid Citizenship Code\n\n\n\nFor passport validation, the numbering format is a letter followed by a 8 digits (http://www.e4.co.za/downloads/passport.pdf).\n\n\n\ndef sa_passport(id_number: str):\n    str_id = str(id_number)\n    assert len(str_id) == 9, 'Passport number must be 9 digits'\n    assert not str_id[0].isdigit(), 'First element must be a character'\n    # Check that the remaining 8 elements are numbers\n    assert str_id[1:].isdigit(), 'The remaining 8 elements must be digits'\n    return True\n\n\n\nUsing Python would make the code for validating the ID numbers more readable. Whereas SQL syntax would be longer, but better for large scale validation."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html",
    "href": "posts/post-with-code/REACH_HR_TEST.html",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "This was an assessment I did for REACH / IMPACT initiatives in December 2023. I was given a 2.5 hour time limit. The data for the exercise can be found here.\n\n\nThis test includes three parts, 1. General Knowledge, 2. Data Processing and 3. Data Project. Use the spreadsheets Annex 1 to help you answer the questions below.\nThe test has been designed to take 2:30 hours and examines several competencies regarding data analysis. Read all questions before you begin and note that the three parts can be completed in any order. When sending back your answers, please share all scripts, code, files etc. that you used to solve the exercises. Please also list all websites/external sources you used to answer the questions. Using generative AI to answer the questions is not permitted. Checks on the use of AI will be performed.\nAll answers can be noted directly on this answer sheet unless otherwise specified. Please return this document with you answers, together with Annex 1 by email.\n\n\n\n\nExplain p-values in layman terms. Feel free to use analogies or examples. Keep it simple, but make sure to stay technically accurate.\n\nWhy are they important?\n\nP-values given a measure of how precise an estimate is. It tells us how likely we are to see a value at least as large under the assumption that the null hypothesis is true. In the social sciences, a p-value less than or equal to 0.05 is considered to be a statistically significant result.\n\nHow can they be interpreted?\n\nThey can be interpreted as the probability of seeing a value at least as large for the coefficient under study given that the null hypothesis is true i.e. no effect. This gives some indication that the results are just noise.\n\nWhat are some common pitfalls/misunderstandings in their use and interpretation?\n\nCommon misunderstandings of p-values are that they are the probability that the null-hypothesis is true or that the alternative hypothesis is true. A common misuse of p-values comes up when dealing with researcher degrees of freedom. Performing multiple comparisons of the data can increase the probability of encountering a false positive. There are also issues such as deciding to average certain groups, exclude other observations, and choosing regression predictors that affect the validity of a p-value (See Gelman).\nWhen would a Mosaic plot be an appropriate visualization?\n\nA Mosaic plot is good for showing percentages of data in groups. It is a graphical representation of a contingency table. This would be useful for showing the effect of an intervention for treatment and control groups, for example. See here for more.\n\nWhat is personally identifiable information (PII)? Provide an example. When is it ok to collect PII?\n\nPersonally identifiable information is sensitive information that can be used to track down or know the identify of a person. This could be someone’s name, phone number, email, or ID number. This is generally something that should be kept safe. Information should not leave the office nor should it be discussed with unauthorized parties. It could be appropriate when signing up for services where PII is mandated or for following up with a customer.\n\n\n\nIn the spreadsheet Annex 1, you will find a raw dataset from a recent data collection exercise that was carried out by your team.\n\nThere are errors in in the dataset. Please identify at least four errors by highlighting them in yellow in the excel sheets. In the cleaning log tab, report the cell IDs, variable name and a small explanation on why you think this value can be an error in the comment column.\n\nSee the spreadsheet available here.\n\nUsing the programming language python, create a new variable characterizing the household drinking water source into improved / unimproved source following the classification below. Paste the code / function you used below.\n\nTable 1: Unimproved / Improved drinking water source categorization\n\n\n\ndrinking_water_source\nImproved water source\n\n\n\n\nProtected dug well\nImproved water source\n\n\nPiped water to yard or plot\nImproved water source\n\n\nPiped water into dwelling (house)\nImproved water source\n\n\nBottled water\nImproved water source\n\n\nTube well or borehole\nImproved water source\n\n\nPublic tap or standpipe\nImproved water source\n\n\nProtected spring\nImproved water source\n\n\nOther\nNA\n\n\n\nStart by importing the relevant libraries.\n\n# Uncomment these lines and run them if you do not have the required packages installed. If you don't want \n# use a virtual environment, leave the first command commented and uncomment the last two.\n\n#!python -m venv .venv\n#!pip install --upgrade pip\n#!pip install -r requirements.txt\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom pathlib import Path\n\nRead the data\n\nDATA_PATH = Path('data')\n\ndf = pd.read_csv(DATA_PATH / 'REACH_HR_TEST_DATA_analyst.docx-EmbeddedFile.xlsm - Annex  1 - REACH Assessment Tes.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nInterviewID\ndata_collection_round\nMarital status - Head of Household\nsingle_headed_household\nNumber household member boy under5 years old\nNumber household member _girl_under5 years old\nNumber household member boy_5_17 years old\nhousehold_girl_5_17\nnumber adult household members years old\nTotal household number\n...\nHouseholds use bottled water as drinking water source\nHousehold treating water\nImprovedsanitationfacility\nMentionedafterdefecating\nMentionedbeforeeating\nMentionedbeforeeatingafterdefecating\nMentionedbeforefeedingchild\nhandwashingfull\nHousehold praticing open defecation\nFrequency respondant report handwhashing a day\n\n\n\n\n0\nbaseline1\nBaseline\nWidowed\nYes\n1\n1\n2\n2\n4\n10\n...\nOther source\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n1\nbaseline10\nBaseline\nMarried\nNo\n0\n0\n1\n0\n1\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n7 times and more\n\n\n2\nbaseline100\nBaseline\nMarried\nNo\n0\n1\n1\n1\n3\n6\n...\nOther source\nYes\nImproved toilet facility\nNo\nYes\nNo\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n3\nbaseline1000\nBaseline\nMarried\nNo\n0\n0\n0\n0\n2\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n4\nbaseline1001\nBaseline\nMarried\nNo\n0\n0\n3\n1\n5\n9\n...\nBottled water\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n\n\n5 rows × 24 columns\n\n\n\nCheck for strange values\n\nfor col in df.columns:\n    print(df[col].value_counts())\n\nInterviewID\nuuid:21e50d02-d5a9-48ed-89dc-5312858a1239    2\nbaseline1                                    1\nuuid:185510ee-8add-4bef-a40f-0149083941ba    1\nuuid:1993bcf6-c7bf-4d14-80ae-19c63c090dc5    1\nuuid:19863d7f-9da4-4ea7-ab40-5dfb98d262a2    1\n                                            ..\nbaseline2443                                 1\nbaseline2442                                 1\nbaseline2441                                 1\nbaseline2440                                 1\nuuid:ffd4a47d-c096-494d-b166-db98d4e87446    1\nName: count, Length: 4818, dtype: int64\ndata_collection_round\nBaseline    3025\nEnd-line    1794\nName: count, dtype: int64\nMarital status - Head of Household\nMarried                                    4395\nWidowed                                     296\nDivorced                                     60\nSingle                                       56\nSeperated_above_18_acting_as_caregiver_      12\nName: count, dtype: int64\nsingle_headed_household\nNo     4391\nYes     428\nName: count, dtype: int64\nNumber household member boy under5 years old\n0    3551\n1    1025\n2     209\n3      31\n4       2\n5       1\nName: count, dtype: int64\nNumber household member _girl_under5 years old\n0    3630\n1     961\n2     209\n3      16\n4       3\nName: count, dtype: int64\nNumber household member boy_5_17 years old\n0    2238\n1    1466\n2     806\n3     211\n4      78\n5      17\n6       2\n7       1\nName: count, dtype: int64\nhousehold_girl_5_17\n0    2453\n1    1480\n2     636\n3     201\n4      43\n5       3\n7       2\n6       1\nName: count, dtype: int64\nnumber adult household members years old\n 2     2403\n 3     1015\n 4      605\n 5      292\n 1      290\n 6      121\n 7       54\n 8       24\n 9        8\n 10       5\n 11       1\n-5        1\nName: count, dtype: int64\nTotal household number\n4        956\n5        923\n3        697\n6        689\n7        465\n2        422\n8        256\n9        154\n1         90\n10        72\n11        34\n12        29\n14        13\n13        12\n15         3\n0          2\n17         1\n12156      1\nName: count, dtype: int64\ndiarrhea_under_5\nNo     1804\nYes     194\nName: count, dtype: int64\nhouse_type\nTimber frame           2201\nTimber and concrete    1124\nHut                     814\nConcrete                524\nMakeshift shelter       155\nName: count, dtype: int64\ndrinking_water_source\nTube well or borehole                1340\nBottled water                         885\nPiped water to yard or plot           619\nPublic tap or standpipe               610\nPiped water into dwelling (house)     453\nProtected dug well                    367\nProtected spring                      219\nUnprotected dug well                  155\nUnprotected spring                     73\nRainwater collection                   35\nTanker-truck                           20\nSurface water                          18\nOther                                  10\nCart with small tank or drum            6\nName: count, dtype: int64\ndrinking water source other\nBeer                  3\nwell well             1\nfiltered water        1\ndon’t know            1\npiped water           1\nrain water            1\nfrom people           1\nAstqrad mn aljeran    1\nName: count, dtype: int64\nHouseholds use bottled water as drinking water source\nOther source     3925\nBottled water     885\nName: count, dtype: int64\nHousehold treating water\nNo     3045\nYes    1770\nName: count, dtype: int64\nImprovedsanitationfacility\nImproved toilet facility      4153\nUnimproved toilet facility     611\nName: count, dtype: int64\nMentionedafterdefecating\nYes    3314\nNo     1500\nName: count, dtype: int64\nMentionedbeforeeating\nYes    4605\nNo      209\nName: count, dtype: int64\nMentionedbeforeeatingafterdefecating\nYes    3244\nNo     1570\nName: count, dtype: int64\nMentionedbeforefeedingchild\nNo     4085\nYes     729\nName: count, dtype: int64\nhandwashingfull\nHandwashing facility with Water & Soap          3906\nNo Handwashing facility                          448\nHandwashing facility with Water without Soap     245\nHandwashing facility without Water and Soap      220\nName: count, dtype: int64\nHousehold praticing open defecation\nNo open defecation    3909\nOpen defecation        880\nName: count, dtype: int64\nFrequency respondant report handwhashing a day\n7 times and more    1803\n5 - 6 times         1642\n3 - 4 times         1298\n0 - 2 times           76\nName: count, dtype: int64\n\n\nCreate a new variable using the mapping from Table 1.\n\nimproved_list = ['Protected dug well', 'Piped water to yard or plot', 'Piped water into dwelling (house)',\n                 'Bottled Water', 'Tube well or borehold', 'Public tap or standpipe', 'Protected spring']\n\n\ndf['improved_water_source'] = ''\ndf.loc[df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Improved water source'\ndf.loc[~df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Unimproved water source'\ndf.loc[df['drinking_water_source'] == 'Other', 'improved_water_source'] = np.nan\n\n\nThis exercise requires the results of the previous exercise. Use any tools, statistics and visualizations that you see fit to analyze the questions below regarding how access to improved water sources changed between the baseline (first data collection round) and the endline (second data collection round, after a water improvement project has been implemented). Records for both rounds are in the same dataset; and come from randomly sampled households in the area of intervention. The column “data_collection_round” is “Baseline” for records of the first round, and “Endline” for records from the second round. Please share all code/files used for the analysis.\n\nDid single headed households receive more/less improvements? (relevant data column: “single_headed_household”)\n\n\n\ndf_single = df.loc[df['single_headed_household'] == 'Yes']\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False)\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False, normalize=True)\nmosaic(df_single, ['data_collection_round', 'improved_water_source'], title='Single headed household')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSingle headed households received fewer improvements in endline than in baseline. There were 137 improved water sources in baseline (50.7% of all water sources) but only 68 improved water sources in endline (43% of all water sources)\n\nDid the improvements affect cases of diarrhea in children under 5? (relevant data column: “diarrhea_under_5”)\n\n\ndf_improved = df.loc[df['improved_water_source'] == 'Improved water source']\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False)\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False, normalize=True)\nmosaic(df_improved, ['data_collection_round', 'diarrhea_under_5'], title='Diarrhea under 5 among households with improved water sources')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nYes, diarrhea in children under 5 went down from 72 cases in baseline (4.9%) to 31 cases in endline (3.8%).\n\n\n\nInstructions\nDescribe in detail the various stages involved in successfully completing a complex data project. You are supporting a large household survey data collection in KoboToolbox. As part of this project, you want to develop an algorithm that would identify suspicious behavior of enumerator during data collection.\nYou have it at your disposal. - the raw data from ODK / KoboToolbox, - the clean data - the cleaning log in which cleaning operations are logged (see example in excel Annex 1), - the audit files from ODK / KoboToolbox that logs enumerator behavior during interview administering the interview. Form Audit Log - ODK Docs getodk.org\nWe want to identify if there are suspicious surveys (fake interviews for example) in the data collection that is coming: - What are the different stages involved in carrying out such a project?\n\nDownload audit.csv from ODK\nWe could first look at the distribution of the length of interviews. Interviews that are suspiciously short or in the bottom 5% of times taken would be marked for further investigation.\nWe would also look at the location data to compare it with where the respondent is supposed to be. Assuming that the interview was conducted at the respondent’s home, data that was filled in outside of this would be suspicious.\nWe would also look at the distribution of the number of changes to response. Enumerators with an unusually high number could be investigated further, perhaps by performing a text analysis of their reasons for changes.\nAnother way of checking for fake surveys is by conducting back checks. A random 10% of households could be selected to be re-interviewed by a different enumerator. If the survey responses differ markedly from the original survey, this could be evidence of a fraudulent survey.\n\nFor each stage, detail the tasks to be carried out, the methods to be followed, possible avenues and critical points. - Thoroughly justify your choices.\n\nAfter downloading audit.csv, a duration variable could be created that is difference between the end and start times of the survey. The elapsed time is accurate even in the start and end timestamps aren’t as mentioned in Form Audit Log - ODK Docs getodk.org. A histogram of the duration could be be plotted. Surveys with times in the bottom 10%, for example, could be checked to see whether the answers are sensible, how many skipped questions there are, etc. This could indicate whether an enumerator is simply rushing through a survey without regard to skip patterns or filling in responses at random.\nFor the location data, the GPS coordinates of where the survey was meant to be conducted can be checked against the location where it was actually done. The downside of this is that if the device shuts off or location data is not turned on, this would make it hard to know whether the survey was done at the appropriate location\nFor the number of changed responses, we could compute the number of changed responses per survey and plot a histogram of the result. Those in the highest 10%, for example, could be set aside for further investigation. We could look at the reasons for the changes and compare those with surveys that are closer to the median.\nFinally, for the back check portion, we can randomly sample a subset of interview IDs to be re-interviewed. A trusted enumerator, preferably separate from the rest of the team, will then be sent to re-interview those households using the entire questionnaire or only the questions of concern. The answers will be compared to those from the first survey. One way to compare is to use the bcstats tool developed by Innovations for Poverty Action. Check here for more information about back check procedures."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#instructions",
    "href": "posts/post-with-code/REACH_HR_TEST.html#instructions",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "This test includes three parts, 1. General Knowledge, 2. Data Processing and 3. Data Project. Use the spreadsheets Annex 1 to help you answer the questions below.\nThe test has been designed to take 2:30 hours and examines several competencies regarding data analysis. Read all questions before you begin and note that the three parts can be completed in any order. When sending back your answers, please share all scripts, code, files etc. that you used to solve the exercises. Please also list all websites/external sources you used to answer the questions. Using generative AI to answer the questions is not permitted. Checks on the use of AI will be performed.\nAll answers can be noted directly on this answer sheet unless otherwise specified. Please return this document with you answers, together with Annex 1 by email."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#part-1-general-knowledge",
    "href": "posts/post-with-code/REACH_HR_TEST.html#part-1-general-knowledge",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "Explain p-values in layman terms. Feel free to use analogies or examples. Keep it simple, but make sure to stay technically accurate.\n\nWhy are they important?\n\nP-values given a measure of how precise an estimate is. It tells us how likely we are to see a value at least as large under the assumption that the null hypothesis is true. In the social sciences, a p-value less than or equal to 0.05 is considered to be a statistically significant result.\n\nHow can they be interpreted?\n\nThey can be interpreted as the probability of seeing a value at least as large for the coefficient under study given that the null hypothesis is true i.e. no effect. This gives some indication that the results are just noise.\n\nWhat are some common pitfalls/misunderstandings in their use and interpretation?\n\nCommon misunderstandings of p-values are that they are the probability that the null-hypothesis is true or that the alternative hypothesis is true. A common misuse of p-values comes up when dealing with researcher degrees of freedom. Performing multiple comparisons of the data can increase the probability of encountering a false positive. There are also issues such as deciding to average certain groups, exclude other observations, and choosing regression predictors that affect the validity of a p-value (See Gelman).\nWhen would a Mosaic plot be an appropriate visualization?\n\nA Mosaic plot is good for showing percentages of data in groups. It is a graphical representation of a contingency table. This would be useful for showing the effect of an intervention for treatment and control groups, for example. See here for more.\n\nWhat is personally identifiable information (PII)? Provide an example. When is it ok to collect PII?\n\nPersonally identifiable information is sensitive information that can be used to track down or know the identify of a person. This could be someone’s name, phone number, email, or ID number. This is generally something that should be kept safe. Information should not leave the office nor should it be discussed with unauthorized parties. It could be appropriate when signing up for services where PII is mandated or for following up with a customer."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#part-2-data-processing",
    "href": "posts/post-with-code/REACH_HR_TEST.html#part-2-data-processing",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "In the spreadsheet Annex 1, you will find a raw dataset from a recent data collection exercise that was carried out by your team.\n\nThere are errors in in the dataset. Please identify at least four errors by highlighting them in yellow in the excel sheets. In the cleaning log tab, report the cell IDs, variable name and a small explanation on why you think this value can be an error in the comment column.\n\nSee the spreadsheet available here.\n\nUsing the programming language python, create a new variable characterizing the household drinking water source into improved / unimproved source following the classification below. Paste the code / function you used below.\n\nTable 1: Unimproved / Improved drinking water source categorization\n\n\n\ndrinking_water_source\nImproved water source\n\n\n\n\nProtected dug well\nImproved water source\n\n\nPiped water to yard or plot\nImproved water source\n\n\nPiped water into dwelling (house)\nImproved water source\n\n\nBottled water\nImproved water source\n\n\nTube well or borehole\nImproved water source\n\n\nPublic tap or standpipe\nImproved water source\n\n\nProtected spring\nImproved water source\n\n\nOther\nNA\n\n\n\nStart by importing the relevant libraries.\n\n# Uncomment these lines and run them if you do not have the required packages installed. If you don't want \n# use a virtual environment, leave the first command commented and uncomment the last two.\n\n#!python -m venv .venv\n#!pip install --upgrade pip\n#!pip install -r requirements.txt\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom pathlib import Path\n\nRead the data\n\nDATA_PATH = Path('data')\n\ndf = pd.read_csv(DATA_PATH / 'REACH_HR_TEST_DATA_analyst.docx-EmbeddedFile.xlsm - Annex  1 - REACH Assessment Tes.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nInterviewID\ndata_collection_round\nMarital status - Head of Household\nsingle_headed_household\nNumber household member boy under5 years old\nNumber household member _girl_under5 years old\nNumber household member boy_5_17 years old\nhousehold_girl_5_17\nnumber adult household members years old\nTotal household number\n...\nHouseholds use bottled water as drinking water source\nHousehold treating water\nImprovedsanitationfacility\nMentionedafterdefecating\nMentionedbeforeeating\nMentionedbeforeeatingafterdefecating\nMentionedbeforefeedingchild\nhandwashingfull\nHousehold praticing open defecation\nFrequency respondant report handwhashing a day\n\n\n\n\n0\nbaseline1\nBaseline\nWidowed\nYes\n1\n1\n2\n2\n4\n10\n...\nOther source\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n1\nbaseline10\nBaseline\nMarried\nNo\n0\n0\n1\n0\n1\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n7 times and more\n\n\n2\nbaseline100\nBaseline\nMarried\nNo\n0\n1\n1\n1\n3\n6\n...\nOther source\nYes\nImproved toilet facility\nNo\nYes\nNo\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n3\nbaseline1000\nBaseline\nMarried\nNo\n0\n0\n0\n0\n2\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n4\nbaseline1001\nBaseline\nMarried\nNo\n0\n0\n3\n1\n5\n9\n...\nBottled water\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n\n\n5 rows × 24 columns\n\n\n\nCheck for strange values\n\nfor col in df.columns:\n    print(df[col].value_counts())\n\nInterviewID\nuuid:21e50d02-d5a9-48ed-89dc-5312858a1239    2\nbaseline1                                    1\nuuid:185510ee-8add-4bef-a40f-0149083941ba    1\nuuid:1993bcf6-c7bf-4d14-80ae-19c63c090dc5    1\nuuid:19863d7f-9da4-4ea7-ab40-5dfb98d262a2    1\n                                            ..\nbaseline2443                                 1\nbaseline2442                                 1\nbaseline2441                                 1\nbaseline2440                                 1\nuuid:ffd4a47d-c096-494d-b166-db98d4e87446    1\nName: count, Length: 4818, dtype: int64\ndata_collection_round\nBaseline    3025\nEnd-line    1794\nName: count, dtype: int64\nMarital status - Head of Household\nMarried                                    4395\nWidowed                                     296\nDivorced                                     60\nSingle                                       56\nSeperated_above_18_acting_as_caregiver_      12\nName: count, dtype: int64\nsingle_headed_household\nNo     4391\nYes     428\nName: count, dtype: int64\nNumber household member boy under5 years old\n0    3551\n1    1025\n2     209\n3      31\n4       2\n5       1\nName: count, dtype: int64\nNumber household member _girl_under5 years old\n0    3630\n1     961\n2     209\n3      16\n4       3\nName: count, dtype: int64\nNumber household member boy_5_17 years old\n0    2238\n1    1466\n2     806\n3     211\n4      78\n5      17\n6       2\n7       1\nName: count, dtype: int64\nhousehold_girl_5_17\n0    2453\n1    1480\n2     636\n3     201\n4      43\n5       3\n7       2\n6       1\nName: count, dtype: int64\nnumber adult household members years old\n 2     2403\n 3     1015\n 4      605\n 5      292\n 1      290\n 6      121\n 7       54\n 8       24\n 9        8\n 10       5\n 11       1\n-5        1\nName: count, dtype: int64\nTotal household number\n4        956\n5        923\n3        697\n6        689\n7        465\n2        422\n8        256\n9        154\n1         90\n10        72\n11        34\n12        29\n14        13\n13        12\n15         3\n0          2\n17         1\n12156      1\nName: count, dtype: int64\ndiarrhea_under_5\nNo     1804\nYes     194\nName: count, dtype: int64\nhouse_type\nTimber frame           2201\nTimber and concrete    1124\nHut                     814\nConcrete                524\nMakeshift shelter       155\nName: count, dtype: int64\ndrinking_water_source\nTube well or borehole                1340\nBottled water                         885\nPiped water to yard or plot           619\nPublic tap or standpipe               610\nPiped water into dwelling (house)     453\nProtected dug well                    367\nProtected spring                      219\nUnprotected dug well                  155\nUnprotected spring                     73\nRainwater collection                   35\nTanker-truck                           20\nSurface water                          18\nOther                                  10\nCart with small tank or drum            6\nName: count, dtype: int64\ndrinking water source other\nBeer                  3\nwell well             1\nfiltered water        1\ndon’t know            1\npiped water           1\nrain water            1\nfrom people           1\nAstqrad mn aljeran    1\nName: count, dtype: int64\nHouseholds use bottled water as drinking water source\nOther source     3925\nBottled water     885\nName: count, dtype: int64\nHousehold treating water\nNo     3045\nYes    1770\nName: count, dtype: int64\nImprovedsanitationfacility\nImproved toilet facility      4153\nUnimproved toilet facility     611\nName: count, dtype: int64\nMentionedafterdefecating\nYes    3314\nNo     1500\nName: count, dtype: int64\nMentionedbeforeeating\nYes    4605\nNo      209\nName: count, dtype: int64\nMentionedbeforeeatingafterdefecating\nYes    3244\nNo     1570\nName: count, dtype: int64\nMentionedbeforefeedingchild\nNo     4085\nYes     729\nName: count, dtype: int64\nhandwashingfull\nHandwashing facility with Water & Soap          3906\nNo Handwashing facility                          448\nHandwashing facility with Water without Soap     245\nHandwashing facility without Water and Soap      220\nName: count, dtype: int64\nHousehold praticing open defecation\nNo open defecation    3909\nOpen defecation        880\nName: count, dtype: int64\nFrequency respondant report handwhashing a day\n7 times and more    1803\n5 - 6 times         1642\n3 - 4 times         1298\n0 - 2 times           76\nName: count, dtype: int64\n\n\nCreate a new variable using the mapping from Table 1.\n\nimproved_list = ['Protected dug well', 'Piped water to yard or plot', 'Piped water into dwelling (house)',\n                 'Bottled Water', 'Tube well or borehold', 'Public tap or standpipe', 'Protected spring']\n\n\ndf['improved_water_source'] = ''\ndf.loc[df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Improved water source'\ndf.loc[~df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Unimproved water source'\ndf.loc[df['drinking_water_source'] == 'Other', 'improved_water_source'] = np.nan\n\n\nThis exercise requires the results of the previous exercise. Use any tools, statistics and visualizations that you see fit to analyze the questions below regarding how access to improved water sources changed between the baseline (first data collection round) and the endline (second data collection round, after a water improvement project has been implemented). Records for both rounds are in the same dataset; and come from randomly sampled households in the area of intervention. The column “data_collection_round” is “Baseline” for records of the first round, and “Endline” for records from the second round. Please share all code/files used for the analysis.\n\nDid single headed households receive more/less improvements? (relevant data column: “single_headed_household”)\n\n\n\ndf_single = df.loc[df['single_headed_household'] == 'Yes']\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False)\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False, normalize=True)\nmosaic(df_single, ['data_collection_round', 'improved_water_source'], title='Single headed household')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSingle headed households received fewer improvements in endline than in baseline. There were 137 improved water sources in baseline (50.7% of all water sources) but only 68 improved water sources in endline (43% of all water sources)\n\nDid the improvements affect cases of diarrhea in children under 5? (relevant data column: “diarrhea_under_5”)\n\n\ndf_improved = df.loc[df['improved_water_source'] == 'Improved water source']\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False)\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False, normalize=True)\nmosaic(df_improved, ['data_collection_round', 'diarrhea_under_5'], title='Diarrhea under 5 among households with improved water sources')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nYes, diarrhea in children under 5 went down from 72 cases in baseline (4.9%) to 31 cases in endline (3.8%)."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#part-3-data-project",
    "href": "posts/post-with-code/REACH_HR_TEST.html#part-3-data-project",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "Instructions\nDescribe in detail the various stages involved in successfully completing a complex data project. You are supporting a large household survey data collection in KoboToolbox. As part of this project, you want to develop an algorithm that would identify suspicious behavior of enumerator during data collection.\nYou have it at your disposal. - the raw data from ODK / KoboToolbox, - the clean data - the cleaning log in which cleaning operations are logged (see example in excel Annex 1), - the audit files from ODK / KoboToolbox that logs enumerator behavior during interview administering the interview. Form Audit Log - ODK Docs getodk.org\nWe want to identify if there are suspicious surveys (fake interviews for example) in the data collection that is coming: - What are the different stages involved in carrying out such a project?\n\nDownload audit.csv from ODK\nWe could first look at the distribution of the length of interviews. Interviews that are suspiciously short or in the bottom 5% of times taken would be marked for further investigation.\nWe would also look at the location data to compare it with where the respondent is supposed to be. Assuming that the interview was conducted at the respondent’s home, data that was filled in outside of this would be suspicious.\nWe would also look at the distribution of the number of changes to response. Enumerators with an unusually high number could be investigated further, perhaps by performing a text analysis of their reasons for changes.\nAnother way of checking for fake surveys is by conducting back checks. A random 10% of households could be selected to be re-interviewed by a different enumerator. If the survey responses differ markedly from the original survey, this could be evidence of a fraudulent survey.\n\nFor each stage, detail the tasks to be carried out, the methods to be followed, possible avenues and critical points. - Thoroughly justify your choices.\n\nAfter downloading audit.csv, a duration variable could be created that is difference between the end and start times of the survey. The elapsed time is accurate even in the start and end timestamps aren’t as mentioned in Form Audit Log - ODK Docs getodk.org. A histogram of the duration could be be plotted. Surveys with times in the bottom 10%, for example, could be checked to see whether the answers are sensible, how many skipped questions there are, etc. This could indicate whether an enumerator is simply rushing through a survey without regard to skip patterns or filling in responses at random.\nFor the location data, the GPS coordinates of where the survey was meant to be conducted can be checked against the location where it was actually done. The downside of this is that if the device shuts off or location data is not turned on, this would make it hard to know whether the survey was done at the appropriate location\nFor the number of changed responses, we could compute the number of changed responses per survey and plot a histogram of the result. Those in the highest 10%, for example, could be set aside for further investigation. We could look at the reasons for the changes and compare those with surveys that are closer to the median.\nFinally, for the back check portion, we can randomly sample a subset of interview IDs to be re-interviewed. A trusted enumerator, preferably separate from the rest of the team, will then be sent to re-interview those households using the entire questionnaire or only the questions of concern. The answers will be compared to those from the first survey. One way to compare is to use the bcstats tool developed by Innovations for Poverty Action. Check here for more information about back check procedures."
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html",
    "href": "posts/post-with-code/give-directly-assessment.html",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "This was a take home assessment I did for GiveDirectly in June 2022. I was given 2.5 hours to complete the assignment. The csv files and instructions can be found here.\n\n\n\nPlease evaluate the data in recipients.csv and survey_attempts.csv to answer the following questions:\n\nHow many recipients are in each of the four stages? Please provide the calculation(s) in the spreadsheet or code that you submit.\nHow many surveys were successfully completed in December, 2020? Please provide the calculation(s) in the spreadsheet or code that you submit.\nDid you find any abnormalities in the source data? If so, how did you account for them in your analysis?\n\n\nFirst install the required packages.\n\n!pip install -r requirements.txt\n\nRequirement already satisfied: pandas==1.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: jupyterlab==3.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: matplotlib==3.5.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.5.2)\nRequirement already satisfied: ipython==8.4.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (8.4.0)\nRequirement already satisfied: flake8==4.0.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (4.0.1)\nRequirement already satisfied: PyQt5==5.15.6 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (5.15.6)\nRequirement already satisfied: statsmodels==0.13.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.13.2)\nRequirement already satisfied: ipykernel==6.13.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (6.13.0)\nRequirement already satisfied: missingno==0.5.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.5.1)\nRequirement already satisfied: scikit-learn==1.1.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: numpy&gt;=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.22.4)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2022.1)\nRequirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3)\nRequirement already satisfied: jupyter-server~=1.16 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: tornado&gt;=6.1.0 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.1)\nRequirement already satisfied: jupyter-core in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.10.0)\nRequirement already satisfied: jinja2&gt;=2.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.1.2)\nRequirement already satisfied: nbclassic~=0.2 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.3.7)\nRequirement already satisfied: jupyterlab-server~=2.10 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.14.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (9.1.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (4.33.3)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (1.4.2)\nRequirement already satisfied: pickleshare in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (3.0.29)\nRequirement already satisfied: traitlets&gt;=5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.2.2.post1)\nRequirement already satisfied: backcall in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pygments&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.12.0)\nRequirement already satisfied: jedi&gt;=0.16 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.18.1)\nRequirement already satisfied: pexpect&gt;4.3; sys_platform != \"win32\" in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (4.8.0)\nRequirement already satisfied: setuptools&gt;=18.5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (44.0.0)\nRequirement already satisfied: decorator in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.1.1)\nRequirement already satisfied: matplotlib-inline in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.1.3)\nRequirement already satisfied: stack-data in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pyflakes&lt;2.5.0,&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.4.0)\nRequirement already satisfied: mccabe&lt;0.7.0,&gt;=0.6.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (0.6.1)\nRequirement already satisfied: pycodestyle&lt;2.9.0,&gt;=2.8.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.8.0)\nRequirement already satisfied: PyQt5-Qt5&gt;=5.15.2 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (5.15.2)\nRequirement already satisfied: PyQt5-sip&lt;13,&gt;=12.8 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (12.10.1)\nRequirement already satisfied: patsy&gt;=0.5.2 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (0.5.2)\nRequirement already satisfied: scipy&gt;=1.3 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (1.8.1)\nRequirement already satisfied: debugpy&gt;=1.0 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.6.0)\nRequirement already satisfied: jupyter-client&gt;=6.1.12 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (7.3.1)\nRequirement already satisfied: nest-asyncio in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.5.5)\nRequirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (5.9.1)\nRequirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (from missingno==0.5.1-&gt;-r requirements.txt (line 9)) (0.11.2)\nRequirement already satisfied: joblib&gt;=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (3.1.0)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: terminado&gt;=0.8.3 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.15.0)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.6.1)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.5.0)\nRequirement already satisfied: Send2Trash in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.8.0)\nRequirement already satisfied: websocket-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.3.2)\nRequirement already satisfied: pyzmq&gt;=17 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (23.1.0)\nRequirement already satisfied: argon2-cffi in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3.0)\nRequirement already satisfied: prometheus-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.14.1)\nRequirement already satisfied: nbformat&gt;=5.2.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.4.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2&gt;=2.1-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.1.1)\nRequirement already satisfied: notebook-shim&gt;=0.1.0 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.1.0)\nRequirement already satisfied: notebook&lt;7 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.4.11)\nRequirement already satisfied: babel in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.10.1)\nRequirement already satisfied: json5 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.9.8)\nRequirement already satisfied: jsonschema&gt;=3.0.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.6.0)\nRequirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.27.1)\nRequirement already satisfied: importlib-metadata&gt;=3.6; python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.4)\nRequirement already satisfied: wcwidth in ./.venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.5)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in ./.venv/lib/python3.8/site-packages (from jedi&gt;=0.16-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != \"win32\"-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.0)\nRequirement already satisfied: pure-eval in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.2)\nRequirement already satisfied: asttokens in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.0.5)\nRequirement already satisfied: executing in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: entrypoints in ./.venv/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.12-&gt;ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (0.4)\nRequirement already satisfied: sniffio&gt;=1.1 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: idna&gt;=2.8 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.3)\nRequirement already satisfied: nbclient&gt;=0.5.0 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.6.4)\nRequirement already satisfied: jupyterlab-pygments in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.2)\nRequirement already satisfied: tinycss2 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.1.1)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.5.0)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.8.4)\nRequirement already satisfied: defusedxml in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.1)\nRequirement already satisfied: bleach in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.0.0)\nRequirement already satisfied: argon2-cffi-bindings in ./.venv/lib/python3.8/site-packages (from argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.2.0)\nRequirement already satisfied: fastjsonschema in ./.venv/lib/python3.8/site-packages (from nbformat&gt;=5.2.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.15.3)\nRequirement already satisfied: ipython-genutils in ./.venv/lib/python3.8/site-packages (from notebook&lt;7-&gt;nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: importlib-resources&gt;=1.4.0; python_version &lt; \"3.9\" in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.7.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2022.5.18.1)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version &gt;= \"3\" in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.0.12)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.26.9)\nRequirement already satisfied: zipp&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata&gt;=3.6; python_version &lt; \"3.10\"-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.8.0)\nRequirement already satisfied: webencodings&gt;=0.4 in ./.venv/lib/python3.8/site-packages (from tinycss2-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.5.1)\nRequirement already satisfied: soupsieve&gt;1.2 in ./.venv/lib/python3.8/site-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.3.2.post1)\nRequirement already satisfied: cffi&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.15.0)\nRequirement already satisfied: pycparser in ./.venv/lib/python3.8/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.21)\n\n\nImport the required libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nmatplotlib.use('QtAgg')\n\nfrom statsmodels.stats.weightstats import CompareMeans\nimport statsmodels.formula.api as smf\nimport missingno as msno\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nLet’s now import the two datasets and merge them.\n\nrecipients = pd.read_csv('recipients.csv')\nattempts = pd.read_csv('survey_attempts.csv')\nmerged = pd.merge(recipients, attempts, on='recipient_id', how='left', indicator=True)\nmerged._merge.value_counts(dropna=False)\n\nboth          520\nleft_only       0\nright_only      0\nName: _merge, dtype: int64\n\n\nIt looks like all the recipients are matched with survey attempts. We can drop the _merge variable.\n\nmerged.drop(columns='_merge', inplace=True)\n\nA quick glance at our data.\n\nmerged.head(10)\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n0\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\n\n\n1\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\n\n\n2\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000151\n12/20/20\nTrue\n\n\n3\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000305\n11/22/20\nFalse\n\n\n4\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000108\n11/28/20\nFalse\n\n\n5\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000116\n01/17/21\nTrue\n\n\n6\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000439\n11/18/20\nFalse\n\n\n7\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000077\n11/06/20\nFalse\n\n\n8\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000130\n01/03/21\nTrue\n\n\n9\nr-00064\nCounty D\n24.0\n26.0\n100000076.0\nActive\ns-000352\n11/04/20\nFalse\n\n\n\n\n\n\n\nCheck for duplicates. We expect that recipient_id and survey_id together form a unique id.\n\nmerged[merged.duplicated(subset=['recipient_id', 'survey_id'], keep=False)]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n15\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n17\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n19\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n22\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n28\nr-00193\nCounty B\n10.0\n62.0\n100000977.0\nActive\ns-000018\n01/22/21\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n506\nr-00191\nCounty B\n10.0\n62.0\n100000126.0\nActive\ns-000026\n12/11/20\nFalse\n\n\n510\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n513\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n518\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n519\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n\n\n100 rows × 9 columns\n\n\n\nThere are 50 pairs of duplicates that need to be dropped.\n\nmerged = merged.drop_duplicates(subset=['recipient_id', 'survey_id'], keep='first').reset_index(drop=True)\n\nLet’s now calculate the stage variable, starting with ‘Start’.\n\nmask = merged.groupby('recipient_id')['success'].any()\none_success = [mask.index[i] for i, m in enumerate(mask) if m]\n# Set the value to start for those with no successful surveys. '~' negates the value \n# of the mask. In this case, '~' means find those without any successful survey\nmerged.loc[~merged.recipient_id.isin(one_success), 'stage'] = 'Start'\n\nNext, the ‘Ineligible’ stage\n\n# Remove the text 'County' from the column\nmerged.county = merged.county.str.replace('County ', '', regex=False)\n    \ninABC = merged.county.isin(['A', 'B', 'C'])\nrecipient_noABC = merged.recipient_id[~inABC]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n    merged.recipient_id.isin(recipient_noABC), 'stage'] = 'Ineligible'\n\nThe ‘Review’ stage\n\nrecipient_yesABC = merged.recipient_id[inABC]\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\nAnd finally the ‘Pay’ stage\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\n\n\n\nmerged.stage.value_counts(dropna=False)\n\nPay           209\nStart         150\nIneligible     70\nReview         35\nNaN             6\nName: stage, dtype: int64\n\n\nThere are 209 in the ‘Pay’ stage, 150 in the ‘Start’, 70 ‘Ineligible’, 35 in ‘Review’, and 6 not in any stage. This is because there are 6 recipients with a missing account_status.\n\nmerged.loc[merged.stage.isna()]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\nstage\n\n\n\n\n0\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\nNaN\n\n\n1\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\nNaN\n\n\n14\nr-00145\nA\n22.0\n24.0\n100000089.0\nNaN\ns-000137\n12/21/20\nTrue\nNaN\n\n\n50\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000161\n11/25/20\nFalse\nNaN\n\n\n51\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000236\n11/15/20\nFalse\nNaN\n\n\n52\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000337\n01/14/21\nTrue\nNaN\n\n\n\n\n\n\n\nWe will impute a value for recipients with a missing account_status, but it could be worthwhile to follow up with someone for the appropriate status.\n\n\n\n\nmerged['month'] = merged['date'].astype(str).str[:2]\nmerged.groupby('month')['success'].sum()\n\nmonth\n01    77\n11     0\n12    93\nName: success, dtype: int64\n\n\nThere were 93 successful surveys in December.\n\n\n\nThere were duplicates in the data that had to be dropped. Some values also did not make much sense and were assgined missing value. The missing data was later imputed.\n\nmerged.describe()\n\n\n\n\n\n\n\n\ntime_county\nage\naccount_number\n\n\n\n\ncount\n455.000000\n460.000000\n4.630000e+02\n\n\nmean\n19.138462\n124.126087\n1.000001e+08\n\n\nstd\n10.501550\n925.986058\n1.074233e+02\n\n\nmin\n-45.000000\n20.000000\n1.000000e+08\n\n\n25%\n10.000000\n25.000000\n1.000001e+08\n\n\n50%\n21.000000\n32.000000\n1.000001e+08\n\n\n75%\n28.000000\n51.000000\n1.000002e+08\n\n\nmax\n35.000000\n9999.000000\n1.000010e+08\n\n\n\n\n\n\n\ntime_county has a minimum of -45 and age has a max of 9999. To deal with these problematic cases, I replace them with missing values to be imputed later. It could also be good to follow up with other teams on the correct value.\n\n\nmerged.age[merged.age == 9999] = pd.np.nan \nmerged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n/tmp/ipykernel_14671/1017566221.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n\nThere are missing values in the data as well. Six columns have missing data.\n\nmerged.isna().sum()\n\nrecipient_id       0\ncounty             8\ntime_county       20\nage               14\naccount_number     7\naccount_status    13\nsurvey_id          0\ndate               0\nsuccess            0\nstage              6\nmonth              0\ndtype: int64\n\n\nMissing data is usually deleted or imputed. Deleting missing data is easiest, but it can lead to biases if the data is not missing at random. If age is more likely to be missing in certain counties, for example, deleting missing data isn’t the best approach. If the data is missing completely at random or missing at random, it can be deleted.\nIf there are many missing values in a row, the row can be deleted. Likewise, if there are many missing values in a column, the column can be deleted.\nImputation can be as simple as replacing the missing value in a column with an arbitrary value such as ‘0’ or the column mean or mode. Other techniques take into account the values in other columns. Suppose a recipient has a missing age, but lives in a certain county and has also responded to the survey. Then the age might be imputed to be slightly lower because of a correlation between success and age or county and age. See more here.\nWe can investigate the missing data with the missingno package.\n\n%matplotlib inline\n\nmsno.matrix(merged)\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_14671/2359591310.py:5: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe white lines represent missing values. The account_number and account_status appear to have coinciding missing values, and the same for age and time_county. We can visualize these relationships with a heatmap.\n\n%matplotlib inline\n\nmsno.heatmap(merged, cmap='rainbow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThere is a strong correlation between account_number and account_status being missing, which would make sense considering that someone without an account_number would not have an account_status and vice versa. There is also a strong association between age and time_county missingness.\nTo investigate this, let’s look at recipients with a missing time_county.\n\ntime_county_missing = merged.loc[merged.time_county.isna()]\nprint(time_county_missing.head())\n\n    recipient_id county  time_county   age  account_number account_status  \\\n69       r-00070      D          NaN  23.0     100000080.0         Active   \n70       r-00070      D          NaN  23.0     100000080.0         Active   \n182      r-00198      B          NaN   NaN     100000006.0         Active   \n183      r-00198      B          NaN   NaN     100000006.0         Active   \n184      r-00198      B          NaN   NaN     100000006.0         Active   \n\n    survey_id      date  success       stage month  \n69   s-000252  12/08/20     True  Ineligible    12  \n70   s-000280  11/10/20    False  Ineligible    11  \n182  s-000284  01/07/21    False       Start    01  \n183  s-000285  12/08/20    False       Start    12  \n184  s-000311  12/18/20    False       Start    12  \n\n\nAnd a missing age\n\nage_missing = merged.loc[merged.age.isna()]\nprint(age_missing.head())\n\n    recipient_id county  time_county  age  account_number account_status  \\\n145      r-00060      B         28.0  NaN     100000018.0     Not Active   \n146      r-00060      B         28.0  NaN     100000018.0     Not Active   \n182      r-00198      B          NaN  NaN     100000006.0         Active   \n183      r-00198      B          NaN  NaN     100000006.0         Active   \n184      r-00198      B          NaN  NaN     100000006.0         Active   \n\n    survey_id      date  success   stage month  \n145  s-000273  11/30/20    False  Review    11  \n146  s-000455  01/29/21     True  Review    01  \n182  s-000284  01/07/21    False   Start    01  \n183  s-000285  12/08/20    False   Start    12  \n184  s-000311  12/18/20    False   Start    12  \n\n\n\nprint(time_county_missing.recipient_id.nunique())\n\n5\n\n\n\nprint(age_missing.recipient_id.nunique())\n\n4\n\n\nSo age is usually missing when time_county is missing. There are five recipients that did not fill in their age and four that did not fill in time_county for multiple survey attempts. They may have been uncomfortable sharing that information.\nDropping these rows could lead to bias because they are not missing at random. Let’s impute the age and time_county for them. We’ll also impute the county and account_status, which are categorical variables. The imputation strategy will regress the column with missing data, say age, on other columns in the data. It will then use the regression coefficients to predict the missing values for age. More details can be found here, here, and here\nStarting with the categorical columns, we’ll prepare the dataset for imputation by converting account_status and county to categorical codes.\n\ncat_cols_na = ['account_status', 'county']\nmerged[cat_cols_na] = merged[cat_cols_na].astype('category')\nd_na = {col: {n: cat for n, cat in enumerate(merged[col].cat.categories)}\n            for col in cat_cols_na}\nmerged[cat_cols_na] = pd.DataFrame(\n    {col: merged[col].cat.codes for col in cat_cols_na},\n    index=merged.index\n)\nprint(merged[cat_cols_na].head())\n\n   account_status  county\n0              -1       1\n1              -1       1\n2               1       2\n3               1       2\n4               0       2\n\n\nNow we can impute the missing values.\n\n\nimp_cat = IterativeImputer(estimator=ExtraTreesClassifier(),\n                            initial_strategy='most_frequent',\n                            max_iter=10, random_state=0, missing_values=-1)\n\n\nmerged[cat_cols_na] = imp_cat.fit_transform(merged[cat_cols_na])\n\nFinally, we convert the numerical codes back to their original labels\n\nfor col in cat_cols_na:\n        merged[col].replace(d_na[col], inplace=True)\nprint(merged[cat_cols_na].head())\n\n  account_status county\n0         Active      B\n1         Active      B\n2     Not Active      C\n3     Not Active      C\n4         Active      C\n\n\nage and time_county are numeric variables, so they do not need a conversion.\n\nnum_cols_na = ['age', 'time_county']\nimp_num = IterativeImputer(estimator=ExtraTreesRegressor(),\n                               initial_strategy='median',\n                               max_iter=10, random_state=0)\nmerged[num_cols_na] = imp_num.fit_transform(merged[num_cols_na])\n\nNow that account_status has been imputed, the stage variable can be recalculated because the six missing cases were missing because account_status was missing at the time of calculation.\n\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\nAnother look at which columns contain missing data shows that there is only the account_number column, which can be ignored for analysis. It also would not be appropriate to impute an identifier.\n\nmerged.isna().sum()\n\nrecipient_id      0\ncounty            0\ntime_county       0\nage               0\naccount_number    7\naccount_status    0\nsurvey_id         0\ndate              0\nsuccess           0\nstage             0\nmonth             0\ndtype: int64\n\n\n\n\n\n\n\nThe program manager has asked for data to help determine whether the field team should focus more effort on calling those in stage Start or following up to resolve issues with those in stage Review. Please write a response to the program manager, including data that may help inform the decision, and some additional factors that you would take into consideration to make the decision. Assume that the program manager’s expertise does not include interpreting data and complex analytics. Please limit your written response to 300 words or less. \n\nIt would not make sense to calculate the chance of someone having a successful survey based on the stage variable because the ‘Start’ group would have no successful surveys, and the other three groups would have all recipients with at least one successful survey. On the other hand, it would be good to know the chances of someone moving from the ‘Review’ stage to a ‘Pay’ stage at a later date, for example.\n\n# Check how many recipients have more than one stage\n(merged.groupby('recipient_id')['stage'].nunique() &gt; 1).sum()\n\n0\n\n\nThere do not appear to be recipients that have changed their stage. Let’s check for recipients that have changed from ‘Not Active’ to ‘Active’ account_status.\n\n(merged.groupby('recipient_id')['account_status'].nunique() &gt; 1).sum()\n\n0\n\n\nNo recipients have changed from ‘Not Active’ to ‘Active’ account_status either.\nOne thing to consider is that the ‘Start’ group has 150 recipients compared to only 35 in the ‘Review’ group. The ‘Start’ group success rate only needs to be 35/150 ~ 23% to match a 100% success rate in the ‘Review’ group. It would be good to look up historical data on the conversion rate from ‘Start’ to ‘Pay’ versus ‘Review’ to ‘Pay’. Data on the cost of converting the ‘Start’ group versus the ‘Review’ group would also be helpful for the decision.\n\nmerged.stage.value_counts(dropna=False)\n\nPay           215\nStart         150\nIneligible     70\nReview         35\nName: stage, dtype: int64\n\n\n\n\n\n\nThe country director is considering investing resources into proactively conducting in-person surveys with recipients in the highest age group across projects to increase overall survey success rate. They believe that this additional cost might outweigh the current costs of repeated failed phone survey attempts, if we can accurately target those recipients least likely to respond to a phone survey. \n\nWhat analysis would you provide from the provided project data to help make this decision? Please provide the calculation(s) in the spreadsheet or code that you submit.\nAre there other factors that might explain the observed survey success rate from this project? Please use your judgment to determine these factors and limit your written response to 400 words or less. \n\n\n\n\nLet’s graph the relationship between age and at least one successful survey.\n\n%matplotlib inline\n\nmerged['one_success'] = 0\nmerged.loc[merged.recipient_id.isin(one_success), 'one_success'] = 1\n\n# Create bins for age broken into quartiles\nmerged['age_bin'] = pd.qcut(merged.age, q=4)\nage_success = merged.groupby('age_bin')['one_success'].sum()\nage = merged.groupby('age_bin')['one_success'].count()\nresults = age_success.div(age, level='age_bin') * 100\n\nresults.plot(kind='bar')\n\nplt.xticks(rotation=0)\nplt.xlabel('Age Group')\nplt.ylabel('% of recipients with at least\\n one successful survey')\nplt.show()\n\n\n\n\n\n\n\n\nPeople in the highest age group are the least likely to have at least one successful survey.\nRunning logistic regression gives\n\nlogit = smf.logit('one_success ~ age', data=merged).fit()\nprint(logit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.278114\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      468\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.5559\nTime:                        13:10:08   Log-Likelihood:                -130.71\nconverged:                       True   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                 3.865e-73\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.8313      0.615     12.744      0.000       6.627       9.036\nage           -0.1740      0.014    -12.308      0.000      -0.202      -0.146\n==============================================================================\n\n\nThere is a negative relationship between age and the chance of at least one successful survey. The coefficient is also statistically significant. To interpret it, we calculate the odds ratio.\n\nodds_ratios = pd.DataFrame({\n        'OR': logit.params, \n        'Lower CI': logit.conf_int()[0],\n        'Upper CI': logit.conf_int()[1]\n    })\n    \nodds_ratios = pd.np.exp(odds_ratios)\nprint(odds_ratios)\nprint(round((odds_ratios['OR'][1] - 1) * 100, 2))\n\n                    OR    Lower CI     Upper CI\nIntercept  2518.096539  755.093254  8397.386877\nage           0.840278    0.817312     0.863888\n-15.97\n\n\n/tmp/ipykernel_14671/4211037893.py:7: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  odds_ratios = pd.np.exp(odds_ratios)\n\n\nEach additional increase of one year in age is associated with a roughly 16 percent decrease in odds of having at least one successful survey.\nSo far, it appears that older recipients are less likely to respond to surveys. Focusing on the older recipients would help target those with lower response rates.\n\n\n\nWe include other variables in our logistic regression to check for confounders.\n\nbig_logit = smf.logit('one_success ~ time_county + age + C(month) + C(account_status)', data=merged).fit()\nprint(big_logit.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.074409\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      464\nMethod:                           MLE   Df Model:                            5\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.8812\nTime:                        13:10:08   Log-Likelihood:                -34.972\nconverged:                      False   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                7.306e-110\n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nIntercept                          10.2072      2.959      3.449      0.001       4.407      16.007\nC(month)[T.11]                     24.6815   8.53e+04      0.000      1.000   -1.67e+05    1.67e+05\nC(month)[T.12]                     -0.3861      0.670     -0.576      0.564      -1.699       0.927\nC(account_status)[T.Not Active]    30.8016   4900.380      0.006      0.995   -9573.767    9635.370\ntime_county                         0.1367      0.069      1.967      0.049       0.001       0.273\nage                                -0.3718      0.093     -4.019      0.000      -0.553      -0.190\n===================================================================================================\n\nPossibly complete quasi-separation: A fraction 0.54 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n/home/dylan/give-directly-exercise/.venv/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nWhen time_county, month, and account_status are included along with age, age and time_county are still significant predictors. We can also look at other feature selection methods.\nLogistic regression can also be used to select features based on their importance in predicting at least one successful survey.\n\nreg = LogisticRegressionCV()\ncat_cols = ['month', 'account_status', 'county']\nnum_cols = ['age', 'time_county']\nd = defaultdict(LabelEncoder)\nle_fit = merged[cat_cols].apply(lambda x: d[x.name].fit_transform(x))\nX = pd.concat((merged[num_cols], le_fit), axis=1)\ny = merged['one_success']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.3, random_state=42\n)\n\nreg.fit(X_train, y_train)\nprint(\"Best accuracy score using built-in LogisticRegCV: %f\" % reg.score(X_test, y_test))\ncoef = pd.Series(reg.coef_.flatten(), index=X.columns)\nimp_coef = coef.sort_values()\n\nimp_coef.plot(kind=\"barh\")\nplt.title(\"Feature importance using Logistic Regression Model\")\nplt.tight_layout()\nplt.show()\n\nBest accuracy score using built-in LogisticRegCV: 0.992908\n\n\n\n\n\n\n\n\n\nIt appears that the account_status and age have the largest effect on whether a survey is successful. age has a negative effect however. Recall that account_status and age are correlated.\n\nmerged.groupby('account_status')['age'].mean()\n\naccount_status\nActive        36.535321\nNot Active    46.908571\nName: age, dtype: float64\n\n\nThe ‘Not Active’ group is slightly older. A test of the equality of group means can confirm this. The null hypothesis is that the means are equal.\n\nactive_age = merged.age[merged.account_status == 'Active']\nnotactive_age = merged.age[merged.account_status == 'Not Active']\nprint(CompareMeans.from_data(active_age, notactive_age).ttest_ind())\n\n(-4.194989773330013, 3.265347532278871e-05, 468.0)\n\n\nThe p-value is close to zero, rejecting the null hypothesis that the means of the two groups are equal. So there is a statistically significant difference between the ages of ‘Active’ and ‘Not Active’ recipients.\nLet’s now use a tree-based classifier for comparison.\n\nreg_extra_tree = ExtraTreesClassifier(n_estimators=10)\nreg_extra_tree.fit(X_train, y_train)\nfeat_imp = pd.Series(\n    reg_extra_tree.feature_importances_,\n    index=X.columns\n).sort_values()\nprint(f\"Mean accuracy on test data is {reg_extra_tree.score(X_test, y_test)}\")\nfeat_imp.plot(kind=\"barh\")\nplt.title(\"Feature importance using Extra Tree Classifier\")\nplt.tight_layout()\nplt.show()\n\nMean accuracy on test data is 1.0\n\n\n\n\n\n\n\n\n\nThis time the model has age as the most important effect, with time_county as a close second. All of the features are chosen by the model, however. Note that there is randomness in the model, so sometimes the features will be ranked differently across different runs. But the model will still choose features important for prediction."
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html#question-1",
    "href": "posts/post-with-code/give-directly-assessment.html#question-1",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "Please evaluate the data in recipients.csv and survey_attempts.csv to answer the following questions:\n\nHow many recipients are in each of the four stages? Please provide the calculation(s) in the spreadsheet or code that you submit.\nHow many surveys were successfully completed in December, 2020? Please provide the calculation(s) in the spreadsheet or code that you submit.\nDid you find any abnormalities in the source data? If so, how did you account for them in your analysis?\n\n\nFirst install the required packages.\n\n!pip install -r requirements.txt\n\nRequirement already satisfied: pandas==1.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: jupyterlab==3.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: matplotlib==3.5.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.5.2)\nRequirement already satisfied: ipython==8.4.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (8.4.0)\nRequirement already satisfied: flake8==4.0.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (4.0.1)\nRequirement already satisfied: PyQt5==5.15.6 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (5.15.6)\nRequirement already satisfied: statsmodels==0.13.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.13.2)\nRequirement already satisfied: ipykernel==6.13.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (6.13.0)\nRequirement already satisfied: missingno==0.5.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.5.1)\nRequirement already satisfied: scikit-learn==1.1.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: numpy&gt;=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.22.4)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2022.1)\nRequirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3)\nRequirement already satisfied: jupyter-server~=1.16 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: tornado&gt;=6.1.0 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.1)\nRequirement already satisfied: jupyter-core in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.10.0)\nRequirement already satisfied: jinja2&gt;=2.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.1.2)\nRequirement already satisfied: nbclassic~=0.2 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.3.7)\nRequirement already satisfied: jupyterlab-server~=2.10 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.14.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (9.1.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (4.33.3)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (1.4.2)\nRequirement already satisfied: pickleshare in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (3.0.29)\nRequirement already satisfied: traitlets&gt;=5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.2.2.post1)\nRequirement already satisfied: backcall in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pygments&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.12.0)\nRequirement already satisfied: jedi&gt;=0.16 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.18.1)\nRequirement already satisfied: pexpect&gt;4.3; sys_platform != \"win32\" in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (4.8.0)\nRequirement already satisfied: setuptools&gt;=18.5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (44.0.0)\nRequirement already satisfied: decorator in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.1.1)\nRequirement already satisfied: matplotlib-inline in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.1.3)\nRequirement already satisfied: stack-data in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pyflakes&lt;2.5.0,&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.4.0)\nRequirement already satisfied: mccabe&lt;0.7.0,&gt;=0.6.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (0.6.1)\nRequirement already satisfied: pycodestyle&lt;2.9.0,&gt;=2.8.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.8.0)\nRequirement already satisfied: PyQt5-Qt5&gt;=5.15.2 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (5.15.2)\nRequirement already satisfied: PyQt5-sip&lt;13,&gt;=12.8 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (12.10.1)\nRequirement already satisfied: patsy&gt;=0.5.2 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (0.5.2)\nRequirement already satisfied: scipy&gt;=1.3 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (1.8.1)\nRequirement already satisfied: debugpy&gt;=1.0 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.6.0)\nRequirement already satisfied: jupyter-client&gt;=6.1.12 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (7.3.1)\nRequirement already satisfied: nest-asyncio in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.5.5)\nRequirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (5.9.1)\nRequirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (from missingno==0.5.1-&gt;-r requirements.txt (line 9)) (0.11.2)\nRequirement already satisfied: joblib&gt;=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (3.1.0)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: terminado&gt;=0.8.3 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.15.0)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.6.1)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.5.0)\nRequirement already satisfied: Send2Trash in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.8.0)\nRequirement already satisfied: websocket-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.3.2)\nRequirement already satisfied: pyzmq&gt;=17 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (23.1.0)\nRequirement already satisfied: argon2-cffi in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3.0)\nRequirement already satisfied: prometheus-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.14.1)\nRequirement already satisfied: nbformat&gt;=5.2.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.4.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2&gt;=2.1-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.1.1)\nRequirement already satisfied: notebook-shim&gt;=0.1.0 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.1.0)\nRequirement already satisfied: notebook&lt;7 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.4.11)\nRequirement already satisfied: babel in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.10.1)\nRequirement already satisfied: json5 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.9.8)\nRequirement already satisfied: jsonschema&gt;=3.0.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.6.0)\nRequirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.27.1)\nRequirement already satisfied: importlib-metadata&gt;=3.6; python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.4)\nRequirement already satisfied: wcwidth in ./.venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.5)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in ./.venv/lib/python3.8/site-packages (from jedi&gt;=0.16-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != \"win32\"-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.0)\nRequirement already satisfied: pure-eval in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.2)\nRequirement already satisfied: asttokens in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.0.5)\nRequirement already satisfied: executing in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: entrypoints in ./.venv/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.12-&gt;ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (0.4)\nRequirement already satisfied: sniffio&gt;=1.1 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: idna&gt;=2.8 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.3)\nRequirement already satisfied: nbclient&gt;=0.5.0 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.6.4)\nRequirement already satisfied: jupyterlab-pygments in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.2)\nRequirement already satisfied: tinycss2 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.1.1)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.5.0)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.8.4)\nRequirement already satisfied: defusedxml in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.1)\nRequirement already satisfied: bleach in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.0.0)\nRequirement already satisfied: argon2-cffi-bindings in ./.venv/lib/python3.8/site-packages (from argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.2.0)\nRequirement already satisfied: fastjsonschema in ./.venv/lib/python3.8/site-packages (from nbformat&gt;=5.2.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.15.3)\nRequirement already satisfied: ipython-genutils in ./.venv/lib/python3.8/site-packages (from notebook&lt;7-&gt;nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: importlib-resources&gt;=1.4.0; python_version &lt; \"3.9\" in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.7.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2022.5.18.1)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version &gt;= \"3\" in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.0.12)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.26.9)\nRequirement already satisfied: zipp&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata&gt;=3.6; python_version &lt; \"3.10\"-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.8.0)\nRequirement already satisfied: webencodings&gt;=0.4 in ./.venv/lib/python3.8/site-packages (from tinycss2-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.5.1)\nRequirement already satisfied: soupsieve&gt;1.2 in ./.venv/lib/python3.8/site-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.3.2.post1)\nRequirement already satisfied: cffi&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.15.0)\nRequirement already satisfied: pycparser in ./.venv/lib/python3.8/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.21)\n\n\nImport the required libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nmatplotlib.use('QtAgg')\n\nfrom statsmodels.stats.weightstats import CompareMeans\nimport statsmodels.formula.api as smf\nimport missingno as msno\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nLet’s now import the two datasets and merge them.\n\nrecipients = pd.read_csv('recipients.csv')\nattempts = pd.read_csv('survey_attempts.csv')\nmerged = pd.merge(recipients, attempts, on='recipient_id', how='left', indicator=True)\nmerged._merge.value_counts(dropna=False)\n\nboth          520\nleft_only       0\nright_only      0\nName: _merge, dtype: int64\n\n\nIt looks like all the recipients are matched with survey attempts. We can drop the _merge variable.\n\nmerged.drop(columns='_merge', inplace=True)\n\nA quick glance at our data.\n\nmerged.head(10)\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n0\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\n\n\n1\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\n\n\n2\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000151\n12/20/20\nTrue\n\n\n3\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000305\n11/22/20\nFalse\n\n\n4\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000108\n11/28/20\nFalse\n\n\n5\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000116\n01/17/21\nTrue\n\n\n6\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000439\n11/18/20\nFalse\n\n\n7\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000077\n11/06/20\nFalse\n\n\n8\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000130\n01/03/21\nTrue\n\n\n9\nr-00064\nCounty D\n24.0\n26.0\n100000076.0\nActive\ns-000352\n11/04/20\nFalse\n\n\n\n\n\n\n\nCheck for duplicates. We expect that recipient_id and survey_id together form a unique id.\n\nmerged[merged.duplicated(subset=['recipient_id', 'survey_id'], keep=False)]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n15\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n17\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n19\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n22\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n28\nr-00193\nCounty B\n10.0\n62.0\n100000977.0\nActive\ns-000018\n01/22/21\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n506\nr-00191\nCounty B\n10.0\n62.0\n100000126.0\nActive\ns-000026\n12/11/20\nFalse\n\n\n510\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n513\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n518\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n519\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n\n\n100 rows × 9 columns\n\n\n\nThere are 50 pairs of duplicates that need to be dropped.\n\nmerged = merged.drop_duplicates(subset=['recipient_id', 'survey_id'], keep='first').reset_index(drop=True)\n\nLet’s now calculate the stage variable, starting with ‘Start’.\n\nmask = merged.groupby('recipient_id')['success'].any()\none_success = [mask.index[i] for i, m in enumerate(mask) if m]\n# Set the value to start for those with no successful surveys. '~' negates the value \n# of the mask. In this case, '~' means find those without any successful survey\nmerged.loc[~merged.recipient_id.isin(one_success), 'stage'] = 'Start'\n\nNext, the ‘Ineligible’ stage\n\n# Remove the text 'County' from the column\nmerged.county = merged.county.str.replace('County ', '', regex=False)\n    \ninABC = merged.county.isin(['A', 'B', 'C'])\nrecipient_noABC = merged.recipient_id[~inABC]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n    merged.recipient_id.isin(recipient_noABC), 'stage'] = 'Ineligible'\n\nThe ‘Review’ stage\n\nrecipient_yesABC = merged.recipient_id[inABC]\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\nAnd finally the ‘Pay’ stage\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\n\n\n\nmerged.stage.value_counts(dropna=False)\n\nPay           209\nStart         150\nIneligible     70\nReview         35\nNaN             6\nName: stage, dtype: int64\n\n\nThere are 209 in the ‘Pay’ stage, 150 in the ‘Start’, 70 ‘Ineligible’, 35 in ‘Review’, and 6 not in any stage. This is because there are 6 recipients with a missing account_status.\n\nmerged.loc[merged.stage.isna()]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\nstage\n\n\n\n\n0\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\nNaN\n\n\n1\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\nNaN\n\n\n14\nr-00145\nA\n22.0\n24.0\n100000089.0\nNaN\ns-000137\n12/21/20\nTrue\nNaN\n\n\n50\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000161\n11/25/20\nFalse\nNaN\n\n\n51\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000236\n11/15/20\nFalse\nNaN\n\n\n52\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000337\n01/14/21\nTrue\nNaN\n\n\n\n\n\n\n\nWe will impute a value for recipients with a missing account_status, but it could be worthwhile to follow up with someone for the appropriate status.\n\n\n\n\nmerged['month'] = merged['date'].astype(str).str[:2]\nmerged.groupby('month')['success'].sum()\n\nmonth\n01    77\n11     0\n12    93\nName: success, dtype: int64\n\n\nThere were 93 successful surveys in December.\n\n\n\nThere were duplicates in the data that had to be dropped. Some values also did not make much sense and were assgined missing value. The missing data was later imputed.\n\nmerged.describe()\n\n\n\n\n\n\n\n\ntime_county\nage\naccount_number\n\n\n\n\ncount\n455.000000\n460.000000\n4.630000e+02\n\n\nmean\n19.138462\n124.126087\n1.000001e+08\n\n\nstd\n10.501550\n925.986058\n1.074233e+02\n\n\nmin\n-45.000000\n20.000000\n1.000000e+08\n\n\n25%\n10.000000\n25.000000\n1.000001e+08\n\n\n50%\n21.000000\n32.000000\n1.000001e+08\n\n\n75%\n28.000000\n51.000000\n1.000002e+08\n\n\nmax\n35.000000\n9999.000000\n1.000010e+08\n\n\n\n\n\n\n\ntime_county has a minimum of -45 and age has a max of 9999. To deal with these problematic cases, I replace them with missing values to be imputed later. It could also be good to follow up with other teams on the correct value.\n\n\nmerged.age[merged.age == 9999] = pd.np.nan \nmerged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n/tmp/ipykernel_14671/1017566221.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n\nThere are missing values in the data as well. Six columns have missing data.\n\nmerged.isna().sum()\n\nrecipient_id       0\ncounty             8\ntime_county       20\nage               14\naccount_number     7\naccount_status    13\nsurvey_id          0\ndate               0\nsuccess            0\nstage              6\nmonth              0\ndtype: int64\n\n\nMissing data is usually deleted or imputed. Deleting missing data is easiest, but it can lead to biases if the data is not missing at random. If age is more likely to be missing in certain counties, for example, deleting missing data isn’t the best approach. If the data is missing completely at random or missing at random, it can be deleted.\nIf there are many missing values in a row, the row can be deleted. Likewise, if there are many missing values in a column, the column can be deleted.\nImputation can be as simple as replacing the missing value in a column with an arbitrary value such as ‘0’ or the column mean or mode. Other techniques take into account the values in other columns. Suppose a recipient has a missing age, but lives in a certain county and has also responded to the survey. Then the age might be imputed to be slightly lower because of a correlation between success and age or county and age. See more here.\nWe can investigate the missing data with the missingno package.\n\n%matplotlib inline\n\nmsno.matrix(merged)\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_14671/2359591310.py:5: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe white lines represent missing values. The account_number and account_status appear to have coinciding missing values, and the same for age and time_county. We can visualize these relationships with a heatmap.\n\n%matplotlib inline\n\nmsno.heatmap(merged, cmap='rainbow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThere is a strong correlation between account_number and account_status being missing, which would make sense considering that someone without an account_number would not have an account_status and vice versa. There is also a strong association between age and time_county missingness.\nTo investigate this, let’s look at recipients with a missing time_county.\n\ntime_county_missing = merged.loc[merged.time_county.isna()]\nprint(time_county_missing.head())\n\n    recipient_id county  time_county   age  account_number account_status  \\\n69       r-00070      D          NaN  23.0     100000080.0         Active   \n70       r-00070      D          NaN  23.0     100000080.0         Active   \n182      r-00198      B          NaN   NaN     100000006.0         Active   \n183      r-00198      B          NaN   NaN     100000006.0         Active   \n184      r-00198      B          NaN   NaN     100000006.0         Active   \n\n    survey_id      date  success       stage month  \n69   s-000252  12/08/20     True  Ineligible    12  \n70   s-000280  11/10/20    False  Ineligible    11  \n182  s-000284  01/07/21    False       Start    01  \n183  s-000285  12/08/20    False       Start    12  \n184  s-000311  12/18/20    False       Start    12  \n\n\nAnd a missing age\n\nage_missing = merged.loc[merged.age.isna()]\nprint(age_missing.head())\n\n    recipient_id county  time_county  age  account_number account_status  \\\n145      r-00060      B         28.0  NaN     100000018.0     Not Active   \n146      r-00060      B         28.0  NaN     100000018.0     Not Active   \n182      r-00198      B          NaN  NaN     100000006.0         Active   \n183      r-00198      B          NaN  NaN     100000006.0         Active   \n184      r-00198      B          NaN  NaN     100000006.0         Active   \n\n    survey_id      date  success   stage month  \n145  s-000273  11/30/20    False  Review    11  \n146  s-000455  01/29/21     True  Review    01  \n182  s-000284  01/07/21    False   Start    01  \n183  s-000285  12/08/20    False   Start    12  \n184  s-000311  12/18/20    False   Start    12  \n\n\n\nprint(time_county_missing.recipient_id.nunique())\n\n5\n\n\n\nprint(age_missing.recipient_id.nunique())\n\n4\n\n\nSo age is usually missing when time_county is missing. There are five recipients that did not fill in their age and four that did not fill in time_county for multiple survey attempts. They may have been uncomfortable sharing that information.\nDropping these rows could lead to bias because they are not missing at random. Let’s impute the age and time_county for them. We’ll also impute the county and account_status, which are categorical variables. The imputation strategy will regress the column with missing data, say age, on other columns in the data. It will then use the regression coefficients to predict the missing values for age. More details can be found here, here, and here\nStarting with the categorical columns, we’ll prepare the dataset for imputation by converting account_status and county to categorical codes.\n\ncat_cols_na = ['account_status', 'county']\nmerged[cat_cols_na] = merged[cat_cols_na].astype('category')\nd_na = {col: {n: cat for n, cat in enumerate(merged[col].cat.categories)}\n            for col in cat_cols_na}\nmerged[cat_cols_na] = pd.DataFrame(\n    {col: merged[col].cat.codes for col in cat_cols_na},\n    index=merged.index\n)\nprint(merged[cat_cols_na].head())\n\n   account_status  county\n0              -1       1\n1              -1       1\n2               1       2\n3               1       2\n4               0       2\n\n\nNow we can impute the missing values.\n\n\nimp_cat = IterativeImputer(estimator=ExtraTreesClassifier(),\n                            initial_strategy='most_frequent',\n                            max_iter=10, random_state=0, missing_values=-1)\n\n\nmerged[cat_cols_na] = imp_cat.fit_transform(merged[cat_cols_na])\n\nFinally, we convert the numerical codes back to their original labels\n\nfor col in cat_cols_na:\n        merged[col].replace(d_na[col], inplace=True)\nprint(merged[cat_cols_na].head())\n\n  account_status county\n0         Active      B\n1         Active      B\n2     Not Active      C\n3     Not Active      C\n4         Active      C\n\n\nage and time_county are numeric variables, so they do not need a conversion.\n\nnum_cols_na = ['age', 'time_county']\nimp_num = IterativeImputer(estimator=ExtraTreesRegressor(),\n                               initial_strategy='median',\n                               max_iter=10, random_state=0)\nmerged[num_cols_na] = imp_num.fit_transform(merged[num_cols_na])\n\nNow that account_status has been imputed, the stage variable can be recalculated because the six missing cases were missing because account_status was missing at the time of calculation.\n\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\nAnother look at which columns contain missing data shows that there is only the account_number column, which can be ignored for analysis. It also would not be appropriate to impute an identifier.\n\nmerged.isna().sum()\n\nrecipient_id      0\ncounty            0\ntime_county       0\nage               0\naccount_number    7\naccount_status    0\nsurvey_id         0\ndate              0\nsuccess           0\nstage             0\nmonth             0\ndtype: int64"
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html#question-2",
    "href": "posts/post-with-code/give-directly-assessment.html#question-2",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "The program manager has asked for data to help determine whether the field team should focus more effort on calling those in stage Start or following up to resolve issues with those in stage Review. Please write a response to the program manager, including data that may help inform the decision, and some additional factors that you would take into consideration to make the decision. Assume that the program manager’s expertise does not include interpreting data and complex analytics. Please limit your written response to 300 words or less. \n\nIt would not make sense to calculate the chance of someone having a successful survey based on the stage variable because the ‘Start’ group would have no successful surveys, and the other three groups would have all recipients with at least one successful survey. On the other hand, it would be good to know the chances of someone moving from the ‘Review’ stage to a ‘Pay’ stage at a later date, for example.\n\n# Check how many recipients have more than one stage\n(merged.groupby('recipient_id')['stage'].nunique() &gt; 1).sum()\n\n0\n\n\nThere do not appear to be recipients that have changed their stage. Let’s check for recipients that have changed from ‘Not Active’ to ‘Active’ account_status.\n\n(merged.groupby('recipient_id')['account_status'].nunique() &gt; 1).sum()\n\n0\n\n\nNo recipients have changed from ‘Not Active’ to ‘Active’ account_status either.\nOne thing to consider is that the ‘Start’ group has 150 recipients compared to only 35 in the ‘Review’ group. The ‘Start’ group success rate only needs to be 35/150 ~ 23% to match a 100% success rate in the ‘Review’ group. It would be good to look up historical data on the conversion rate from ‘Start’ to ‘Pay’ versus ‘Review’ to ‘Pay’. Data on the cost of converting the ‘Start’ group versus the ‘Review’ group would also be helpful for the decision.\n\nmerged.stage.value_counts(dropna=False)\n\nPay           215\nStart         150\nIneligible     70\nReview         35\nName: stage, dtype: int64"
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html#question-3",
    "href": "posts/post-with-code/give-directly-assessment.html#question-3",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "The country director is considering investing resources into proactively conducting in-person surveys with recipients in the highest age group across projects to increase overall survey success rate. They believe that this additional cost might outweigh the current costs of repeated failed phone survey attempts, if we can accurately target those recipients least likely to respond to a phone survey. \n\nWhat analysis would you provide from the provided project data to help make this decision? Please provide the calculation(s) in the spreadsheet or code that you submit.\nAre there other factors that might explain the observed survey success rate from this project? Please use your judgment to determine these factors and limit your written response to 400 words or less. \n\n\n\n\nLet’s graph the relationship between age and at least one successful survey.\n\n%matplotlib inline\n\nmerged['one_success'] = 0\nmerged.loc[merged.recipient_id.isin(one_success), 'one_success'] = 1\n\n# Create bins for age broken into quartiles\nmerged['age_bin'] = pd.qcut(merged.age, q=4)\nage_success = merged.groupby('age_bin')['one_success'].sum()\nage = merged.groupby('age_bin')['one_success'].count()\nresults = age_success.div(age, level='age_bin') * 100\n\nresults.plot(kind='bar')\n\nplt.xticks(rotation=0)\nplt.xlabel('Age Group')\nplt.ylabel('% of recipients with at least\\n one successful survey')\nplt.show()\n\n\n\n\n\n\n\n\nPeople in the highest age group are the least likely to have at least one successful survey.\nRunning logistic regression gives\n\nlogit = smf.logit('one_success ~ age', data=merged).fit()\nprint(logit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.278114\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      468\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.5559\nTime:                        13:10:08   Log-Likelihood:                -130.71\nconverged:                       True   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                 3.865e-73\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.8313      0.615     12.744      0.000       6.627       9.036\nage           -0.1740      0.014    -12.308      0.000      -0.202      -0.146\n==============================================================================\n\n\nThere is a negative relationship between age and the chance of at least one successful survey. The coefficient is also statistically significant. To interpret it, we calculate the odds ratio.\n\nodds_ratios = pd.DataFrame({\n        'OR': logit.params, \n        'Lower CI': logit.conf_int()[0],\n        'Upper CI': logit.conf_int()[1]\n    })\n    \nodds_ratios = pd.np.exp(odds_ratios)\nprint(odds_ratios)\nprint(round((odds_ratios['OR'][1] - 1) * 100, 2))\n\n                    OR    Lower CI     Upper CI\nIntercept  2518.096539  755.093254  8397.386877\nage           0.840278    0.817312     0.863888\n-15.97\n\n\n/tmp/ipykernel_14671/4211037893.py:7: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  odds_ratios = pd.np.exp(odds_ratios)\n\n\nEach additional increase of one year in age is associated with a roughly 16 percent decrease in odds of having at least one successful survey.\nSo far, it appears that older recipients are less likely to respond to surveys. Focusing on the older recipients would help target those with lower response rates.\n\n\n\nWe include other variables in our logistic regression to check for confounders.\n\nbig_logit = smf.logit('one_success ~ time_county + age + C(month) + C(account_status)', data=merged).fit()\nprint(big_logit.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.074409\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      464\nMethod:                           MLE   Df Model:                            5\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.8812\nTime:                        13:10:08   Log-Likelihood:                -34.972\nconverged:                      False   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                7.306e-110\n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nIntercept                          10.2072      2.959      3.449      0.001       4.407      16.007\nC(month)[T.11]                     24.6815   8.53e+04      0.000      1.000   -1.67e+05    1.67e+05\nC(month)[T.12]                     -0.3861      0.670     -0.576      0.564      -1.699       0.927\nC(account_status)[T.Not Active]    30.8016   4900.380      0.006      0.995   -9573.767    9635.370\ntime_county                         0.1367      0.069      1.967      0.049       0.001       0.273\nage                                -0.3718      0.093     -4.019      0.000      -0.553      -0.190\n===================================================================================================\n\nPossibly complete quasi-separation: A fraction 0.54 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n/home/dylan/give-directly-exercise/.venv/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nWhen time_county, month, and account_status are included along with age, age and time_county are still significant predictors. We can also look at other feature selection methods.\nLogistic regression can also be used to select features based on their importance in predicting at least one successful survey.\n\nreg = LogisticRegressionCV()\ncat_cols = ['month', 'account_status', 'county']\nnum_cols = ['age', 'time_county']\nd = defaultdict(LabelEncoder)\nle_fit = merged[cat_cols].apply(lambda x: d[x.name].fit_transform(x))\nX = pd.concat((merged[num_cols], le_fit), axis=1)\ny = merged['one_success']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.3, random_state=42\n)\n\nreg.fit(X_train, y_train)\nprint(\"Best accuracy score using built-in LogisticRegCV: %f\" % reg.score(X_test, y_test))\ncoef = pd.Series(reg.coef_.flatten(), index=X.columns)\nimp_coef = coef.sort_values()\n\nimp_coef.plot(kind=\"barh\")\nplt.title(\"Feature importance using Logistic Regression Model\")\nplt.tight_layout()\nplt.show()\n\nBest accuracy score using built-in LogisticRegCV: 0.992908\n\n\n\n\n\n\n\n\n\nIt appears that the account_status and age have the largest effect on whether a survey is successful. age has a negative effect however. Recall that account_status and age are correlated.\n\nmerged.groupby('account_status')['age'].mean()\n\naccount_status\nActive        36.535321\nNot Active    46.908571\nName: age, dtype: float64\n\n\nThe ‘Not Active’ group is slightly older. A test of the equality of group means can confirm this. The null hypothesis is that the means are equal.\n\nactive_age = merged.age[merged.account_status == 'Active']\nnotactive_age = merged.age[merged.account_status == 'Not Active']\nprint(CompareMeans.from_data(active_age, notactive_age).ttest_ind())\n\n(-4.194989773330013, 3.265347532278871e-05, 468.0)\n\n\nThe p-value is close to zero, rejecting the null hypothesis that the means of the two groups are equal. So there is a statistically significant difference between the ages of ‘Active’ and ‘Not Active’ recipients.\nLet’s now use a tree-based classifier for comparison.\n\nreg_extra_tree = ExtraTreesClassifier(n_estimators=10)\nreg_extra_tree.fit(X_train, y_train)\nfeat_imp = pd.Series(\n    reg_extra_tree.feature_importances_,\n    index=X.columns\n).sort_values()\nprint(f\"Mean accuracy on test data is {reg_extra_tree.score(X_test, y_test)}\")\nfeat_imp.plot(kind=\"barh\")\nplt.title(\"Feature importance using Extra Tree Classifier\")\nplt.tight_layout()\nplt.show()\n\nMean accuracy on test data is 1.0\n\n\n\n\n\n\n\n\n\nThis time the model has age as the most important effect, with time_county as a close second. All of the features are chosen by the model, however. Note that there is randomness in the model, so sometimes the features will be ranked differently across different runs. But the model will still choose features important for prediction."
  }
]