[
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html",
    "href": "posts/post-with-code/REACH_HR_TEST.html",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "This was an assessment I did for REACH / IMPACT initiatives in December 2023. I was given a 2.5 hour time limit. The data for the exercise can be found here.\n\n\nThis test includes three parts, 1. General Knowledge, 2. Data Processing and 3. Data Project. Use the spreadsheets Annex 1 to help you answer the questions below.\nThe test has been designed to take 2:30 hours and examines several competencies regarding data analysis. Read all questions before you begin and note that the three parts can be completed in any order. When sending back your answers, please share all scripts, code, files etc. that you used to solve the exercises. Please also list all websites/external sources you used to answer the questions. Using generative AI to answer the questions is not permitted. Checks on the use of AI will be performed.\nAll answers can be noted directly on this answer sheet unless otherwise specified. Please return this document with you answers, together with Annex 1 by email.\n\n\n\n\nExplain p-values in layman terms. Feel free to use analogies or examples. Keep it simple, but make sure to stay technically accurate.\n\nWhy are they important?\n\nP-values given a measure of how precise an estimate is. It tells us how likely we are to see a value at least as large under the assumption that the null hypothesis is true. In the social sciences, a p-value less than or equal to 0.05 is considered to be a statistically significant result.\n\nHow can they be interpreted?\n\nThey can be interpreted as the probability of seeing a value at least as large for the coefficient under study given that the null hypothesis is true i.e. no effect. This gives some indication that the results are just noise.\n\nWhat are some common pitfalls/misunderstandings in their use and interpretation?\n\nCommon misunderstandings of p-values are that they are the probability that the null-hypothesis is true or that the alternative hypothesis is true. A common misuse of p-values comes up when dealing with researcher degrees of freedom. Performing multiple comparisons of the data can increase the probability of encountering a false positive. There are also issues such as deciding to average certain groups, exclude other observations, and choosing regression predictors that affect the validity of a p-value (See Gelman).\nWhen would a Mosaic plot be an appropriate visualization?\n\nA Mosaic plot is good for showing percentages of data in groups. It is a graphical representation of a contingency table. This would be useful for showing the effect of an intervention for treatment and control groups, for example. See here for more.\n\nWhat is personally identifiable information (PII)? Provide an example. When is it ok to collect PII?\n\nPersonally identifiable information is sensitive information that can be used to track down or know the identify of a person. This could be someone’s name, phone number, email, or ID number. This is generally something that should be kept safe. Information should not leave the office nor should it be discussed with unauthorized parties. It could be appropriate when signing up for services where PII is mandated or for following up with a customer.\n\n\n\nIn the spreadsheet Annex 1, you will find a raw dataset from a recent data collection exercise that was carried out by your team.\n\nThere are errors in in the dataset. Please identify at least four errors by highlighting them in yellow in the excel sheets. In the cleaning log tab, report the cell IDs, variable name and a small explanation on why you think this value can be an error in the comment column.\n\nSee the spreadsheet available here.\n\nUsing the programming language python, create a new variable characterizing the household drinking water source into improved / unimproved source following the classification below. Paste the code / function you used below.\n\nTable 1: Unimproved / Improved drinking water source categorization\n\n\n\ndrinking_water_source\nImproved water source\n\n\n\n\nProtected dug well\nImproved water source\n\n\nPiped water to yard or plot\nImproved water source\n\n\nPiped water into dwelling (house)\nImproved water source\n\n\nBottled water\nImproved water source\n\n\nTube well or borehole\nImproved water source\n\n\nPublic tap or standpipe\nImproved water source\n\n\nProtected spring\nImproved water source\n\n\nOther\nNA\n\n\n\nStart by importing the relevant libraries.\n\n# Uncomment these lines and run them if you do not have the required packages installed. If you don't want \n# use a virtual environment, leave the first command commented and uncomment the last two.\n\n#!python -m venv .venv\n#!pip install --upgrade pip\n#!pip install -r requirements.txt\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom pathlib import Path\n\nRead the data\n\nDATA_PATH = Path('data')\n\ndf = pd.read_csv(DATA_PATH / 'REACH_HR_TEST_DATA_analyst.docx-EmbeddedFile.xlsm - Annex  1 - REACH Assessment Tes.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nInterviewID\ndata_collection_round\nMarital status - Head of Household\nsingle_headed_household\nNumber household member boy under5 years old\nNumber household member _girl_under5 years old\nNumber household member boy_5_17 years old\nhousehold_girl_5_17\nnumber adult household members years old\nTotal household number\n...\nHouseholds use bottled water as drinking water source\nHousehold treating water\nImprovedsanitationfacility\nMentionedafterdefecating\nMentionedbeforeeating\nMentionedbeforeeatingafterdefecating\nMentionedbeforefeedingchild\nhandwashingfull\nHousehold praticing open defecation\nFrequency respondant report handwhashing a day\n\n\n\n\n0\nbaseline1\nBaseline\nWidowed\nYes\n1\n1\n2\n2\n4\n10\n...\nOther source\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n1\nbaseline10\nBaseline\nMarried\nNo\n0\n0\n1\n0\n1\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n7 times and more\n\n\n2\nbaseline100\nBaseline\nMarried\nNo\n0\n1\n1\n1\n3\n6\n...\nOther source\nYes\nImproved toilet facility\nNo\nYes\nNo\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n3\nbaseline1000\nBaseline\nMarried\nNo\n0\n0\n0\n0\n2\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n4\nbaseline1001\nBaseline\nMarried\nNo\n0\n0\n3\n1\n5\n9\n...\nBottled water\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n\n\n5 rows × 24 columns\n\n\n\nCheck for strange values\n\nfor col in df.columns:\n    print(df[col].value_counts())\n\nInterviewID\nuuid:21e50d02-d5a9-48ed-89dc-5312858a1239    2\nbaseline1                                    1\nuuid:185510ee-8add-4bef-a40f-0149083941ba    1\nuuid:1993bcf6-c7bf-4d14-80ae-19c63c090dc5    1\nuuid:19863d7f-9da4-4ea7-ab40-5dfb98d262a2    1\n                                            ..\nbaseline2443                                 1\nbaseline2442                                 1\nbaseline2441                                 1\nbaseline2440                                 1\nuuid:ffd4a47d-c096-494d-b166-db98d4e87446    1\nName: count, Length: 4818, dtype: int64\ndata_collection_round\nBaseline    3025\nEnd-line    1794\nName: count, dtype: int64\nMarital status - Head of Household\nMarried                                    4395\nWidowed                                     296\nDivorced                                     60\nSingle                                       56\nSeperated_above_18_acting_as_caregiver_      12\nName: count, dtype: int64\nsingle_headed_household\nNo     4391\nYes     428\nName: count, dtype: int64\nNumber household member boy under5 years old\n0    3551\n1    1025\n2     209\n3      31\n4       2\n5       1\nName: count, dtype: int64\nNumber household member _girl_under5 years old\n0    3630\n1     961\n2     209\n3      16\n4       3\nName: count, dtype: int64\nNumber household member boy_5_17 years old\n0    2238\n1    1466\n2     806\n3     211\n4      78\n5      17\n6       2\n7       1\nName: count, dtype: int64\nhousehold_girl_5_17\n0    2453\n1    1480\n2     636\n3     201\n4      43\n5       3\n7       2\n6       1\nName: count, dtype: int64\nnumber adult household members years old\n 2     2403\n 3     1015\n 4      605\n 5      292\n 1      290\n 6      121\n 7       54\n 8       24\n 9        8\n 10       5\n 11       1\n-5        1\nName: count, dtype: int64\nTotal household number\n4        956\n5        923\n3        697\n6        689\n7        465\n2        422\n8        256\n9        154\n1         90\n10        72\n11        34\n12        29\n14        13\n13        12\n15         3\n0          2\n17         1\n12156      1\nName: count, dtype: int64\ndiarrhea_under_5\nNo     1804\nYes     194\nName: count, dtype: int64\nhouse_type\nTimber frame           2201\nTimber and concrete    1124\nHut                     814\nConcrete                524\nMakeshift shelter       155\nName: count, dtype: int64\ndrinking_water_source\nTube well or borehole                1340\nBottled water                         885\nPiped water to yard or plot           619\nPublic tap or standpipe               610\nPiped water into dwelling (house)     453\nProtected dug well                    367\nProtected spring                      219\nUnprotected dug well                  155\nUnprotected spring                     73\nRainwater collection                   35\nTanker-truck                           20\nSurface water                          18\nOther                                  10\nCart with small tank or drum            6\nName: count, dtype: int64\ndrinking water source other\nBeer                  3\nwell well             1\nfiltered water        1\ndon’t know            1\npiped water           1\nrain water            1\nfrom people           1\nAstqrad mn aljeran    1\nName: count, dtype: int64\nHouseholds use bottled water as drinking water source\nOther source     3925\nBottled water     885\nName: count, dtype: int64\nHousehold treating water\nNo     3045\nYes    1770\nName: count, dtype: int64\nImprovedsanitationfacility\nImproved toilet facility      4153\nUnimproved toilet facility     611\nName: count, dtype: int64\nMentionedafterdefecating\nYes    3314\nNo     1500\nName: count, dtype: int64\nMentionedbeforeeating\nYes    4605\nNo      209\nName: count, dtype: int64\nMentionedbeforeeatingafterdefecating\nYes    3244\nNo     1570\nName: count, dtype: int64\nMentionedbeforefeedingchild\nNo     4085\nYes     729\nName: count, dtype: int64\nhandwashingfull\nHandwashing facility with Water & Soap          3906\nNo Handwashing facility                          448\nHandwashing facility with Water without Soap     245\nHandwashing facility without Water and Soap      220\nName: count, dtype: int64\nHousehold praticing open defecation\nNo open defecation    3909\nOpen defecation        880\nName: count, dtype: int64\nFrequency respondant report handwhashing a day\n7 times and more    1803\n5 - 6 times         1642\n3 - 4 times         1298\n0 - 2 times           76\nName: count, dtype: int64\n\n\nCreate a new variable using the mapping from Table 1.\n\nimproved_list = ['Protected dug well', 'Piped water to yard or plot', 'Piped water into dwelling (house)',\n                 'Bottled Water', 'Tube well or borehold', 'Public tap or standpipe', 'Protected spring']\n\n\ndf['improved_water_source'] = ''\ndf.loc[df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Improved water source'\ndf.loc[~df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Unimproved water source'\ndf.loc[df['drinking_water_source'] == 'Other', 'improved_water_source'] = np.nan\n\n\nThis exercise requires the results of the previous exercise. Use any tools, statistics and visualizations that you see fit to analyze the questions below regarding how access to improved water sources changed between the baseline (first data collection round) and the endline (second data collection round, after a water improvement project has been implemented). Records for both rounds are in the same dataset; and come from randomly sampled households in the area of intervention. The column “data_collection_round” is “Baseline” for records of the first round, and “Endline” for records from the second round. Please share all code/files used for the analysis.\n\nDid single headed households receive more/less improvements? (relevant data column: “single_headed_household”)\n\n\n\ndf_single = df.loc[df['single_headed_household'] == 'Yes']\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False)\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False, normalize=True)\nmosaic(df_single, ['data_collection_round', 'improved_water_source'], title='Single headed household')\nplt.tight_layout()\nplt.show()\n\n\n\n\nSingle headed households received fewer improvements in endline than in baseline. There were 137 improved water sources in baseline (50.7% of all water sources) but only 68 improved water sources in endline (43% of all water sources)\n\nDid the improvements affect cases of diarrhea in children under 5? (relevant data column: “diarrhea_under_5”)\n\n\ndf_improved = df.loc[df['improved_water_source'] == 'Improved water source']\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False)\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False, normalize=True)\nmosaic(df_improved, ['data_collection_round', 'diarrhea_under_5'], title='Diarrhea under 5 among households with improved water sources')\nplt.tight_layout()\nplt.show()\n\n\n\n\nYes, diarrhea in children under 5 went down from 72 cases in baseline (4.9%) to 31 cases in endline (3.8%).\n\n\n\nInstructions\nDescribe in detail the various stages involved in successfully completing a complex data project. You are supporting a large household survey data collection in KoboToolbox. As part of this project, you want to develop an algorithm that would identify suspicious behavior of enumerator during data collection.\nYou have it at your disposal. - the raw data from ODK / KoboToolbox, - the clean data - the cleaning log in which cleaning operations are logged (see example in excel Annex 1), - the audit files from ODK / KoboToolbox that logs enumerator behavior during interview administering the interview. Form Audit Log - ODK Docs (getodk.org)\nWe want to identify if there are suspicious surveys (fake interviews for example) in the data collection that is coming: - What are the different stages involved in carrying out such a project?\n\nDownload audit.csv from ODK\nWe could first look at the distribution of the length of interviews. Interviews that are suspiciously short or in the bottom 5% of times taken would be marked for further investigation.\nWe would also look at the location data to compare it with where the respondent is supposed to be. Assuming that the interview was conducted at the respondent’s home, data that was filled in outside of this would be suspicious.\nWe would also look at the distribution of the number of changes to response. Enumerators with an unusually high number could be investigated further, perhaps by performing a text analysis of their reasons for changes.\nAnother way of checking for fake surveys is by conducting back checks. A random 10% of households could be selected to be re-interviewed by a different enumerator. If the survey responses differ markedly from the original survey, this could be evidence of a fraudulent survey.\n\nFor each stage, detail the tasks to be carried out, the methods to be followed, possible avenues and critical points. - Thoroughly justify your choices.\n\nAfter downloading audit.csv, a duration variable could be created that is difference between the end and start times of the survey. The elapsed time is accurate even in the start and end timestamps aren’t as mentioned in Form Audit Log - ODK Docs (getodk.org). A histogram of the duration could be be plotted. Surveys with times in the bottom 10%, for example, could be checked to see whether the answers are sensible, how many skipped questions there are, etc. This could indicate whether an enumerator is simply rushing through a survey without regard to skip patterns or filling in responses at random.\nFor the location data, the GPS coordinates of where the survey was meant to be conducted can be checked against the location where it was actually done. The downside of this is that if the device shuts off or location data is not turned on, this would make it hard to know whether the survey was done at the appropriate location\nFor the number of changed responses, we could compute the number of changed responses per survey and plot a histogram of the result. Those in the highest 10%, for example, could be set aside for further investigation. We could look at the reasons for the changes and compare those with surveys that are closer to the median.\nFinally, for the back check portion, we can randomly sample a subset of interview IDs to be re-interviewed. A trusted enumerator, preferably separate from the rest of the team, will then be sent to re-interview those households using the entire questionnaire or only the questions of concern. The answers will be compared to those from the first survey. One way to compare is to use the bcstats tool developed by Innovations for Poverty Action https://github.com/PovertyAction/ipabcstats. For more information about back check procecdures, see https://dimewiki.worldbank.org/Back_Checks."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#instructions",
    "href": "posts/post-with-code/REACH_HR_TEST.html#instructions",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "This test includes three parts, 1. General Knowledge, 2. Data Processing and 3. Data Project. Use the spreadsheets Annex 1 to help you answer the questions below.\nThe test has been designed to take 2:30 hours and examines several competencies regarding data analysis. Read all questions before you begin and note that the three parts can be completed in any order. When sending back your answers, please share all scripts, code, files etc. that you used to solve the exercises. Please also list all websites/external sources you used to answer the questions. Using generative AI to answer the questions is not permitted. Checks on the use of AI will be performed.\nAll answers can be noted directly on this answer sheet unless otherwise specified. Please return this document with you answers, together with Annex 1 by email."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#part-1-general-knowledge",
    "href": "posts/post-with-code/REACH_HR_TEST.html#part-1-general-knowledge",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "Explain p-values in layman terms. Feel free to use analogies or examples. Keep it simple, but make sure to stay technically accurate.\n\nWhy are they important?\n\nP-values given a measure of how precise an estimate is. It tells us how likely we are to see a value at least as large under the assumption that the null hypothesis is true. In the social sciences, a p-value less than or equal to 0.05 is considered to be a statistically significant result.\n\nHow can they be interpreted?\n\nThey can be interpreted as the probability of seeing a value at least as large for the coefficient under study given that the null hypothesis is true i.e. no effect. This gives some indication that the results are just noise.\n\nWhat are some common pitfalls/misunderstandings in their use and interpretation?\n\nCommon misunderstandings of p-values are that they are the probability that the null-hypothesis is true or that the alternative hypothesis is true. A common misuse of p-values comes up when dealing with researcher degrees of freedom. Performing multiple comparisons of the data can increase the probability of encountering a false positive. There are also issues such as deciding to average certain groups, exclude other observations, and choosing regression predictors that affect the validity of a p-value (See Gelman).\nWhen would a Mosaic plot be an appropriate visualization?\n\nA Mosaic plot is good for showing percentages of data in groups. It is a graphical representation of a contingency table. This would be useful for showing the effect of an intervention for treatment and control groups, for example. See here for more.\n\nWhat is personally identifiable information (PII)? Provide an example. When is it ok to collect PII?\n\nPersonally identifiable information is sensitive information that can be used to track down or know the identify of a person. This could be someone’s name, phone number, email, or ID number. This is generally something that should be kept safe. Information should not leave the office nor should it be discussed with unauthorized parties. It could be appropriate when signing up for services where PII is mandated or for following up with a customer."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#part-2-data-processing",
    "href": "posts/post-with-code/REACH_HR_TEST.html#part-2-data-processing",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "In the spreadsheet Annex 1, you will find a raw dataset from a recent data collection exercise that was carried out by your team.\n\nThere are errors in in the dataset. Please identify at least four errors by highlighting them in yellow in the excel sheets. In the cleaning log tab, report the cell IDs, variable name and a small explanation on why you think this value can be an error in the comment column.\n\nSee the spreadsheet available here.\n\nUsing the programming language python, create a new variable characterizing the household drinking water source into improved / unimproved source following the classification below. Paste the code / function you used below.\n\nTable 1: Unimproved / Improved drinking water source categorization\n\n\n\ndrinking_water_source\nImproved water source\n\n\n\n\nProtected dug well\nImproved water source\n\n\nPiped water to yard or plot\nImproved water source\n\n\nPiped water into dwelling (house)\nImproved water source\n\n\nBottled water\nImproved water source\n\n\nTube well or borehole\nImproved water source\n\n\nPublic tap or standpipe\nImproved water source\n\n\nProtected spring\nImproved water source\n\n\nOther\nNA\n\n\n\nStart by importing the relevant libraries.\n\n# Uncomment these lines and run them if you do not have the required packages installed. If you don't want \n# use a virtual environment, leave the first command commented and uncomment the last two.\n\n#!python -m venv .venv\n#!pip install --upgrade pip\n#!pip install -r requirements.txt\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom pathlib import Path\n\nRead the data\n\nDATA_PATH = Path('data')\n\ndf = pd.read_csv(DATA_PATH / 'REACH_HR_TEST_DATA_analyst.docx-EmbeddedFile.xlsm - Annex  1 - REACH Assessment Tes.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nInterviewID\ndata_collection_round\nMarital status - Head of Household\nsingle_headed_household\nNumber household member boy under5 years old\nNumber household member _girl_under5 years old\nNumber household member boy_5_17 years old\nhousehold_girl_5_17\nnumber adult household members years old\nTotal household number\n...\nHouseholds use bottled water as drinking water source\nHousehold treating water\nImprovedsanitationfacility\nMentionedafterdefecating\nMentionedbeforeeating\nMentionedbeforeeatingafterdefecating\nMentionedbeforefeedingchild\nhandwashingfull\nHousehold praticing open defecation\nFrequency respondant report handwhashing a day\n\n\n\n\n0\nbaseline1\nBaseline\nWidowed\nYes\n1\n1\n2\n2\n4\n10\n...\nOther source\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n1\nbaseline10\nBaseline\nMarried\nNo\n0\n0\n1\n0\n1\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n7 times and more\n\n\n2\nbaseline100\nBaseline\nMarried\nNo\n0\n1\n1\n1\n3\n6\n...\nOther source\nYes\nImproved toilet facility\nNo\nYes\nNo\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n3\nbaseline1000\nBaseline\nMarried\nNo\n0\n0\n0\n0\n2\n2\n...\nOther source\nYes\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n3 - 4 times\n\n\n4\nbaseline1001\nBaseline\nMarried\nNo\n0\n0\n3\n1\n5\n9\n...\nBottled water\nNo\nImproved toilet facility\nYes\nYes\nYes\nNo\nHandwashing facility with Water & Soap\nNo open defecation\n5 - 6 times\n\n\n\n\n5 rows × 24 columns\n\n\n\nCheck for strange values\n\nfor col in df.columns:\n    print(df[col].value_counts())\n\nInterviewID\nuuid:21e50d02-d5a9-48ed-89dc-5312858a1239    2\nbaseline1                                    1\nuuid:185510ee-8add-4bef-a40f-0149083941ba    1\nuuid:1993bcf6-c7bf-4d14-80ae-19c63c090dc5    1\nuuid:19863d7f-9da4-4ea7-ab40-5dfb98d262a2    1\n                                            ..\nbaseline2443                                 1\nbaseline2442                                 1\nbaseline2441                                 1\nbaseline2440                                 1\nuuid:ffd4a47d-c096-494d-b166-db98d4e87446    1\nName: count, Length: 4818, dtype: int64\ndata_collection_round\nBaseline    3025\nEnd-line    1794\nName: count, dtype: int64\nMarital status - Head of Household\nMarried                                    4395\nWidowed                                     296\nDivorced                                     60\nSingle                                       56\nSeperated_above_18_acting_as_caregiver_      12\nName: count, dtype: int64\nsingle_headed_household\nNo     4391\nYes     428\nName: count, dtype: int64\nNumber household member boy under5 years old\n0    3551\n1    1025\n2     209\n3      31\n4       2\n5       1\nName: count, dtype: int64\nNumber household member _girl_under5 years old\n0    3630\n1     961\n2     209\n3      16\n4       3\nName: count, dtype: int64\nNumber household member boy_5_17 years old\n0    2238\n1    1466\n2     806\n3     211\n4      78\n5      17\n6       2\n7       1\nName: count, dtype: int64\nhousehold_girl_5_17\n0    2453\n1    1480\n2     636\n3     201\n4      43\n5       3\n7       2\n6       1\nName: count, dtype: int64\nnumber adult household members years old\n 2     2403\n 3     1015\n 4      605\n 5      292\n 1      290\n 6      121\n 7       54\n 8       24\n 9        8\n 10       5\n 11       1\n-5        1\nName: count, dtype: int64\nTotal household number\n4        956\n5        923\n3        697\n6        689\n7        465\n2        422\n8        256\n9        154\n1         90\n10        72\n11        34\n12        29\n14        13\n13        12\n15         3\n0          2\n17         1\n12156      1\nName: count, dtype: int64\ndiarrhea_under_5\nNo     1804\nYes     194\nName: count, dtype: int64\nhouse_type\nTimber frame           2201\nTimber and concrete    1124\nHut                     814\nConcrete                524\nMakeshift shelter       155\nName: count, dtype: int64\ndrinking_water_source\nTube well or borehole                1340\nBottled water                         885\nPiped water to yard or plot           619\nPublic tap or standpipe               610\nPiped water into dwelling (house)     453\nProtected dug well                    367\nProtected spring                      219\nUnprotected dug well                  155\nUnprotected spring                     73\nRainwater collection                   35\nTanker-truck                           20\nSurface water                          18\nOther                                  10\nCart with small tank or drum            6\nName: count, dtype: int64\ndrinking water source other\nBeer                  3\nwell well             1\nfiltered water        1\ndon’t know            1\npiped water           1\nrain water            1\nfrom people           1\nAstqrad mn aljeran    1\nName: count, dtype: int64\nHouseholds use bottled water as drinking water source\nOther source     3925\nBottled water     885\nName: count, dtype: int64\nHousehold treating water\nNo     3045\nYes    1770\nName: count, dtype: int64\nImprovedsanitationfacility\nImproved toilet facility      4153\nUnimproved toilet facility     611\nName: count, dtype: int64\nMentionedafterdefecating\nYes    3314\nNo     1500\nName: count, dtype: int64\nMentionedbeforeeating\nYes    4605\nNo      209\nName: count, dtype: int64\nMentionedbeforeeatingafterdefecating\nYes    3244\nNo     1570\nName: count, dtype: int64\nMentionedbeforefeedingchild\nNo     4085\nYes     729\nName: count, dtype: int64\nhandwashingfull\nHandwashing facility with Water & Soap          3906\nNo Handwashing facility                          448\nHandwashing facility with Water without Soap     245\nHandwashing facility without Water and Soap      220\nName: count, dtype: int64\nHousehold praticing open defecation\nNo open defecation    3909\nOpen defecation        880\nName: count, dtype: int64\nFrequency respondant report handwhashing a day\n7 times and more    1803\n5 - 6 times         1642\n3 - 4 times         1298\n0 - 2 times           76\nName: count, dtype: int64\n\n\nCreate a new variable using the mapping from Table 1.\n\nimproved_list = ['Protected dug well', 'Piped water to yard or plot', 'Piped water into dwelling (house)',\n                 'Bottled Water', 'Tube well or borehold', 'Public tap or standpipe', 'Protected spring']\n\n\ndf['improved_water_source'] = ''\ndf.loc[df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Improved water source'\ndf.loc[~df['drinking_water_source'].isin(improved_list), 'improved_water_source'] = 'Unimproved water source'\ndf.loc[df['drinking_water_source'] == 'Other', 'improved_water_source'] = np.nan\n\n\nThis exercise requires the results of the previous exercise. Use any tools, statistics and visualizations that you see fit to analyze the questions below regarding how access to improved water sources changed between the baseline (first data collection round) and the endline (second data collection round, after a water improvement project has been implemented). Records for both rounds are in the same dataset; and come from randomly sampled households in the area of intervention. The column “data_collection_round” is “Baseline” for records of the first round, and “Endline” for records from the second round. Please share all code/files used for the analysis.\n\nDid single headed households receive more/less improvements? (relevant data column: “single_headed_household”)\n\n\n\ndf_single = df.loc[df['single_headed_household'] == 'Yes']\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False)\ndf_single.groupby('data_collection_round')['improved_water_source'].value_counts(dropna=False, normalize=True)\nmosaic(df_single, ['data_collection_round', 'improved_water_source'], title='Single headed household')\nplt.tight_layout()\nplt.show()\n\n\n\n\nSingle headed households received fewer improvements in endline than in baseline. There were 137 improved water sources in baseline (50.7% of all water sources) but only 68 improved water sources in endline (43% of all water sources)\n\nDid the improvements affect cases of diarrhea in children under 5? (relevant data column: “diarrhea_under_5”)\n\n\ndf_improved = df.loc[df['improved_water_source'] == 'Improved water source']\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False)\ndf_improved.groupby('data_collection_round')['diarrhea_under_5'].value_counts(dropna=False, normalize=True)\nmosaic(df_improved, ['data_collection_round', 'diarrhea_under_5'], title='Diarrhea under 5 among households with improved water sources')\nplt.tight_layout()\nplt.show()\n\n\n\n\nYes, diarrhea in children under 5 went down from 72 cases in baseline (4.9%) to 31 cases in endline (3.8%)."
  },
  {
    "objectID": "posts/post-with-code/REACH_HR_TEST.html#part-3-data-project",
    "href": "posts/post-with-code/REACH_HR_TEST.html#part-3-data-project",
    "title": "REACH / IMPACT Initiatives Assessment",
    "section": "",
    "text": "Instructions\nDescribe in detail the various stages involved in successfully completing a complex data project. You are supporting a large household survey data collection in KoboToolbox. As part of this project, you want to develop an algorithm that would identify suspicious behavior of enumerator during data collection.\nYou have it at your disposal. - the raw data from ODK / KoboToolbox, - the clean data - the cleaning log in which cleaning operations are logged (see example in excel Annex 1), - the audit files from ODK / KoboToolbox that logs enumerator behavior during interview administering the interview. Form Audit Log - ODK Docs (getodk.org)\nWe want to identify if there are suspicious surveys (fake interviews for example) in the data collection that is coming: - What are the different stages involved in carrying out such a project?\n\nDownload audit.csv from ODK\nWe could first look at the distribution of the length of interviews. Interviews that are suspiciously short or in the bottom 5% of times taken would be marked for further investigation.\nWe would also look at the location data to compare it with where the respondent is supposed to be. Assuming that the interview was conducted at the respondent’s home, data that was filled in outside of this would be suspicious.\nWe would also look at the distribution of the number of changes to response. Enumerators with an unusually high number could be investigated further, perhaps by performing a text analysis of their reasons for changes.\nAnother way of checking for fake surveys is by conducting back checks. A random 10% of households could be selected to be re-interviewed by a different enumerator. If the survey responses differ markedly from the original survey, this could be evidence of a fraudulent survey.\n\nFor each stage, detail the tasks to be carried out, the methods to be followed, possible avenues and critical points. - Thoroughly justify your choices.\n\nAfter downloading audit.csv, a duration variable could be created that is difference between the end and start times of the survey. The elapsed time is accurate even in the start and end timestamps aren’t as mentioned in Form Audit Log - ODK Docs (getodk.org). A histogram of the duration could be be plotted. Surveys with times in the bottom 10%, for example, could be checked to see whether the answers are sensible, how many skipped questions there are, etc. This could indicate whether an enumerator is simply rushing through a survey without regard to skip patterns or filling in responses at random.\nFor the location data, the GPS coordinates of where the survey was meant to be conducted can be checked against the location where it was actually done. The downside of this is that if the device shuts off or location data is not turned on, this would make it hard to know whether the survey was done at the appropriate location\nFor the number of changed responses, we could compute the number of changed responses per survey and plot a histogram of the result. Those in the highest 10%, for example, could be set aside for further investigation. We could look at the reasons for the changes and compare those with surveys that are closer to the median.\nFinally, for the back check portion, we can randomly sample a subset of interview IDs to be re-interviewed. A trusted enumerator, preferably separate from the rest of the team, will then be sent to re-interview those households using the entire questionnaire or only the questions of concern. The answers will be compared to those from the first survey. One way to compare is to use the bcstats tool developed by Innovations for Poverty Action https://github.com/PovertyAction/ipabcstats. For more information about back check procecdures, see https://dimewiki.worldbank.org/Back_Checks."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome! I will be sharing some blog posts on various topics."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A few notes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A few notes",
    "section": "",
    "text": "REACH / IMPACT Initiatives Assessment\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\n  \n\n\n\n\nGiveDirectly Assessment\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ninterview\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html",
    "href": "posts/post-with-code/give-directly-assessment.html",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "This was a take home assessment I did for GiveDirectly in June 2022. I was given 2.5 hours to complete the assignment. The csv files and instructions can be found here.\n\n\n\nPlease evaluate the data in recipients.csv and survey_attempts.csv to answer the following questions:\n\nHow many recipients are in each of the four stages? Please provide the calculation(s) in the spreadsheet or code that you submit.\nHow many surveys were successfully completed in December, 2020? Please provide the calculation(s) in the spreadsheet or code that you submit.\nDid you find any abnormalities in the source data? If so, how did you account for them in your analysis?\n\n\nFirst install the required packages.\n\n!pip install -r requirements.txt\n\nRequirement already satisfied: pandas==1.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: jupyterlab==3.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: matplotlib==3.5.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.5.2)\nRequirement already satisfied: ipython==8.4.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (8.4.0)\nRequirement already satisfied: flake8==4.0.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (4.0.1)\nRequirement already satisfied: PyQt5==5.15.6 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (5.15.6)\nRequirement already satisfied: statsmodels==0.13.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.13.2)\nRequirement already satisfied: ipykernel==6.13.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (6.13.0)\nRequirement already satisfied: missingno==0.5.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.5.1)\nRequirement already satisfied: scikit-learn==1.1.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: numpy&gt;=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.22.4)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2022.1)\nRequirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3)\nRequirement already satisfied: jupyter-server~=1.16 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: tornado&gt;=6.1.0 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.1)\nRequirement already satisfied: jupyter-core in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.10.0)\nRequirement already satisfied: jinja2&gt;=2.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.1.2)\nRequirement already satisfied: nbclassic~=0.2 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.3.7)\nRequirement already satisfied: jupyterlab-server~=2.10 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.14.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (9.1.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (4.33.3)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (1.4.2)\nRequirement already satisfied: pickleshare in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (3.0.29)\nRequirement already satisfied: traitlets&gt;=5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.2.2.post1)\nRequirement already satisfied: backcall in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pygments&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.12.0)\nRequirement already satisfied: jedi&gt;=0.16 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.18.1)\nRequirement already satisfied: pexpect&gt;4.3; sys_platform != \"win32\" in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (4.8.0)\nRequirement already satisfied: setuptools&gt;=18.5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (44.0.0)\nRequirement already satisfied: decorator in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.1.1)\nRequirement already satisfied: matplotlib-inline in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.1.3)\nRequirement already satisfied: stack-data in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pyflakes&lt;2.5.0,&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.4.0)\nRequirement already satisfied: mccabe&lt;0.7.0,&gt;=0.6.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (0.6.1)\nRequirement already satisfied: pycodestyle&lt;2.9.0,&gt;=2.8.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.8.0)\nRequirement already satisfied: PyQt5-Qt5&gt;=5.15.2 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (5.15.2)\nRequirement already satisfied: PyQt5-sip&lt;13,&gt;=12.8 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (12.10.1)\nRequirement already satisfied: patsy&gt;=0.5.2 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (0.5.2)\nRequirement already satisfied: scipy&gt;=1.3 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (1.8.1)\nRequirement already satisfied: debugpy&gt;=1.0 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.6.0)\nRequirement already satisfied: jupyter-client&gt;=6.1.12 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (7.3.1)\nRequirement already satisfied: nest-asyncio in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.5.5)\nRequirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (5.9.1)\nRequirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (from missingno==0.5.1-&gt;-r requirements.txt (line 9)) (0.11.2)\nRequirement already satisfied: joblib&gt;=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (3.1.0)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: terminado&gt;=0.8.3 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.15.0)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.6.1)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.5.0)\nRequirement already satisfied: Send2Trash in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.8.0)\nRequirement already satisfied: websocket-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.3.2)\nRequirement already satisfied: pyzmq&gt;=17 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (23.1.0)\nRequirement already satisfied: argon2-cffi in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3.0)\nRequirement already satisfied: prometheus-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.14.1)\nRequirement already satisfied: nbformat&gt;=5.2.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.4.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2&gt;=2.1-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.1.1)\nRequirement already satisfied: notebook-shim&gt;=0.1.0 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.1.0)\nRequirement already satisfied: notebook&lt;7 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.4.11)\nRequirement already satisfied: babel in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.10.1)\nRequirement already satisfied: json5 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.9.8)\nRequirement already satisfied: jsonschema&gt;=3.0.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.6.0)\nRequirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.27.1)\nRequirement already satisfied: importlib-metadata&gt;=3.6; python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.4)\nRequirement already satisfied: wcwidth in ./.venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.5)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in ./.venv/lib/python3.8/site-packages (from jedi&gt;=0.16-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != \"win32\"-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.0)\nRequirement already satisfied: pure-eval in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.2)\nRequirement already satisfied: asttokens in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.0.5)\nRequirement already satisfied: executing in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: entrypoints in ./.venv/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.12-&gt;ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (0.4)\nRequirement already satisfied: sniffio&gt;=1.1 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: idna&gt;=2.8 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.3)\nRequirement already satisfied: nbclient&gt;=0.5.0 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.6.4)\nRequirement already satisfied: jupyterlab-pygments in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.2)\nRequirement already satisfied: tinycss2 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.1.1)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.5.0)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.8.4)\nRequirement already satisfied: defusedxml in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.1)\nRequirement already satisfied: bleach in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.0.0)\nRequirement already satisfied: argon2-cffi-bindings in ./.venv/lib/python3.8/site-packages (from argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.2.0)\nRequirement already satisfied: fastjsonschema in ./.venv/lib/python3.8/site-packages (from nbformat&gt;=5.2.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.15.3)\nRequirement already satisfied: ipython-genutils in ./.venv/lib/python3.8/site-packages (from notebook&lt;7-&gt;nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: importlib-resources&gt;=1.4.0; python_version &lt; \"3.9\" in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.7.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2022.5.18.1)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version &gt;= \"3\" in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.0.12)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.26.9)\nRequirement already satisfied: zipp&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata&gt;=3.6; python_version &lt; \"3.10\"-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.8.0)\nRequirement already satisfied: webencodings&gt;=0.4 in ./.venv/lib/python3.8/site-packages (from tinycss2-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.5.1)\nRequirement already satisfied: soupsieve&gt;1.2 in ./.venv/lib/python3.8/site-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.3.2.post1)\nRequirement already satisfied: cffi&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.15.0)\nRequirement already satisfied: pycparser in ./.venv/lib/python3.8/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.21)\n\n\nImport the required libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nmatplotlib.use('QtAgg')\n\nfrom statsmodels.stats.weightstats import CompareMeans\nimport statsmodels.formula.api as smf\nimport missingno as msno\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nLet’s now import the two datasets and merge them.\n\nrecipients = pd.read_csv('recipients.csv')\nattempts = pd.read_csv('survey_attempts.csv')\nmerged = pd.merge(recipients, attempts, on='recipient_id', how='left', indicator=True)\nmerged._merge.value_counts(dropna=False)\n\nboth          520\nleft_only       0\nright_only      0\nName: _merge, dtype: int64\n\n\nIt looks like all the recipients are matched with survey attempts. We can drop the _merge variable.\n\nmerged.drop(columns='_merge', inplace=True)\n\nA quick glance at our data.\n\nmerged.head(10)\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n0\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\n\n\n1\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\n\n\n2\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000151\n12/20/20\nTrue\n\n\n3\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000305\n11/22/20\nFalse\n\n\n4\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000108\n11/28/20\nFalse\n\n\n5\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000116\n01/17/21\nTrue\n\n\n6\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000439\n11/18/20\nFalse\n\n\n7\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000077\n11/06/20\nFalse\n\n\n8\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000130\n01/03/21\nTrue\n\n\n9\nr-00064\nCounty D\n24.0\n26.0\n100000076.0\nActive\ns-000352\n11/04/20\nFalse\n\n\n\n\n\n\n\nCheck for duplicates. We expect that recipient_id and survey_id together form a unique id.\n\nmerged[merged.duplicated(subset=['recipient_id', 'survey_id'], keep=False)]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n15\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n17\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n19\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n22\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n28\nr-00193\nCounty B\n10.0\n62.0\n100000977.0\nActive\ns-000018\n01/22/21\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n506\nr-00191\nCounty B\n10.0\n62.0\n100000126.0\nActive\ns-000026\n12/11/20\nFalse\n\n\n510\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n513\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n518\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n519\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n\n\n100 rows × 9 columns\n\n\n\nThere are 50 pairs of duplicates that need to be dropped.\n\nmerged = merged.drop_duplicates(subset=['recipient_id', 'survey_id'], keep='first').reset_index(drop=True)\n\nLet’s now calculate the stage variable, starting with ‘Start’.\n\nmask = merged.groupby('recipient_id')['success'].any()\none_success = [mask.index[i] for i, m in enumerate(mask) if m]\n# Set the value to start for those with no successful surveys. '~' negates the value \n# of the mask. In this case, '~' means find those without any successful survey\nmerged.loc[~merged.recipient_id.isin(one_success), 'stage'] = 'Start'\n\nNext, the ‘Ineligible’ stage\n\n# Remove the text 'County' from the column\nmerged.county = merged.county.str.replace('County ', '', regex=False)\n    \ninABC = merged.county.isin(['A', 'B', 'C'])\nrecipient_noABC = merged.recipient_id[~inABC]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n    merged.recipient_id.isin(recipient_noABC), 'stage'] = 'Ineligible'\n\nThe ‘Review’ stage\n\nrecipient_yesABC = merged.recipient_id[inABC]\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\nAnd finally the ‘Pay’ stage\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\n\n\n\nmerged.stage.value_counts(dropna=False)\n\nPay           209\nStart         150\nIneligible     70\nReview         35\nNaN             6\nName: stage, dtype: int64\n\n\nThere are 209 in the ‘Pay’ stage, 150 in the ‘Start’, 70 ‘Ineligible’, 35 in ‘Review’, and 6 not in any stage. This is because there are 6 recipients with a missing account_status.\n\nmerged.loc[merged.stage.isna()]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\nstage\n\n\n\n\n0\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\nNaN\n\n\n1\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\nNaN\n\n\n14\nr-00145\nA\n22.0\n24.0\n100000089.0\nNaN\ns-000137\n12/21/20\nTrue\nNaN\n\n\n50\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000161\n11/25/20\nFalse\nNaN\n\n\n51\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000236\n11/15/20\nFalse\nNaN\n\n\n52\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000337\n01/14/21\nTrue\nNaN\n\n\n\n\n\n\n\nWe will impute a value for recipients with a missing account_status, but it could be worthwhile to follow up with someone for the appropriate status.\n\n\n\n\nmerged['month'] = merged['date'].astype(str).str[:2]\nmerged.groupby('month')['success'].sum()\n\nmonth\n01    77\n11     0\n12    93\nName: success, dtype: int64\n\n\nThere were 93 successful surveys in December.\n\n\n\nThere were duplicates in the data that had to be dropped. Some values also did not make much sense and were assgined missing value. The missing data was later imputed.\n\nmerged.describe()\n\n\n\n\n\n\n\n\ntime_county\nage\naccount_number\n\n\n\n\ncount\n455.000000\n460.000000\n4.630000e+02\n\n\nmean\n19.138462\n124.126087\n1.000001e+08\n\n\nstd\n10.501550\n925.986058\n1.074233e+02\n\n\nmin\n-45.000000\n20.000000\n1.000000e+08\n\n\n25%\n10.000000\n25.000000\n1.000001e+08\n\n\n50%\n21.000000\n32.000000\n1.000001e+08\n\n\n75%\n28.000000\n51.000000\n1.000002e+08\n\n\nmax\n35.000000\n9999.000000\n1.000010e+08\n\n\n\n\n\n\n\ntime_county has a minimum of -45 and age has a max of 9999. To deal with these problematic cases, I replace them with missing values to be imputed later. It could also be good to follow up with other teams on the correct value.\n\n\nmerged.age[merged.age == 9999] = pd.np.nan \nmerged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n/tmp/ipykernel_14671/1017566221.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n\nThere are missing values in the data as well. Six columns have missing data.\n\nmerged.isna().sum()\n\nrecipient_id       0\ncounty             8\ntime_county       20\nage               14\naccount_number     7\naccount_status    13\nsurvey_id          0\ndate               0\nsuccess            0\nstage              6\nmonth              0\ndtype: int64\n\n\nMissing data is usually deleted or imputed. Deleting missing data is easiest, but it can lead to biases if the data is not missing at random. If age is more likely to be missing in certain counties, for example, deleting missing data isn’t the best approach. If the data is missing completely at random or missing at random, it can be deleted.\nIf there are many missing values in a row, the row can be deleted. Likewise, if there are many missing values in a column, the column can be deleted.\nImputation can be as simple as replacing the missing value in a column with an arbitrary value such as ‘0’ or the column mean or mode. Other techniques take into account the values in other columns. Suppose a recipient has a missing age, but lives in a certain county and has also responded to the survey. Then the age might be imputed to be slightly lower because of a correlation between success and age or county and age. See more here.\nWe can investigate the missing data with the missingno package.\n\n%matplotlib inline\n\nmsno.matrix(merged)\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_14671/2359591310.py:5: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\nThe white lines represent missing values. The account_number and account_status appear to have coinciding missing values, and the same for age and time_county. We can visualize these relationships with a heatmap.\n\n%matplotlib inline\n\nmsno.heatmap(merged, cmap='rainbow')\nplt.tight_layout()\nplt.show()\n\n\n\n\nThere is a strong correlation between account_number and account_status being missing, which would make sense considering that someone without an account_number would not have an account_status and vice versa. There is also a strong association between age and time_county missingness.\nTo investigate this, let’s look at recipients with a missing time_county.\n\ntime_county_missing = merged.loc[merged.time_county.isna()]\nprint(time_county_missing.head())\n\n    recipient_id county  time_county   age  account_number account_status  \\\n69       r-00070      D          NaN  23.0     100000080.0         Active   \n70       r-00070      D          NaN  23.0     100000080.0         Active   \n182      r-00198      B          NaN   NaN     100000006.0         Active   \n183      r-00198      B          NaN   NaN     100000006.0         Active   \n184      r-00198      B          NaN   NaN     100000006.0         Active   \n\n    survey_id      date  success       stage month  \n69   s-000252  12/08/20     True  Ineligible    12  \n70   s-000280  11/10/20    False  Ineligible    11  \n182  s-000284  01/07/21    False       Start    01  \n183  s-000285  12/08/20    False       Start    12  \n184  s-000311  12/18/20    False       Start    12  \n\n\nAnd a missing age\n\nage_missing = merged.loc[merged.age.isna()]\nprint(age_missing.head())\n\n    recipient_id county  time_county  age  account_number account_status  \\\n145      r-00060      B         28.0  NaN     100000018.0     Not Active   \n146      r-00060      B         28.0  NaN     100000018.0     Not Active   \n182      r-00198      B          NaN  NaN     100000006.0         Active   \n183      r-00198      B          NaN  NaN     100000006.0         Active   \n184      r-00198      B          NaN  NaN     100000006.0         Active   \n\n    survey_id      date  success   stage month  \n145  s-000273  11/30/20    False  Review    11  \n146  s-000455  01/29/21     True  Review    01  \n182  s-000284  01/07/21    False   Start    01  \n183  s-000285  12/08/20    False   Start    12  \n184  s-000311  12/18/20    False   Start    12  \n\n\n\nprint(time_county_missing.recipient_id.nunique())\n\n5\n\n\n\nprint(age_missing.recipient_id.nunique())\n\n4\n\n\nSo age is usually missing when time_county is missing. There are five recipients that did not fill in their age and four that did not fill in time_county for multiple survey attempts. They may have been uncomfortable sharing that information.\nDropping these rows could lead to bias because they are not missing at random. Let’s impute the age and time_county for them. We’ll also impute the county and account_status, which are categorical variables. The imputation strategy will regress the column with missing data, say age, on other columns in the data. It will then use the regression coefficients to predict the missing values for age. More details can be found here, here, and here\nStarting with the categorical columns, we’ll prepare the dataset for imputation by converting account_status and county to categorical codes.\n\ncat_cols_na = ['account_status', 'county']\nmerged[cat_cols_na] = merged[cat_cols_na].astype('category')\nd_na = {col: {n: cat for n, cat in enumerate(merged[col].cat.categories)}\n            for col in cat_cols_na}\nmerged[cat_cols_na] = pd.DataFrame(\n    {col: merged[col].cat.codes for col in cat_cols_na},\n    index=merged.index\n)\nprint(merged[cat_cols_na].head())\n\n   account_status  county\n0              -1       1\n1              -1       1\n2               1       2\n3               1       2\n4               0       2\n\n\nNow we can impute the missing values.\n\n\nimp_cat = IterativeImputer(estimator=ExtraTreesClassifier(),\n                            initial_strategy='most_frequent',\n                            max_iter=10, random_state=0, missing_values=-1)\n\n\nmerged[cat_cols_na] = imp_cat.fit_transform(merged[cat_cols_na])\n\nFinally, we convert the numerical codes back to their original labels\n\nfor col in cat_cols_na:\n        merged[col].replace(d_na[col], inplace=True)\nprint(merged[cat_cols_na].head())\n\n  account_status county\n0         Active      B\n1         Active      B\n2     Not Active      C\n3     Not Active      C\n4         Active      C\n\n\nage and time_county are numeric variables, so they do not need a conversion.\n\nnum_cols_na = ['age', 'time_county']\nimp_num = IterativeImputer(estimator=ExtraTreesRegressor(),\n                               initial_strategy='median',\n                               max_iter=10, random_state=0)\nmerged[num_cols_na] = imp_num.fit_transform(merged[num_cols_na])\n\nNow that account_status has been imputed, the stage variable can be recalculated because the six missing cases were missing because account_status was missing at the time of calculation.\n\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\nAnother look at which columns contain missing data shows that there is only the account_number column, which can be ignored for analysis. It also would not be appropriate to impute an identifier.\n\nmerged.isna().sum()\n\nrecipient_id      0\ncounty            0\ntime_county       0\nage               0\naccount_number    7\naccount_status    0\nsurvey_id         0\ndate              0\nsuccess           0\nstage             0\nmonth             0\ndtype: int64\n\n\n\n\n\n\n\nThe program manager has asked for data to help determine whether the field team should focus more effort on calling those in stage Start or following up to resolve issues with those in stage Review. Please write a response to the program manager, including data that may help inform the decision, and some additional factors that you would take into consideration to make the decision. Assume that the program manager’s expertise does not include interpreting data and complex analytics. Please limit your written response to 300 words or less. \n\nIt would not make sense to calculate the chance of someone having a successful survey based on the stage variable because the ‘Start’ group would have no successful surveys, and the other three groups would have all recipients with at least one successful survey. On the other hand, it would be good to know the chances of someone moving from the ‘Review’ stage to a ‘Pay’ stage at a later date, for example.\n\n# Check how many recipients have more than one stage\n(merged.groupby('recipient_id')['stage'].nunique() &gt; 1).sum()\n\n0\n\n\nThere do not appear to be recipients that have changed their stage. Let’s check for recipients that have changed from ‘Not Active’ to ‘Active’ account_status.\n\n(merged.groupby('recipient_id')['account_status'].nunique() &gt; 1).sum()\n\n0\n\n\nNo recipients have changed from ‘Not Active’ to ‘Active’ account_status either.\nOne thing to consider is that the ‘Start’ group has 150 recipients compared to only 35 in the ‘Review’ group. The ‘Start’ group success rate only needs to be 35/150 ~ 23% to match a 100% success rate in the ‘Review’ group. It would be good to look up historical data on the conversion rate from ‘Start’ to ‘Pay’ versus ‘Review’ to ‘Pay’. Data on the cost of converting the ‘Start’ group versus the ‘Review’ group would also be helpful for the decision.\n\nmerged.stage.value_counts(dropna=False)\n\nPay           215\nStart         150\nIneligible     70\nReview         35\nName: stage, dtype: int64\n\n\n\n\n\n\nThe country director is considering investing resources into proactively conducting in-person surveys with recipients in the highest age group across projects to increase overall survey success rate. They believe that this additional cost might outweigh the current costs of repeated failed phone survey attempts, if we can accurately target those recipients least likely to respond to a phone survey. \n\nWhat analysis would you provide from the provided project data to help make this decision? Please provide the calculation(s) in the spreadsheet or code that you submit.\nAre there other factors that might explain the observed survey success rate from this project? Please use your judgment to determine these factors and limit your written response to 400 words or less. \n\n\n\n\nLet’s graph the relationship between age and at least one successful survey.\n\n%matplotlib inline\n\nmerged['one_success'] = 0\nmerged.loc[merged.recipient_id.isin(one_success), 'one_success'] = 1\n\n# Create bins for age broken into quartiles\nmerged['age_bin'] = pd.qcut(merged.age, q=4)\nage_success = merged.groupby('age_bin')['one_success'].sum()\nage = merged.groupby('age_bin')['one_success'].count()\nresults = age_success.div(age, level='age_bin') * 100\n\nresults.plot(kind='bar')\n\nplt.xticks(rotation=0)\nplt.xlabel('Age Group')\nplt.ylabel('% of recipients with at least\\n one successful survey')\nplt.show()\n\n\n\n\nPeople in the highest age group are the least likely to have at least one successful survey.\nRunning logistic regression gives\n\nlogit = smf.logit('one_success ~ age', data=merged).fit()\nprint(logit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.278114\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      468\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.5559\nTime:                        13:10:08   Log-Likelihood:                -130.71\nconverged:                       True   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                 3.865e-73\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.8313      0.615     12.744      0.000       6.627       9.036\nage           -0.1740      0.014    -12.308      0.000      -0.202      -0.146\n==============================================================================\n\n\nThere is a negative relationship between age and the chance of at least one successful survey. The coefficient is also statistically significant. To interpret it, we calculate the odds ratio.\n\nodds_ratios = pd.DataFrame({\n        'OR': logit.params, \n        'Lower CI': logit.conf_int()[0],\n        'Upper CI': logit.conf_int()[1]\n    })\n    \nodds_ratios = pd.np.exp(odds_ratios)\nprint(odds_ratios)\nprint(round((odds_ratios['OR'][1] - 1) * 100, 2))\n\n                    OR    Lower CI     Upper CI\nIntercept  2518.096539  755.093254  8397.386877\nage           0.840278    0.817312     0.863888\n-15.97\n\n\n/tmp/ipykernel_14671/4211037893.py:7: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  odds_ratios = pd.np.exp(odds_ratios)\n\n\nEach additional increase of one year in age is associated with a roughly 16 percent decrease in odds of having at least one successful survey.\nSo far, it appears that older recipients are less likely to respond to surveys. Focusing on the older recipients would help target those with lower response rates.\n\n\n\nWe include other variables in our logistic regression to check for confounders.\n\nbig_logit = smf.logit('one_success ~ time_county + age + C(month) + C(account_status)', data=merged).fit()\nprint(big_logit.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.074409\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      464\nMethod:                           MLE   Df Model:                            5\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.8812\nTime:                        13:10:08   Log-Likelihood:                -34.972\nconverged:                      False   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                7.306e-110\n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nIntercept                          10.2072      2.959      3.449      0.001       4.407      16.007\nC(month)[T.11]                     24.6815   8.53e+04      0.000      1.000   -1.67e+05    1.67e+05\nC(month)[T.12]                     -0.3861      0.670     -0.576      0.564      -1.699       0.927\nC(account_status)[T.Not Active]    30.8016   4900.380      0.006      0.995   -9573.767    9635.370\ntime_county                         0.1367      0.069      1.967      0.049       0.001       0.273\nage                                -0.3718      0.093     -4.019      0.000      -0.553      -0.190\n===================================================================================================\n\nPossibly complete quasi-separation: A fraction 0.54 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n/home/dylan/give-directly-exercise/.venv/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nWhen time_county, month, and account_status are included along with age, age and time_county are still significant predictors. We can also look at other feature selection methods.\nLogistic regression can also be used to select features based on their importance in predicting at least one successful survey.\n\nreg = LogisticRegressionCV()\ncat_cols = ['month', 'account_status', 'county']\nnum_cols = ['age', 'time_county']\nd = defaultdict(LabelEncoder)\nle_fit = merged[cat_cols].apply(lambda x: d[x.name].fit_transform(x))\nX = pd.concat((merged[num_cols], le_fit), axis=1)\ny = merged['one_success']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.3, random_state=42\n)\n\nreg.fit(X_train, y_train)\nprint(\"Best accuracy score using built-in LogisticRegCV: %f\" % reg.score(X_test, y_test))\ncoef = pd.Series(reg.coef_.flatten(), index=X.columns)\nimp_coef = coef.sort_values()\n\nimp_coef.plot(kind=\"barh\")\nplt.title(\"Feature importance using Logistic Regression Model\")\nplt.tight_layout()\nplt.show()\n\nBest accuracy score using built-in LogisticRegCV: 0.992908\n\n\n\n\n\nIt appears that the account_status and age have the largest effect on whether a survey is successful. age has a negative effect however. Recall that account_status and age are correlated.\n\nmerged.groupby('account_status')['age'].mean()\n\naccount_status\nActive        36.535321\nNot Active    46.908571\nName: age, dtype: float64\n\n\nThe ‘Not Active’ group is slightly older. A test of the equality of group means can confirm this. The null hypothesis is that the means are equal.\n\nactive_age = merged.age[merged.account_status == 'Active']\nnotactive_age = merged.age[merged.account_status == 'Not Active']\nprint(CompareMeans.from_data(active_age, notactive_age).ttest_ind())\n\n(-4.194989773330013, 3.265347532278871e-05, 468.0)\n\n\nThe p-value is close to zero, rejecting the null hypothesis that the means of the two groups are equal. So there is a statistically significant difference between the ages of ‘Active’ and ‘Not Active’ recipients.\nLet’s now use a tree-based classifier for comparison.\n\nreg_extra_tree = ExtraTreesClassifier(n_estimators=10)\nreg_extra_tree.fit(X_train, y_train)\nfeat_imp = pd.Series(\n    reg_extra_tree.feature_importances_,\n    index=X.columns\n).sort_values()\nprint(f\"Mean accuracy on test data is {reg_extra_tree.score(X_test, y_test)}\")\nfeat_imp.plot(kind=\"barh\")\nplt.title(\"Feature importance using Extra Tree Classifier\")\nplt.tight_layout()\nplt.show()\n\nMean accuracy on test data is 1.0\n\n\n\n\n\nThis time the model has age as the most important effect, with time_county as a close second. All of the features are chosen by the model, however. Note that there is randomness in the model, so sometimes the features will be ranked differently across different runs. But the model will still choose features important for prediction."
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html#question-1",
    "href": "posts/post-with-code/give-directly-assessment.html#question-1",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "Please evaluate the data in recipients.csv and survey_attempts.csv to answer the following questions:\n\nHow many recipients are in each of the four stages? Please provide the calculation(s) in the spreadsheet or code that you submit.\nHow many surveys were successfully completed in December, 2020? Please provide the calculation(s) in the spreadsheet or code that you submit.\nDid you find any abnormalities in the source data? If so, how did you account for them in your analysis?\n\n\nFirst install the required packages.\n\n!pip install -r requirements.txt\n\nRequirement already satisfied: pandas==1.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: jupyterlab==3.4.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: matplotlib==3.5.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.5.2)\nRequirement already satisfied: ipython==8.4.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (8.4.0)\nRequirement already satisfied: flake8==4.0.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (4.0.1)\nRequirement already satisfied: PyQt5==5.15.6 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (5.15.6)\nRequirement already satisfied: statsmodels==0.13.2 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.13.2)\nRequirement already satisfied: ipykernel==6.13.0 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (6.13.0)\nRequirement already satisfied: missingno==0.5.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.5.1)\nRequirement already satisfied: scikit-learn==1.1.1 in ./.venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: numpy&gt;=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.22.4)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.4.2-&gt;-r requirements.txt (line 1)) (2022.1)\nRequirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3)\nRequirement already satisfied: jupyter-server~=1.16 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: tornado&gt;=6.1.0 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.1)\nRequirement already satisfied: jupyter-core in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.10.0)\nRequirement already satisfied: jinja2&gt;=2.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.1.2)\nRequirement already satisfied: nbclassic~=0.2 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.3.7)\nRequirement already satisfied: jupyterlab-server~=2.10 in ./.venv/lib/python3.8/site-packages (from jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.14.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (9.1.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (4.33.3)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib==3.5.2-&gt;-r requirements.txt (line 3)) (1.4.2)\nRequirement already satisfied: pickleshare in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (3.0.29)\nRequirement already satisfied: traitlets&gt;=5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.2.2.post1)\nRequirement already satisfied: backcall in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pygments&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.12.0)\nRequirement already satisfied: jedi&gt;=0.16 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.18.1)\nRequirement already satisfied: pexpect&gt;4.3; sys_platform != \"win32\" in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (4.8.0)\nRequirement already satisfied: setuptools&gt;=18.5 in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (44.0.0)\nRequirement already satisfied: decorator in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (5.1.1)\nRequirement already satisfied: matplotlib-inline in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.1.3)\nRequirement already satisfied: stack-data in ./.venv/lib/python3.8/site-packages (from ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: pyflakes&lt;2.5.0,&gt;=2.4.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.4.0)\nRequirement already satisfied: mccabe&lt;0.7.0,&gt;=0.6.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (0.6.1)\nRequirement already satisfied: pycodestyle&lt;2.9.0,&gt;=2.8.0 in ./.venv/lib/python3.8/site-packages (from flake8==4.0.1-&gt;-r requirements.txt (line 5)) (2.8.0)\nRequirement already satisfied: PyQt5-Qt5&gt;=5.15.2 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (5.15.2)\nRequirement already satisfied: PyQt5-sip&lt;13,&gt;=12.8 in ./.venv/lib/python3.8/site-packages (from PyQt5==5.15.6-&gt;-r requirements.txt (line 6)) (12.10.1)\nRequirement already satisfied: patsy&gt;=0.5.2 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (0.5.2)\nRequirement already satisfied: scipy&gt;=1.3 in ./.venv/lib/python3.8/site-packages (from statsmodels==0.13.2-&gt;-r requirements.txt (line 7)) (1.8.1)\nRequirement already satisfied: debugpy&gt;=1.0 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.6.0)\nRequirement already satisfied: jupyter-client&gt;=6.1.12 in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (7.3.1)\nRequirement already satisfied: nest-asyncio in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (1.5.5)\nRequirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (5.9.1)\nRequirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (from missingno==0.5.1-&gt;-r requirements.txt (line 9)) (0.11.2)\nRequirement already satisfied: joblib&gt;=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.1-&gt;-r requirements.txt (line 10)) (3.1.0)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas==1.4.2-&gt;-r requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: terminado&gt;=0.8.3 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.15.0)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.6.1)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.5.0)\nRequirement already satisfied: Send2Trash in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.8.0)\nRequirement already satisfied: websocket-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.3.2)\nRequirement already satisfied: pyzmq&gt;=17 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (23.1.0)\nRequirement already satisfied: argon2-cffi in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.3.0)\nRequirement already satisfied: prometheus-client in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.14.1)\nRequirement already satisfied: nbformat&gt;=5.2.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.4.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2&gt;=2.1-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.1.1)\nRequirement already satisfied: notebook-shim&gt;=0.1.0 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.1.0)\nRequirement already satisfied: notebook&lt;7 in ./.venv/lib/python3.8/site-packages (from nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (6.4.11)\nRequirement already satisfied: babel in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.10.1)\nRequirement already satisfied: json5 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.9.8)\nRequirement already satisfied: jsonschema&gt;=3.0.1 in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.6.0)\nRequirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.27.1)\nRequirement already satisfied: importlib-metadata&gt;=3.6; python_version &lt; \"3.10\" in ./.venv/lib/python3.8/site-packages (from jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.4)\nRequirement already satisfied: wcwidth in ./.venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.5)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in ./.venv/lib/python3.8/site-packages (from jedi&gt;=0.16-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != \"win32\"-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.7.0)\nRequirement already satisfied: pure-eval in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.2.2)\nRequirement already satisfied: asttokens in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (2.0.5)\nRequirement already satisfied: executing in ./.venv/lib/python3.8/site-packages (from stack-data-&gt;ipython==8.4.0-&gt;-r requirements.txt (line 4)) (0.8.3)\nRequirement already satisfied: entrypoints in ./.venv/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.12-&gt;ipykernel==6.13.0-&gt;-r requirements.txt (line 8)) (0.4)\nRequirement already satisfied: sniffio&gt;=1.1 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: idna&gt;=2.8 in ./.venv/lib/python3.8/site-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.3)\nRequirement already satisfied: nbclient&gt;=0.5.0 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.6.4)\nRequirement already satisfied: jupyterlab-pygments in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.2)\nRequirement already satisfied: tinycss2 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.1.1)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.5.0)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.8.4)\nRequirement already satisfied: defusedxml in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (4.11.1)\nRequirement already satisfied: bleach in ./.venv/lib/python3.8/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.0.0)\nRequirement already satisfied: argon2-cffi-bindings in ./.venv/lib/python3.8/site-packages (from argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.2.0)\nRequirement already satisfied: fastjsonschema in ./.venv/lib/python3.8/site-packages (from nbformat&gt;=5.2.0-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.15.3)\nRequirement already satisfied: ipython-genutils in ./.venv/lib/python3.8/site-packages (from notebook&lt;7-&gt;nbclassic~=0.2-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.2.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: importlib-resources&gt;=1.4.0; python_version &lt; \"3.9\" in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (5.7.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema&gt;=3.0.1-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2022.5.18.1)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version &gt;= \"3\" in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.0.12)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.26.9)\nRequirement already satisfied: zipp&gt;=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata&gt;=3.6; python_version &lt; \"3.10\"-&gt;jupyterlab-server~=2.10-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (3.8.0)\nRequirement already satisfied: webencodings&gt;=0.4 in ./.venv/lib/python3.8/site-packages (from tinycss2-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (0.5.1)\nRequirement already satisfied: soupsieve&gt;1.2 in ./.venv/lib/python3.8/site-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.3.2.post1)\nRequirement already satisfied: cffi&gt;=1.0.1 in ./.venv/lib/python3.8/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (1.15.0)\nRequirement already satisfied: pycparser in ./.venv/lib/python3.8/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server~=1.16-&gt;jupyterlab==3.4.2-&gt;-r requirements.txt (line 2)) (2.21)\n\n\nImport the required libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nmatplotlib.use('QtAgg')\n\nfrom statsmodels.stats.weightstats import CompareMeans\nimport statsmodels.formula.api as smf\nimport missingno as msno\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nLet’s now import the two datasets and merge them.\n\nrecipients = pd.read_csv('recipients.csv')\nattempts = pd.read_csv('survey_attempts.csv')\nmerged = pd.merge(recipients, attempts, on='recipient_id', how='left', indicator=True)\nmerged._merge.value_counts(dropna=False)\n\nboth          520\nleft_only       0\nright_only      0\nName: _merge, dtype: int64\n\n\nIt looks like all the recipients are matched with survey attempts. We can drop the _merge variable.\n\nmerged.drop(columns='_merge', inplace=True)\n\nA quick glance at our data.\n\nmerged.head(10)\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n0\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\n\n\n1\nr-00085\nCounty B\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\n\n\n2\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000151\n12/20/20\nTrue\n\n\n3\nr-00082\nCounty C\n29.0\n31.0\n100000023.0\nNot Active\ns-000305\n11/22/20\nFalse\n\n\n4\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000108\n11/28/20\nFalse\n\n\n5\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000116\n01/17/21\nTrue\n\n\n6\nr-00048\nCounty C\n22.0\n24.0\n100000035.0\nActive\ns-000439\n11/18/20\nFalse\n\n\n7\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000077\n11/06/20\nFalse\n\n\n8\nr-00096\nCounty B\n22.0\n25.0\n100000166.0\nActive\ns-000130\n01/03/21\nTrue\n\n\n9\nr-00064\nCounty D\n24.0\n26.0\n100000076.0\nActive\ns-000352\n11/04/20\nFalse\n\n\n\n\n\n\n\nCheck for duplicates. We expect that recipient_id and survey_id together form a unique id.\n\nmerged[merged.duplicated(subset=['recipient_id', 'survey_id'], keep=False)]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\n\n\n\n\n15\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n17\nr-00100\nCounty C\n30.0\n32.0\n100000107.0\nActive\ns-000045\n11/10/20\nFalse\n\n\n19\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n22\nr-00030\nCounty C\n23.0\n26.0\n100000179.0\nActive\ns-000036\n11/10/20\nFalse\n\n\n28\nr-00193\nCounty B\n10.0\n62.0\n100000977.0\nActive\ns-000018\n01/22/21\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n506\nr-00191\nCounty B\n10.0\n62.0\n100000126.0\nActive\ns-000026\n12/11/20\nFalse\n\n\n510\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n513\nr-00036\nCounty C\n32.0\n32.0\n100000183.0\nActive\ns-000003\n11/16/20\nFalse\n\n\n518\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n519\nr-00122\nCounty A\n8.0\n49.0\n100000013.0\nNot Active\ns-000005\n01/29/21\nTrue\n\n\n\n\n100 rows × 9 columns\n\n\n\nThere are 50 pairs of duplicates that need to be dropped.\n\nmerged = merged.drop_duplicates(subset=['recipient_id', 'survey_id'], keep='first').reset_index(drop=True)\n\nLet’s now calculate the stage variable, starting with ‘Start’.\n\nmask = merged.groupby('recipient_id')['success'].any()\none_success = [mask.index[i] for i, m in enumerate(mask) if m]\n# Set the value to start for those with no successful surveys. '~' negates the value \n# of the mask. In this case, '~' means find those without any successful survey\nmerged.loc[~merged.recipient_id.isin(one_success), 'stage'] = 'Start'\n\nNext, the ‘Ineligible’ stage\n\n# Remove the text 'County' from the column\nmerged.county = merged.county.str.replace('County ', '', regex=False)\n    \ninABC = merged.county.isin(['A', 'B', 'C'])\nrecipient_noABC = merged.recipient_id[~inABC]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n    merged.recipient_id.isin(recipient_noABC), 'stage'] = 'Ineligible'\n\nThe ‘Review’ stage\n\nrecipient_yesABC = merged.recipient_id[inABC]\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\nAnd finally the ‘Pay’ stage\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\n\n\n\nmerged.stage.value_counts(dropna=False)\n\nPay           209\nStart         150\nIneligible     70\nReview         35\nNaN             6\nName: stage, dtype: int64\n\n\nThere are 209 in the ‘Pay’ stage, 150 in the ‘Start’, 70 ‘Ineligible’, 35 in ‘Review’, and 6 not in any stage. This is because there are 6 recipients with a missing account_status.\n\nmerged.loc[merged.stage.isna()]\n\n\n\n\n\n\n\n\nrecipient_id\ncounty\ntime_county\nage\naccount_number\naccount_status\nsurvey_id\ndate\nsuccess\nstage\n\n\n\n\n0\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000342\n12/23/20\nTrue\nNaN\n\n\n1\nr-00085\nB\n22.0\n22.0\n100000022.0\nNaN\ns-000448\n11/25/20\nFalse\nNaN\n\n\n14\nr-00145\nA\n22.0\n24.0\n100000089.0\nNaN\ns-000137\n12/21/20\nTrue\nNaN\n\n\n50\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000161\n11/25/20\nFalse\nNaN\n\n\n51\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000236\n11/15/20\nFalse\nNaN\n\n\n52\nr-00045\nC\n26.0\n30.0\n100000199.0\nNaN\ns-000337\n01/14/21\nTrue\nNaN\n\n\n\n\n\n\n\nWe will impute a value for recipients with a missing account_status, but it could be worthwhile to follow up with someone for the appropriate status.\n\n\n\n\nmerged['month'] = merged['date'].astype(str).str[:2]\nmerged.groupby('month')['success'].sum()\n\nmonth\n01    77\n11     0\n12    93\nName: success, dtype: int64\n\n\nThere were 93 successful surveys in December.\n\n\n\nThere were duplicates in the data that had to be dropped. Some values also did not make much sense and were assgined missing value. The missing data was later imputed.\n\nmerged.describe()\n\n\n\n\n\n\n\n\ntime_county\nage\naccount_number\n\n\n\n\ncount\n455.000000\n460.000000\n4.630000e+02\n\n\nmean\n19.138462\n124.126087\n1.000001e+08\n\n\nstd\n10.501550\n925.986058\n1.074233e+02\n\n\nmin\n-45.000000\n20.000000\n1.000000e+08\n\n\n25%\n10.000000\n25.000000\n1.000001e+08\n\n\n50%\n21.000000\n32.000000\n1.000001e+08\n\n\n75%\n28.000000\n51.000000\n1.000002e+08\n\n\nmax\n35.000000\n9999.000000\n1.000010e+08\n\n\n\n\n\n\n\ntime_county has a minimum of -45 and age has a max of 9999. To deal with these problematic cases, I replace them with missing values to be imputed later. It could also be good to follow up with other teams on the correct value.\n\n\nmerged.age[merged.age == 9999] = pd.np.nan \nmerged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n/tmp/ipykernel_14671/1017566221.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.age[merged.age == 9999] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n/tmp/ipykernel_14671/1017566221.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  merged.time_county[merged.time_county &lt; 0] = pd.np.nan\n\n\nThere are missing values in the data as well. Six columns have missing data.\n\nmerged.isna().sum()\n\nrecipient_id       0\ncounty             8\ntime_county       20\nage               14\naccount_number     7\naccount_status    13\nsurvey_id          0\ndate               0\nsuccess            0\nstage              6\nmonth              0\ndtype: int64\n\n\nMissing data is usually deleted or imputed. Deleting missing data is easiest, but it can lead to biases if the data is not missing at random. If age is more likely to be missing in certain counties, for example, deleting missing data isn’t the best approach. If the data is missing completely at random or missing at random, it can be deleted.\nIf there are many missing values in a row, the row can be deleted. Likewise, if there are many missing values in a column, the column can be deleted.\nImputation can be as simple as replacing the missing value in a column with an arbitrary value such as ‘0’ or the column mean or mode. Other techniques take into account the values in other columns. Suppose a recipient has a missing age, but lives in a certain county and has also responded to the survey. Then the age might be imputed to be slightly lower because of a correlation between success and age or county and age. See more here.\nWe can investigate the missing data with the missingno package.\n\n%matplotlib inline\n\nmsno.matrix(merged)\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_14671/2359591310.py:5: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\nThe white lines represent missing values. The account_number and account_status appear to have coinciding missing values, and the same for age and time_county. We can visualize these relationships with a heatmap.\n\n%matplotlib inline\n\nmsno.heatmap(merged, cmap='rainbow')\nplt.tight_layout()\nplt.show()\n\n\n\n\nThere is a strong correlation between account_number and account_status being missing, which would make sense considering that someone without an account_number would not have an account_status and vice versa. There is also a strong association between age and time_county missingness.\nTo investigate this, let’s look at recipients with a missing time_county.\n\ntime_county_missing = merged.loc[merged.time_county.isna()]\nprint(time_county_missing.head())\n\n    recipient_id county  time_county   age  account_number account_status  \\\n69       r-00070      D          NaN  23.0     100000080.0         Active   \n70       r-00070      D          NaN  23.0     100000080.0         Active   \n182      r-00198      B          NaN   NaN     100000006.0         Active   \n183      r-00198      B          NaN   NaN     100000006.0         Active   \n184      r-00198      B          NaN   NaN     100000006.0         Active   \n\n    survey_id      date  success       stage month  \n69   s-000252  12/08/20     True  Ineligible    12  \n70   s-000280  11/10/20    False  Ineligible    11  \n182  s-000284  01/07/21    False       Start    01  \n183  s-000285  12/08/20    False       Start    12  \n184  s-000311  12/18/20    False       Start    12  \n\n\nAnd a missing age\n\nage_missing = merged.loc[merged.age.isna()]\nprint(age_missing.head())\n\n    recipient_id county  time_county  age  account_number account_status  \\\n145      r-00060      B         28.0  NaN     100000018.0     Not Active   \n146      r-00060      B         28.0  NaN     100000018.0     Not Active   \n182      r-00198      B          NaN  NaN     100000006.0         Active   \n183      r-00198      B          NaN  NaN     100000006.0         Active   \n184      r-00198      B          NaN  NaN     100000006.0         Active   \n\n    survey_id      date  success   stage month  \n145  s-000273  11/30/20    False  Review    11  \n146  s-000455  01/29/21     True  Review    01  \n182  s-000284  01/07/21    False   Start    01  \n183  s-000285  12/08/20    False   Start    12  \n184  s-000311  12/18/20    False   Start    12  \n\n\n\nprint(time_county_missing.recipient_id.nunique())\n\n5\n\n\n\nprint(age_missing.recipient_id.nunique())\n\n4\n\n\nSo age is usually missing when time_county is missing. There are five recipients that did not fill in their age and four that did not fill in time_county for multiple survey attempts. They may have been uncomfortable sharing that information.\nDropping these rows could lead to bias because they are not missing at random. Let’s impute the age and time_county for them. We’ll also impute the county and account_status, which are categorical variables. The imputation strategy will regress the column with missing data, say age, on other columns in the data. It will then use the regression coefficients to predict the missing values for age. More details can be found here, here, and here\nStarting with the categorical columns, we’ll prepare the dataset for imputation by converting account_status and county to categorical codes.\n\ncat_cols_na = ['account_status', 'county']\nmerged[cat_cols_na] = merged[cat_cols_na].astype('category')\nd_na = {col: {n: cat for n, cat in enumerate(merged[col].cat.categories)}\n            for col in cat_cols_na}\nmerged[cat_cols_na] = pd.DataFrame(\n    {col: merged[col].cat.codes for col in cat_cols_na},\n    index=merged.index\n)\nprint(merged[cat_cols_na].head())\n\n   account_status  county\n0              -1       1\n1              -1       1\n2               1       2\n3               1       2\n4               0       2\n\n\nNow we can impute the missing values.\n\n\nimp_cat = IterativeImputer(estimator=ExtraTreesClassifier(),\n                            initial_strategy='most_frequent',\n                            max_iter=10, random_state=0, missing_values=-1)\n\n\nmerged[cat_cols_na] = imp_cat.fit_transform(merged[cat_cols_na])\n\nFinally, we convert the numerical codes back to their original labels\n\nfor col in cat_cols_na:\n        merged[col].replace(d_na[col], inplace=True)\nprint(merged[cat_cols_na].head())\n\n  account_status county\n0         Active      B\n1         Active      B\n2     Not Active      C\n3     Not Active      C\n4         Active      C\n\n\nage and time_county are numeric variables, so they do not need a conversion.\n\nnum_cols_na = ['age', 'time_county']\nimp_num = IterativeImputer(estimator=ExtraTreesRegressor(),\n                               initial_strategy='median',\n                               max_iter=10, random_state=0)\nmerged[num_cols_na] = imp_num.fit_transform(merged[num_cols_na])\n\nNow that account_status has been imputed, the stage variable can be recalculated because the six missing cases were missing because account_status was missing at the time of calculation.\n\nnotActive = merged.account_status == 'Not Active'\nrecipient_notActive = merged.recipient_id[notActive]\n\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_notActive),\n           'stage'] = 'Review'\n\n\nactive = merged.account_status == 'Active'\nrecipient_active = merged.recipient_id[active]\nmerged.loc[merged.recipient_id.isin(one_success) &\n           merged.recipient_id.isin(recipient_yesABC) &\n           merged.recipient_id.isin(recipient_active), 'stage'] = 'Pay'\n\nAnother look at which columns contain missing data shows that there is only the account_number column, which can be ignored for analysis. It also would not be appropriate to impute an identifier.\n\nmerged.isna().sum()\n\nrecipient_id      0\ncounty            0\ntime_county       0\nage               0\naccount_number    7\naccount_status    0\nsurvey_id         0\ndate              0\nsuccess           0\nstage             0\nmonth             0\ndtype: int64"
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html#question-2",
    "href": "posts/post-with-code/give-directly-assessment.html#question-2",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "The program manager has asked for data to help determine whether the field team should focus more effort on calling those in stage Start or following up to resolve issues with those in stage Review. Please write a response to the program manager, including data that may help inform the decision, and some additional factors that you would take into consideration to make the decision. Assume that the program manager’s expertise does not include interpreting data and complex analytics. Please limit your written response to 300 words or less. \n\nIt would not make sense to calculate the chance of someone having a successful survey based on the stage variable because the ‘Start’ group would have no successful surveys, and the other three groups would have all recipients with at least one successful survey. On the other hand, it would be good to know the chances of someone moving from the ‘Review’ stage to a ‘Pay’ stage at a later date, for example.\n\n# Check how many recipients have more than one stage\n(merged.groupby('recipient_id')['stage'].nunique() &gt; 1).sum()\n\n0\n\n\nThere do not appear to be recipients that have changed their stage. Let’s check for recipients that have changed from ‘Not Active’ to ‘Active’ account_status.\n\n(merged.groupby('recipient_id')['account_status'].nunique() &gt; 1).sum()\n\n0\n\n\nNo recipients have changed from ‘Not Active’ to ‘Active’ account_status either.\nOne thing to consider is that the ‘Start’ group has 150 recipients compared to only 35 in the ‘Review’ group. The ‘Start’ group success rate only needs to be 35/150 ~ 23% to match a 100% success rate in the ‘Review’ group. It would be good to look up historical data on the conversion rate from ‘Start’ to ‘Pay’ versus ‘Review’ to ‘Pay’. Data on the cost of converting the ‘Start’ group versus the ‘Review’ group would also be helpful for the decision.\n\nmerged.stage.value_counts(dropna=False)\n\nPay           215\nStart         150\nIneligible     70\nReview         35\nName: stage, dtype: int64"
  },
  {
    "objectID": "posts/post-with-code/give-directly-assessment.html#question-3",
    "href": "posts/post-with-code/give-directly-assessment.html#question-3",
    "title": "GiveDirectly Assessment",
    "section": "",
    "text": "The country director is considering investing resources into proactively conducting in-person surveys with recipients in the highest age group across projects to increase overall survey success rate. They believe that this additional cost might outweigh the current costs of repeated failed phone survey attempts, if we can accurately target those recipients least likely to respond to a phone survey. \n\nWhat analysis would you provide from the provided project data to help make this decision? Please provide the calculation(s) in the spreadsheet or code that you submit.\nAre there other factors that might explain the observed survey success rate from this project? Please use your judgment to determine these factors and limit your written response to 400 words or less. \n\n\n\n\nLet’s graph the relationship between age and at least one successful survey.\n\n%matplotlib inline\n\nmerged['one_success'] = 0\nmerged.loc[merged.recipient_id.isin(one_success), 'one_success'] = 1\n\n# Create bins for age broken into quartiles\nmerged['age_bin'] = pd.qcut(merged.age, q=4)\nage_success = merged.groupby('age_bin')['one_success'].sum()\nage = merged.groupby('age_bin')['one_success'].count()\nresults = age_success.div(age, level='age_bin') * 100\n\nresults.plot(kind='bar')\n\nplt.xticks(rotation=0)\nplt.xlabel('Age Group')\nplt.ylabel('% of recipients with at least\\n one successful survey')\nplt.show()\n\n\n\n\nPeople in the highest age group are the least likely to have at least one successful survey.\nRunning logistic regression gives\n\nlogit = smf.logit('one_success ~ age', data=merged).fit()\nprint(logit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.278114\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      468\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.5559\nTime:                        13:10:08   Log-Likelihood:                -130.71\nconverged:                       True   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                 3.865e-73\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.8313      0.615     12.744      0.000       6.627       9.036\nage           -0.1740      0.014    -12.308      0.000      -0.202      -0.146\n==============================================================================\n\n\nThere is a negative relationship between age and the chance of at least one successful survey. The coefficient is also statistically significant. To interpret it, we calculate the odds ratio.\n\nodds_ratios = pd.DataFrame({\n        'OR': logit.params, \n        'Lower CI': logit.conf_int()[0],\n        'Upper CI': logit.conf_int()[1]\n    })\n    \nodds_ratios = pd.np.exp(odds_ratios)\nprint(odds_ratios)\nprint(round((odds_ratios['OR'][1] - 1) * 100, 2))\n\n                    OR    Lower CI     Upper CI\nIntercept  2518.096539  755.093254  8397.386877\nage           0.840278    0.817312     0.863888\n-15.97\n\n\n/tmp/ipykernel_14671/4211037893.py:7: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  odds_ratios = pd.np.exp(odds_ratios)\n\n\nEach additional increase of one year in age is associated with a roughly 16 percent decrease in odds of having at least one successful survey.\nSo far, it appears that older recipients are less likely to respond to surveys. Focusing on the older recipients would help target those with lower response rates.\n\n\n\nWe include other variables in our logistic regression to check for confounders.\n\nbig_logit = smf.logit('one_success ~ time_county + age + C(month) + C(account_status)', data=merged).fit()\nprint(big_logit.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.074409\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            one_success   No. Observations:                  470\nModel:                          Logit   Df Residuals:                      464\nMethod:                           MLE   Df Model:                            5\nDate:                Wed, 08 Jun 2022   Pseudo R-squ.:                  0.8812\nTime:                        13:10:08   Log-Likelihood:                -34.972\nconverged:                      False   LL-Null:                       -294.33\nCovariance Type:            nonrobust   LLR p-value:                7.306e-110\n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nIntercept                          10.2072      2.959      3.449      0.001       4.407      16.007\nC(month)[T.11]                     24.6815   8.53e+04      0.000      1.000   -1.67e+05    1.67e+05\nC(month)[T.12]                     -0.3861      0.670     -0.576      0.564      -1.699       0.927\nC(account_status)[T.Not Active]    30.8016   4900.380      0.006      0.995   -9573.767    9635.370\ntime_county                         0.1367      0.069      1.967      0.049       0.001       0.273\nage                                -0.3718      0.093     -4.019      0.000      -0.553      -0.190\n===================================================================================================\n\nPossibly complete quasi-separation: A fraction 0.54 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n/home/dylan/give-directly-exercise/.venv/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nWhen time_county, month, and account_status are included along with age, age and time_county are still significant predictors. We can also look at other feature selection methods.\nLogistic regression can also be used to select features based on their importance in predicting at least one successful survey.\n\nreg = LogisticRegressionCV()\ncat_cols = ['month', 'account_status', 'county']\nnum_cols = ['age', 'time_county']\nd = defaultdict(LabelEncoder)\nle_fit = merged[cat_cols].apply(lambda x: d[x.name].fit_transform(x))\nX = pd.concat((merged[num_cols], le_fit), axis=1)\ny = merged['one_success']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.3, random_state=42\n)\n\nreg.fit(X_train, y_train)\nprint(\"Best accuracy score using built-in LogisticRegCV: %f\" % reg.score(X_test, y_test))\ncoef = pd.Series(reg.coef_.flatten(), index=X.columns)\nimp_coef = coef.sort_values()\n\nimp_coef.plot(kind=\"barh\")\nplt.title(\"Feature importance using Logistic Regression Model\")\nplt.tight_layout()\nplt.show()\n\nBest accuracy score using built-in LogisticRegCV: 0.992908\n\n\n\n\n\nIt appears that the account_status and age have the largest effect on whether a survey is successful. age has a negative effect however. Recall that account_status and age are correlated.\n\nmerged.groupby('account_status')['age'].mean()\n\naccount_status\nActive        36.535321\nNot Active    46.908571\nName: age, dtype: float64\n\n\nThe ‘Not Active’ group is slightly older. A test of the equality of group means can confirm this. The null hypothesis is that the means are equal.\n\nactive_age = merged.age[merged.account_status == 'Active']\nnotactive_age = merged.age[merged.account_status == 'Not Active']\nprint(CompareMeans.from_data(active_age, notactive_age).ttest_ind())\n\n(-4.194989773330013, 3.265347532278871e-05, 468.0)\n\n\nThe p-value is close to zero, rejecting the null hypothesis that the means of the two groups are equal. So there is a statistically significant difference between the ages of ‘Active’ and ‘Not Active’ recipients.\nLet’s now use a tree-based classifier for comparison.\n\nreg_extra_tree = ExtraTreesClassifier(n_estimators=10)\nreg_extra_tree.fit(X_train, y_train)\nfeat_imp = pd.Series(\n    reg_extra_tree.feature_importances_,\n    index=X.columns\n).sort_values()\nprint(f\"Mean accuracy on test data is {reg_extra_tree.score(X_test, y_test)}\")\nfeat_imp.plot(kind=\"barh\")\nplt.title(\"Feature importance using Extra Tree Classifier\")\nplt.tight_layout()\nplt.show()\n\nMean accuracy on test data is 1.0\n\n\n\n\n\nThis time the model has age as the most important effect, with time_county as a close second. All of the features are chosen by the model, however. Note that there is randomness in the model, so sometimes the features will be ranked differently across different runs. But the model will still choose features important for prediction."
  }
]